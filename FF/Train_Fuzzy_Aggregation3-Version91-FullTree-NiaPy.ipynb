{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:29.616136Z",
     "start_time": "2020-02-26T10:57:29.600670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"notebook_name = \" + \"'\"+thename+\"'\";\n",
       "kernel.execute(command);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import Javascript\n",
    "Javascript(\"\"\"\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"notebook_name = \" + \"'\"+thename+\"'\";\n",
    "kernel.execute(command);\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:29.641327Z",
     "start_time": "2020-02-26T10:57:29.628891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Fuzzy_Aggregation3-Version91-FullTree-NiaPy\n"
     ]
    }
   ],
   "source": [
    "print(notebook_name)\n",
    "def getFileNumber():\n",
    "    if \"Copy\" not in notebook_name :\n",
    "        return \"1\"\n",
    "    \n",
    "    temp = notebook_name.split(\".\")[0]\n",
    "    temp = temp.split(\"-\")[-1]\n",
    "    temp = temp.replace(\"Copy\",\"\")\n",
    "    return str(int(temp) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:31.807545Z",
     "start_time": "2020-02-26T10:57:29.646593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "#25-2-63\n",
    "\n",
    "#Using Pocket Algor \n",
    "\n",
    "#pip install NiaPy==2.0.0rc10\n",
    "\n",
    "#https://grega.xyz/post/niapy_optimize_knn/  \n",
    "%autosave 60\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import bisect\n",
    "import math\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Display graph sequently\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "#set_trace()\n",
    "\n",
    "import random\n",
    "import gc\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:31.842651Z",
     "start_time": "2020-02-26T10:57:31.831994Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert to categorical\n",
    "def to_categorical(val,left_v,right_v) :\n",
    "    if left_v < 0.1 :\n",
    "        left_v = 0.2\n",
    "    if right_v < 0.1 :\n",
    "        right_v = 0.2\n",
    "    output = []\n",
    "    for v in val :\n",
    "        if v < 0.5 :\n",
    "            output.append([left_v,0.1])\n",
    "        else :\n",
    "            output.append([0.1,right_v])\n",
    "    return np.asarray(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:31.880310Z",
     "start_time": "2020-02-26T10:57:31.857245Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred,classes,normalize=False,title=None,cmap=plt.cm.Blues):\n",
    "    y_true = np.asarray(y_true).astype('int')\n",
    "    y_pred = np.asarray(y_pred).astype('int')\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    display.display(fig)\n",
    "    plt.close(fig)\n",
    "#     return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller to show result of Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:31.909782Z",
     "start_time": "2020-02-26T10:57:31.887956Z"
    }
   },
   "outputs": [],
   "source": [
    "class StoreOptimization(object) :\n",
    "    results = []\n",
    "    generation = 0\n",
    "    \n",
    "    count_generation = 0\n",
    "    \n",
    "    store_best_weights = []\n",
    "    store_best_error = 1\n",
    "    \n",
    "    #default\n",
    "    n_generation_wanted = 1\n",
    "    \n",
    "    #Pocket Algorithm\n",
    "    '''\n",
    "    Hs - history of total correct class\n",
    "    Hs1 - history class 1 correct class\n",
    "    Hs2 - history class 2 correct class\n",
    "    '''\n",
    "    \n",
    "    Hs = 0 \n",
    "    Hs1 = 0\n",
    "    Hs2 = 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def static_N_Gen(val):\n",
    "         StoreOptimization.n_generation_wanted = val\n",
    "    \n",
    "    @staticmethod\n",
    "    def static_clearAll() :\n",
    "        StoreOptimization.generation = 0\n",
    "        StoreOptimization.count_generation = 0\n",
    "        StoreOptimization.store_best_weights = []\n",
    "        StoreOptimization.store_best_error = 1\n",
    "        StoreOptimization.Hs = 0\n",
    "        StoreOptimization.Hs1 = 0\n",
    "        StoreOptimization.Hs2 = 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def static_appendString(head_str) :\n",
    "        logging.info('%s' % (head_str))\n",
    "        print('%s' % (head_str))\n",
    "    \n",
    "    @staticmethod\n",
    "    def static_addResult(Pc1,Pc2,Pc,error_rmse,weights,n_fold) :\n",
    "        \n",
    "            \n",
    "        StoreOptimization.count_generation = StoreOptimization.count_generation + 1\n",
    "        StoreOptimization.generation = StoreOptimization.generation + 1\n",
    "        \n",
    "        if StoreOptimization.n_generation_wanted == StoreOptimization.count_generation :\n",
    "            StoreOptimization.count_generation = 0\n",
    "            StoreOptimization.static_appendString('fold %s nFES %s error %s' % (n_fold,StoreOptimization.generation, error_rmse))\n",
    "        \n",
    "        if (Pc1 > StoreOptimization.Hs1 or Pc2 > StoreOptimization.Hs2) and (Pc > StoreOptimization.Hs) :\n",
    "            StoreOptimization.store_best_error = error_rmse \n",
    "            StoreOptimization.store_best_weights = weights\n",
    "            StoreOptimization.Hs1 = Pc1\n",
    "            StoreOptimization.Hs2 = Pc2\n",
    "            StoreOptimization.Hs = Pc\n",
    "            StoreOptimization.static_appendString('fold %s best at nFES %s error %s (%s,%s,%s)' % (n_fold,StoreOptimization.generation, error_rmse,Pc1,Pc2,Pc))\n",
    "            logging.info(weights)\n",
    "\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:31.936528Z",
     "start_time": "2020-02-26T10:57:31.912587Z"
    }
   },
   "outputs": [],
   "source": [
    "class leaf_node :\n",
    "    def __init__(self,count_leaves) :\n",
    "        self.list_weight_input = np.ones(count_leaves).tolist()\n",
    "        self.gammar_operator = 1.0\n",
    "        \n",
    "    def check_input(self,val) :\n",
    "        '''\n",
    "        Axiom 1\n",
    "        Check input h(0,0,0) = 0\n",
    "        Check input h(1,1,1) = 1\n",
    "        '''\n",
    "        repeat_zeros = np.repeat(0.00000001,len(val))\n",
    "        repeat_ones = np.repeat(0.99999999,len(val))\n",
    "        \n",
    "        if np.sum(val == repeat_zeros) == len(val) :\n",
    "            return np.repeat(0.0,len(val))\n",
    "        \n",
    "        if np.sum(val == repeat_ones) == len(val) :\n",
    "            return np.repeat(1.0,len(val))\n",
    "            \n",
    "        return val\n",
    "        \n",
    "        \n",
    "    def cal_y(self,input_x) :\n",
    "        \n",
    "        input_x = self.check_input(input_x)\n",
    "         \n",
    "        if len(input_x) != len(self.list_weight_input) :\n",
    "            print(\"Error cal , input not equal weight\")\n",
    "            return\n",
    "    \n",
    "        \n",
    "        temp_mul_y1 = [ np.power(input_x[idx] , self.list_weight_input[idx]) for idx in range(0,len(input_x)) ]\n",
    "        \n",
    "        prod_y1 = 1\n",
    "        for idx in  temp_mul_y1  :\n",
    "            prod_y1 = prod_y1 * idx\n",
    "            \n",
    "        y1_out = np.power(prod_y1,1-self.gammar_operator)\n",
    "        \n",
    "        temp_mul_y2 = [ np.power(1-input_x[idx],self.list_weight_input[idx]) for idx in range(0,len(input_x)) ]\n",
    "        \n",
    "        prod_y2 = 1\n",
    "        for idx in  temp_mul_y2  :\n",
    "            prod_y2 = prod_y2 * idx\n",
    "            \n",
    "        y2_out = np.power(1 - prod_y2,self.gammar_operator)\n",
    "        \n",
    "        real_out = y1_out * y2_out\n",
    "         \n",
    "        if real_out == 0 :\n",
    "            real_out = 0.00000001\n",
    "        elif real_out == 1 :\n",
    "            real_out = 0.99999999\n",
    "            \n",
    "        return real_out\n",
    "\n",
    "    def change_weights(self,new_weight,new_gammar) :\n",
    "\n",
    "        if self.count_weight() == len(new_weight) :\n",
    "            self.list_weight_input = new_weight\n",
    "            self.gammar_operator = new_gammar\n",
    "\n",
    "        else :\n",
    "            print(\"Error Update weight!!!!!\")\n",
    "            raise\n",
    "            \n",
    "    def count_weight(self) :\n",
    "        return len(self.list_weight_input)\n",
    "    \n",
    "    def get_weights_gammar(self) :\n",
    "        return self.list_weight_input + [self.gammar_operator]\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:32.011053Z",
     "start_time": "2020-02-26T10:57:31.952328Z"
    }
   },
   "outputs": [],
   "source": [
    "class root_tree :\n",
    "    \n",
    "    def __init__(self,list_group) :\n",
    "        self.number_layers = len(list_group)\n",
    "        self.all_nodes = []\n",
    "        \n",
    "\n",
    "        #Leaf (Top) -> Root (Bottom)\n",
    "        \n",
    "        for count_layer in range(0,len(list_group)) :\n",
    "            temp_leaves = []\n",
    "            for count_node in  range(0,len(list_group[count_layer])) :\n",
    "                temp_leaves.append(leaf_node(list_group[count_layer][count_node]))\n",
    "            self.all_nodes.append(temp_leaves)\n",
    "            \n",
    "        #Split vector input to input each group\n",
    "        self.group_input = list_group\n",
    "\n",
    "    \n",
    "    def get_all_weights_wGammar(self) :\n",
    "        #Return weight and gammar all tree to vector\n",
    "        output = []\n",
    "    \n",
    "        for each_layer in self.all_nodes :\n",
    "            for each_node in each_layer :\n",
    "                \n",
    "                #New position vector X\n",
    "                output = output+each_node.get_weights_gammar()\n",
    "            \n",
    "        return output\n",
    "                \n",
    "    def set_all_weights_wGammar(self,new_weights) :\n",
    "        count_weight_idx = 0\n",
    "        \n",
    "        for each_layer in self.all_nodes :\n",
    "            for each_node in each_layer :\n",
    "                \n",
    "                len_weight = each_node.count_weight()\n",
    "                temp_weights = new_weights[count_weight_idx:count_weight_idx + len_weight]\n",
    "                temp_gammar = new_weights[count_weight_idx+len_weight]\n",
    "                \n",
    "                \n",
    "                #Change weights\n",
    "                try : \n",
    "                    each_node.change_weights(temp_weights,temp_gammar)\n",
    "                except :\n",
    "                    print(\"Error change weight !\")\n",
    "                    sys.exit()\n",
    "                \n",
    "                #New position\n",
    "                count_weight_idx = count_weight_idx+len_weight+1\n",
    "        \n",
    "        \n",
    "                \n",
    "          \n",
    "    #For multi objective to find error\n",
    "    def predict_eachLeaf(self,input_X) :\n",
    "        \n",
    "        summation_error = 0\n",
    "        \n",
    "        output_previous_layer = input_X\n",
    "        \n",
    "        summation_resultY = []\n",
    "        \n",
    "        #feed only leaves tree \n",
    "        for idx,each_layer in enumerate(self.all_nodes) :\n",
    "\n",
    "            count_idx = 0\n",
    "            temp_previous_result = []\n",
    "            \n",
    "            if idx != len(self.all_nodes) - 1 :\n",
    "            \n",
    "                for each_node in each_layer :\n",
    "\n",
    "                    input_x_vector = output_previous_layer[count_idx:count_idx + each_node.count_weight()]\n",
    "\n",
    "                    resultFrom_calY = each_node.cal_y(input_x_vector)\n",
    "\n",
    "                    summation_resultY =  summation_resultY + [resultFrom_calY]\n",
    "\n",
    "                    temp_previous_result.append(resultFrom_calY)\n",
    "\n",
    "                    #New position vector X\n",
    "                    count_idx = count_idx+each_node.count_weight()\n",
    "            else :\n",
    "                \n",
    "                #Get old output from previous cal and calculate two output to classify\n",
    "        \n",
    "                temp_output2class = []\n",
    "                for each_node in each_layer :\n",
    "                    input_x_vector = output_previous_layer[0:count_idx + each_node.count_weight()]\n",
    " \n",
    "                    resultFrom_calY = each_node.cal_y(input_x_vector)\n",
    "                    temp_output2class.append(resultFrom_calY)\n",
    "\n",
    "                summation_resultY =  summation_resultY + temp_output2class\n",
    " \n",
    "            output_previous_layer =  temp_previous_result\n",
    "\n",
    "        return summation_resultY\n",
    "    \n",
    "    \n",
    "    #Feed input to get output only\n",
    "    def predict(self,input_X) :\n",
    "        temp = self.predict_eachLeaf(input_X)[-2::1]\n",
    "        if (temp[0] == 0 and temp[1] == 0) or np.isinf(temp[0]) or np.isinf(temp[1]) or np.isnan(temp[0]) or np.isnan(temp[1]) :\n",
    "            temp = self.predict_eachLeaf(input_X)[-2::1]\n",
    "        return temp\n",
    "        \n",
    "    \n",
    "    def readable(self) :\n",
    "        #Return weight and gammar all tree to vector\n",
    "        temp = []\n",
    "        for (idx,each_layer) in enumerate(self.all_nodes) :\n",
    "            temp = []\n",
    "            for (i,each_node) in enumerate(each_layer) :\n",
    "                temp_get_weight = each_node.get_weights_gammar()\n",
    "                print(\"Weight layer \",idx,\":\",i+1,\" \",temp_get_weight[0:-1])\n",
    "                print(\"Gammar operator : \",temp_get_weight[-1])\n",
    "                print(\"Sum Weights : \",np.sum(temp_get_weight[:-1]))\n",
    "                print(\"\")\n",
    "                #Show important weight each group ratios at final step\n",
    "                \n",
    "                temp.append(temp_get_weight)\n",
    "        \n",
    "            print(\"For check sum at layer: \",np.sum([np.sum(x[0:-1]) for x in temp]))\n",
    "            print(\"\")\n",
    "                \n",
    "        print(\"\")\n",
    "        print(\"#### Left output node ####\")\n",
    "        print(\"liquidity ratio : \",temp[0][0])\n",
    "        print(\"Asset utilization or turnover ratio : \",temp[0][1])\n",
    "        print(\"Profitability : \",temp[0][2])\n",
    "        print(\"Gammar operator : \",temp[0][3])\n",
    "        print(\"Leverage : \",temp[0][4])\n",
    "        print(\"Growth ratios : \",temp[0][5])\n",
    "        print(\"Gammar operator : \",temp[0][6])\n",
    "        print(\"For check sum : \",np.sum(temp[0]) - temp[0][-1])\n",
    "        print(\"\")\n",
    "        print(\"#### Right output node ####\")\n",
    "        print(\"liquidity ratio : \",temp[1][0])\n",
    "        print(\"Asset utilization or turnover ratio : \",temp[1][1])\n",
    "        print(\"Profitability : \",temp[1][2])\n",
    "        print(\"Asset structure ratios : \",temp[1][3])\n",
    "        print(\"Leverage : \",temp[1][4])\n",
    "        print(\"Growth ratios : \",temp[1][5])\n",
    "        print(\"Gammar operator : \",temp[1][6])\n",
    "        print(\"For check sum : \",np.sum(temp[1])-temp[1][-1])\n",
    "    \n",
    "    def countAllNodesOutput(self) :\n",
    "        output = 0\n",
    "        for count_layer in self.group_input :\n",
    "            output = output + len(count_layer)\n",
    "        return output\n",
    "                \n",
    "\n",
    "    \n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:32.231485Z",
     "start_time": "2020-02-26T10:57:32.018410Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from NiaPy.benchmarks.benchmark import Benchmark\n",
    "\n",
    "def try_div(x,y):\n",
    "    try: \n",
    "        return x/y\n",
    "    except ZeroDivisionError: \n",
    "        return 0\n",
    "    \n",
    "def swap_weight(val) :\n",
    "    len_weight = len(val)\n",
    "    temp_multiply =  try_div(len_weight,np.sum(val))\n",
    "    if math.isnan(temp_multiply) or temp_multiply == 0 :\n",
    "        temp_multiply = 1\n",
    "        \n",
    "    output = (temp_multiply * np.asarray(val)).tolist()\n",
    "    output = [0 if math.isnan(x) else x for x in output]\n",
    "\n",
    "    return output\n",
    "    \n",
    "def swap_gammar(val) :\n",
    "    if math.isnan(val) :\n",
    "        return 0\n",
    "    return val\n",
    "\n",
    "# our custom benchmark classs\n",
    "class BenchmarkAllTree(Benchmark):\n",
    "    def __init__(self,tree_shape,start_pos_f,end_pos_f,n_fold):\n",
    "        \n",
    "        self.tree_shape = tree_shape\n",
    "        \n",
    "        self.start_pos_f = start_pos_f\n",
    "        self.end_pos_f = end_pos_f\n",
    "        \n",
    "        self.n_fold = n_fold\n",
    "        \n",
    "        self.fuzzy_system = fuzzy_aggregation(tree_shape = self.tree_shape,current_fold_train=n_fold)\n",
    "        Benchmark.__init__(self, Lower=0, Upper=1)\n",
    "        \n",
    "        \n",
    "    # function which returns evaluate function\n",
    "    def function(self):\n",
    "        def evaluate(D,sol):\n",
    "            gc.collect()\n",
    "            sol = np.abs(sol)\n",
    "\n",
    "            all_weight = []\n",
    "\n",
    "            last_idx = 0\n",
    "\n",
    "\n",
    "            for idx,each_layer in enumerate(self.tree_shape) :\n",
    "                \n",
    "                if idx == len(self.tree_shape) -1 :\n",
    "\n",
    "                        for each_node in each_layer :\n",
    "                            #find weights and gamma before\n",
    "\n",
    "                            len_each_layer_weights = each_node\n",
    "                            len_each_layer_gammas = len(each_layer)\n",
    "\n",
    "                            temp_pos_next_weights = last_idx + len_each_layer_weights\n",
    "\n",
    "\n",
    "                            temp_vector_weights = swap_weight(sol[last_idx:temp_pos_next_weights])\n",
    "                            temp_vector_gammars = sol[temp_pos_next_weights : temp_pos_next_weights + len_each_layer_gammas]\n",
    "\n",
    "                            temp_idx_w = 0\n",
    "                            temp_idx_g = 0\n",
    "                        \n",
    "                        \n",
    "                            all_weight = all_weight + temp_vector_weights[temp_idx_w:temp_idx_w + each_node] + [temp_vector_gammars[temp_idx_g]]\n",
    "                            temp_idx_w = temp_idx_w + each_node\n",
    "                            temp_idx_g = temp_idx_g + 1\n",
    "                            last_idx = last_idx + each_node + 1\n",
    "                    \n",
    "                else :\n",
    "                    #find weights and gamma before\n",
    "                    for each_node in each_layer :\n",
    "                        temp_weights = swap_weight(sol[last_idx:last_idx + each_node])\n",
    "                        temp_gammar = swap_gammar(sol[last_idx + each_node])\n",
    "                        last_idx = last_idx + each_node + 1\n",
    "                        all_weight = all_weight + temp_weights + [temp_gammar]\n",
    "\n",
    "\n",
    "            set_acc,fitness = self.fuzzy_system.evaluate(all_weight,self.start_pos_f ,self.end_pos_f)\n",
    "            StoreOptimization.static_addResult(set_acc[1],set_acc[4],set_acc[0],fitness,all_weight,self.n_fold)\n",
    "\n",
    "            if math.isnan(fitness) :\n",
    "                set_trace()\n",
    "                \n",
    "\n",
    "            return fitness\n",
    "        return evaluate\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:32.308907Z",
     "start_time": "2020-02-26T10:57:32.235523Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class fuzzy_aggregation :\n",
    "    def __init__(self,tree_shape, seed=1235,current_fold_train=1,ignore_writeClass=False) :\n",
    "        self.classifier_root_tree = root_tree(tree_shape)\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.current_fold_train = current_fold_train\n",
    "        global crossvalidation\n",
    "        self.crossvalidation = crossvalidation\n",
    "        \n",
    "        global number_feature\n",
    "        self.number_feature = number_feature\n",
    "        self.all_shape = tree_shape \n",
    "        \n",
    "        global all_data_train\n",
    "        self.all_data_train = all_data_train\n",
    "\n",
    "        random.seed(self.seed)\n",
    "        idx = random.sample(range(0, len(self.all_data_train.index)), len(self.all_data_train.index))\n",
    "\n",
    "\n",
    "#         self.data_train,self.data_blindtest = self.split_blind_test(data_train=np.asarray(self.all_data_train)[idx,:])\n",
    "        self.data_train =np.asarray(self.all_data_train)[idx,:]\n",
    "        \n",
    "        temp_X_train,temp_X_test = self.select_fold(self.current_fold_train)\n",
    "        \n",
    "        #Var for used\n",
    "        self.X_train,self.y_train = self.split_features_class(temp_X_train)\n",
    "        self.X_test,self.y_test = self.split_features_class(temp_X_test)\n",
    "        \n",
    "        \n",
    "#         self.X_blindtest,self.y_blindtest = self.split_features_class(self.data_blindtest)\n",
    "\n",
    "        print(\"Y Train count abnormal : \",len(self.y_train[self.y_train == 1]))\n",
    "        print(np.sum(self.y_train==1) ,len(self.y_train))\n",
    "        print(np.sum(self.y_train==0) ,len(self.y_train))\n",
    "        #set wight output\n",
    "        self.weight_classOne = 1 - (np.sum(self.y_train==1) / len(self.y_train))\n",
    "        self.weight_classZero = 1 - (np.sum(self.y_train==0) / len(self.y_train))\n",
    "        \n",
    "       \n",
    "        self.y_real_train_category = to_categorical(self.y_train,self.weight_classZero,self.weight_classOne)\n",
    "        self.y_real_test_category = to_categorical(self.y_test,self.weight_classZero,self.weight_classOne)\n",
    "        \n",
    "        if not ignore_writeClass :\n",
    "            allWeightOutput = to_categorical([0,1],self.weight_classZero,self.weight_classOne)\n",
    "            StoreOptimization.static_appendString(\"## parameter weight output class zero [%s,%s] \" % (allWeightOutput[0][0],allWeightOutput[0][1]))\n",
    "            StoreOptimization.static_appendString(\"## parameter weight output class one [%s,%s] \" % (allWeightOutput[1][0],allWeightOutput[1][1]))\n",
    "            del allWeightOutput\n",
    "        \n",
    "    def split_blind_test(self,test_size=0.8,data_train = []) :\n",
    "    \n",
    "        range_split= math.floor(len(data_train)*test_size)\n",
    "        return data_train[0:range_split,:], data_train[range_split:,:]\n",
    "    \n",
    "    def select_fold(self,number_fold):\n",
    "        #return train,test\n",
    "\n",
    "        range_v = math.floor(len(self.data_train)/self.crossvalidation)\n",
    "        if number_fold == 1 :\n",
    "            return self.data_train[range_v:,:] , self.data_train[0:range_v,:] \n",
    "        elif number_fold == self.crossvalidation :\n",
    "            return self.data_train[0:range_v*(number_fold-1),:], self.data_train[range_v*(number_fold-1):,:] \n",
    "        else :\n",
    "            temp_data_first = self.data_train[0:range_v*(number_fold-1),:]\n",
    "\n",
    "            temp_data_second = self.data_train[range_v*(number_fold):,:]\n",
    "            final_con = np.concatenate((temp_data_first, temp_data_second))\n",
    "            return final_con , self.data_train[range_v*(number_fold-1):range_v*(number_fold),:]\n",
    "    \n",
    "    def split_features_class(self,v_input) :\n",
    "        #Vector feature , Vector class\n",
    "        return v_input[:,0:self.number_feature],v_input[:,-1]\n",
    "    \n",
    "    def calculate_RMSE(self,y_real,y_predict) :\n",
    "        if len(y_predict) != len(y_real) :\n",
    "            print(\"Cannot calculate RMSE\")\n",
    "            sys.exit()\n",
    "\n",
    "        temp_minus = y_real - y_predict\n",
    "        \n",
    "\n",
    "        error_all = []\n",
    "        for y in temp_minus :\n",
    "            error_all.append((y[0]*y[0] + y[1]*y[1])/2)\n",
    "        \n",
    "        return math.sqrt(np.sum(error_all)/len(error_all))\n",
    "\n",
    "    def predict(self,start_f_in,end_f_in) :\n",
    "       \n",
    "        return [ self.classifier_root_tree.predict( self.X_train[idx,start_f_in:end_f_in+1] ) for idx in range(self.X_train.shape[0]) ]\n",
    "        \n",
    "    def evaluate(self,weight_gammar,start_f_in,end_f_in,show_graph=False) :\n",
    "        \n",
    "        self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "        y_prediction = self.predict(start_f_in,end_f_in)\n",
    "        \n",
    "        \n",
    "        y_binary_prediction = []\n",
    "        for y in y_prediction :\n",
    "            \n",
    "            #[0,1] class 1\n",
    "            if y[0] < y[1] :\n",
    "                y_binary_prediction.append(1)\n",
    "            else :\n",
    "                #[0,1] class 1\n",
    "                y_binary_prediction.append(0)\n",
    "                \n",
    "        #Root mean squared error (RMSE)\n",
    "#         return self.accuracy_cal(self.y_train,y_binary_prediction),self.calculate_RMSE(self.y_train,y_prediction)\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_train,y_binary_prediction).ravel()\n",
    "        acc_train = self.accuracy_cal(self.y_train,y_binary_prediction,show_graph=show_graph)\n",
    "        return  [acc_train,tn, fp, fn, tp],self.calculate_RMSE(self.y_real_train_category,y_prediction)\n",
    "    \n",
    "    def predict_validateTest(self,start_f_in,end_f_in) :\n",
    "        return [ self.classifier_root_tree.predict( self.X_test[idx,start_f_in:end_f_in+1]  ) for idx in range(self.X_test.shape[0]) ]\n",
    "        \n",
    "    \n",
    "    def validate_test(self,weight_gammar,start_f_in,end_f_in,show_graph=False) :\n",
    "        self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "        \n",
    "        y_prediction = self.predict_validateTest(start_f_in,end_f_in)\n",
    "\n",
    "        y_binary_prediction = []\n",
    "        for y in y_prediction :\n",
    "            #[0,1] class 1\n",
    "            if y[0] < y[1] :\n",
    "                y_binary_prediction.append(1)\n",
    "            else :\n",
    "                #[0,1] class 1\n",
    "                y_binary_prediction.append(0)\n",
    "                \n",
    "        return self.accuracy_cal(self.y_test,y_binary_prediction,show_graph=show_graph),self.calculate_RMSE(self.y_real_test_category,y_prediction)\n",
    "    \n",
    "#     def predict_blindtest(self,start_f_in,end_f_in) :\n",
    "#         return [ self.classifier_root_tree.predict( self.X_blindtest[idx,start_f_in:end_f_in+1]  ) for idx in range(self.X_blindtest.shape[0]) ]\n",
    "        \n",
    "    \n",
    "#     def blind_test(self,weight_gammar,start_f_in,end_f_in) :\n",
    "#         self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "        \n",
    "#         y_prediction = self.predict_blindtest(start_f_in,end_f_in)\n",
    "        \n",
    "#         return self.accuracy_cal(self.y_blindtest,y_prediction),self.calculate_RMSE(self.y_blindtest,y_prediction)\n",
    "    \n",
    "    def calculate_evaluate_model(self,result_wanted_y,y_prediction,y_categorical) :\n",
    "        error_all = []\n",
    "        \n",
    "        y_binary_prediction = []\n",
    "        for (y1,y2) in zip(y_prediction,y_categorical) :\n",
    "            print(\"Predict %s , Actual %s \" % (\"[\" + str( y1[0]) +\",\"+ str(y1[1]) + \"]\",y2))\n",
    "            error_all.append(np.abs(np.sum(y1-y2)/2))\n",
    "            #[1,0] class 0\n",
    "            if  y1[0] < y1[1] :\n",
    "                y_binary_prediction.append(1)\n",
    "            else :\n",
    "                #[0,1] class 1\n",
    "                y_binary_prediction.append(0)\n",
    "            \n",
    "        print(\"MAE = \",np.sum(error_all)/len(error_all))\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(result_wanted_y,y_binary_prediction).ravel()\n",
    "        print(\"\\t-----------\\t \\t-----------\\t\")\n",
    "        print(\"|\\t tn - %s \\t|,|\\t fp - %s \\t|\" % (tn, fp))\n",
    "        print(\"|\\t fn - %s \\t|,|\\t tp - %s \\t|\" % (fn, tp))\n",
    "        print(\"\\t-----------\\t \\t-----------\\t\")\n",
    "        acc = accuracy_score( result_wanted_y,y_binary_prediction)\n",
    "        f1 = self.f1_cal(result_wanted_y, y_binary_prediction)\n",
    "\n",
    "        print(\"Acc = %s , f1 = %s\" % (acc,f1))\n",
    "        print(\"RMSE = \",self.calculate_RMSE(y_categorical,y_prediction))\n",
    "        \n",
    "        \n",
    "        plot_confusion_matrix(result_wanted_y, y_binary_prediction,classes = np.asarray(['normal', 'Signed']) )\n",
    "      \n",
    "        \n",
    "    def print_results(self,weight_gammar,start_f_in,end_f_in) :\n",
    "        self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "\n",
    "        #Config\n",
    "        result_wanted_y = self.y_test\n",
    "        result_wanted_X = self.X_test\n",
    "        \n",
    "        y_prediction = [ self.classifier_root_tree.predict( result_wanted_X[idx,start_f_in:end_f_in+1]  ) for idx in range(result_wanted_X.shape[0]) ]\n",
    "        \n",
    "        self.calculate_evaluate_model(result_wanted_y,y_prediction,self.y_real_test_category)\n",
    "    \n",
    "    def print_results_train(self,weight_gammar,start_f_in,end_f_in) :\n",
    "        self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "\n",
    "        #Config\n",
    "        result_wanted_y = self.y_train\n",
    "        result_wanted_X = self.X_train\n",
    "        \n",
    "        y_prediction = [ self.classifier_root_tree.predict( result_wanted_X[idx,start_f_in:end_f_in+1]  ) for idx in range(result_wanted_X.shape[0]) ]\n",
    "        \n",
    "        self.calculate_evaluate_model(result_wanted_y,y_prediction,self.y_real_train_category)\n",
    "\n",
    "    def set_allWeights(self,new_weight) :\n",
    "        self.classifier_root_tree.set_all_weights_wGammar(new_weight)\n",
    "        \n",
    "    def showAllWeights(self) :\n",
    "        self.classifier_root_tree.readable()\n",
    "    \n",
    "    def accuracy_cal(self,y_real,y_predict,show_graph=False) :\n",
    "        \n",
    "        acc = accuracy_score( y_real,y_predict)\n",
    "        if show_graph == True :\n",
    "            plot_confusion_matrix( y_real,y_predict,classes = np.asarray(['normal', 'Signed']) )\n",
    "        \n",
    "        return acc\n",
    "        \n",
    "    def f1_cal(self,y_real,y_predict) :\n",
    "        f_score = f1_score(y_real,y_predict,average='weighted')\n",
    "        \n",
    "        return f_score\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:32.322235Z",
     "start_time": "2020-02-26T10:57:32.314623Z"
    }
   },
   "outputs": [],
   "source": [
    "# logger = logging.getLogger()\n",
    "# fhandler = logging.FileHandler(filename='log_train_opt.log')\n",
    "# formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "# fhandler.setFormatter(formatter)\n",
    "# logger.addHandler(fhandler)\n",
    "# logger.setLevel(logging.INFO)\n",
    "import datetime \n",
    "class logging(object) :\n",
    "    \n",
    "    file_name = \"log_train_Finan_1.log\"\n",
    "    \n",
    "    def initial(file_name):\n",
    "        logging.file_name = file_name\n",
    "    \n",
    "    @staticmethod\n",
    "    def info(msg) :\n",
    "        f = open(logging.file_name, \"a+\")\n",
    "        f.write(\"%s - %s \\n\" % (datetime.datetime.now(),msg) )\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:32.335781Z",
     "start_time": "2020-02-26T10:57:32.328625Z"
    }
   },
   "outputs": [],
   "source": [
    "def getFileNameSwarmTrain(val):\n",
    "    if val == 0 :\n",
    "        return \"FP\"\n",
    "    elif val == 1 :\n",
    "        return \"FF\"\n",
    "    elif val == 2 :\n",
    "        return \"GWO\"\n",
    "    elif val == 3 :\n",
    "        return \"CSO\"\n",
    "    else:\n",
    "        return \"MFO\"\n",
    "    \n",
    "    ## 0 - Flower Pollination\n",
    "## 1 - firefly\n",
    "## 2 - GWO\n",
    "## 3 - CSO\n",
    "## 4 - Moth-Flame Optimization (MFO) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:32.582081Z",
     "start_time": "2020-02-26T10:57:32.338830Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from NiaPy.algorithms.basic import FlowerPollinationAlgorithm\n",
    "from NiaPy.algorithms.basic import FireflyAlgorithm\n",
    "from NiaPy.algorithms.basic import GreyWolfOptimizer\n",
    "from NiaPy.task.task import StoppingTask, OptimizationType\n",
    "\n",
    "def count_weight_from_tree(shape_OfTree):\n",
    "    count_weight = 0\n",
    "    for each_layer in shape_OfTree :\n",
    "        list_all_weight_gammar = [ w + 1 for w in each_layer ]\n",
    "        count_weight = count_weight + np.sum(list_all_weight_gammar)\n",
    "\n",
    "    return count_weight\n",
    "\n",
    "# tree_shape = [[8,8,8,8,8,8,8,8],[2,3,3],[2,1],[2]]\n",
    "#[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4]\n",
    "tree_shape = [[8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8],\n",
    "              [3,2,2,4,4,2,1,4,2,1,2,2,5,1,1,2],\n",
    "              [2,5,3,2,2,2],[6,6]]\n",
    "n_weight_gammar = count_weight_from_tree(tree_shape) \n",
    "\n",
    "try :\n",
    "    name_file_train = \"./Final_Data_Train_8_MultiNC.csv\"\n",
    "    all_data_train = pd.read_csv(name_file_train,index_col=\"index_firm\")\n",
    "except :\n",
    "    name_file_train = \"../Final_Data_Train_8_MultiNC.csv\"\n",
    "    all_data_train = pd.read_csv(name_file_train,index_col=\"index_firm\")\n",
    "    \n",
    "for col in all_data_train.columns[:-1]:\n",
    "    all_data_train[col] = [x if x > 0 else 0.00000001 for x in all_data_train[col]]\n",
    "    all_data_train[col] = [x if x < 1 else 0.99999999 for x in all_data_train[col]]\n",
    "\n",
    "## Arrange columns ##\n",
    "list_ratio = [\"CR\",\"QR\",\"CashR\",\"CA2NA\",\"NWC2TA\"]\n",
    "list_ratio = list_ratio + [\"ART\",\"ACP\",\"ITR\",\"ASP\",\"FA2NW\",\"FAT\",\"NWCTR\",\"TAT\",\"ETR\",\"CA2TR\",\"APT\",\"APP\",\"CC\"]\n",
    "list_ratio = list_ratio + [\"ROA\",\"GPM\",\"NPM\",\"ROE\",\"P2NWC\",\"ROS\",\"OE2NS\"]\n",
    "list_ratio = list_ratio + [\"CA2TA\",\"LA2TA\",\"I2CA\",\"CCE2CA\"]\n",
    "list_ratio = list_ratio + [\"D2TAR\",\"D2E\",\"TL2NW\",\"STD2TD\",\"ICR\",\"RE2S\"]\n",
    "list_ratio = list_ratio + [\"AGR\",\"SGR\",\"NPGR\"]\n",
    "new_name_columns = []\n",
    "for idx , val in enumerate(list_ratio) :\n",
    "    for i in range(0,tree_shape[0][0]) :\n",
    "        new_name_columns.append(list_ratio[idx] + \"-Last\" + str(i))\n",
    "new_name_columns = new_name_columns + [\"class\"]\n",
    "\n",
    "all_data_train = all_data_train[new_name_columns]\n",
    "\n",
    "\n",
    "##### Columns drop #######\n",
    "# columns_drop = []\n",
    "# for ratio in [\"TAT\",\"R2NWC\",\"RE2S\",\"S2NW\",\"emscore\",\"CR\",\"QR\"] :\n",
    "#     for idx in range(0,8) :\n",
    "#         columns_drop = columns_drop + [ratio+\"-Last\"+str(idx)]\n",
    "\n",
    "# all_data_train = all_data_train.drop(columns_drop, axis=1)\n",
    "\n",
    "##################### Store Best fold #########################\n",
    "\n",
    "best_sol_cross = []\n",
    "acc_cross = 0\n",
    "error_cross = 0\n",
    "best_fold = 1\n",
    "best_cross_train = 0\n",
    "best_acc_train = 0\n",
    "\n",
    "\n",
    "############## Start & End pos feature #######################\n",
    "pos_start = 0\n",
    "pos_end =len(all_data_train.columns)-2\n",
    "\n",
    "\n",
    "\n",
    "################# In Aggregation algor ##################\n",
    "number_feature = pos_end + 1\n",
    "crossvalidation = 2\n",
    "\n",
    "######### Select algorithm to train ############\n",
    "## 0 - Flower Pollination\n",
    "## 1 - firefly\n",
    "## 2 - GWO\n",
    "\n",
    "_select_sw_train = 2\n",
    "\n",
    "\n",
    "\n",
    "############### Config Params ###############\n",
    "\n",
    "train_params = {}\n",
    "train_params[\"NP\"] = 10\n",
    "n_generations = 2\n",
    "\n",
    "train_params[\"nFES\"] = train_params[\"NP\"] * n_generations\n",
    "train_params[\"dimension\"] = n_weight_gammar\n",
    "\n",
    "\n",
    "######Flower Pollination##########\n",
    "if _select_sw_train == 0 :\n",
    "    train_params[\"p\"] = 0.1\n",
    "\n",
    "\n",
    "########## Firefly algor ##############\n",
    "if _select_sw_train == 1 :\n",
    "    train_params[\"alpha\"] = 1\n",
    "    train_params[\"betamin\"] = 1.5\n",
    "    train_params[\"gamma\"] = 1.5\n",
    "    \n",
    "###### Grey Wolf Optimization ##########\n",
    "if _select_sw_train == 2 :\n",
    "    train_params[\"A_val\"] = 2\n",
    "    train_params[\"C_val\"] = 2\n",
    "\n",
    "\n",
    "# 'D' = D {array} or {int} â€“ Shape of return random numbers \n",
    "name_file_fold1 = \"log_t_f_38s_8Q_NiaPy_\"+getFileNameSwarmTrain(_select_sw_train)+\"_\"+getFileNumber()+\"_fold1.log\"\n",
    "name_file_fold2 = \"log_t_f_38s_8Q_NiaPy_\"+getFileNameSwarmTrain(_select_sw_train)+\"_\"+getFileNumber()+\"_fold2.log\"\n",
    "\n",
    "def trainWithFold(nFold,nameSaveFile) :\n",
    "    global best_sol_cross,acc_cross,error_cross,best_fold,best_cross_train,best_acc_train,name_file_train\n",
    "    \n",
    "    logging.initial(nameSaveFile)\n",
    "    StoreOptimization.static_appendString(\"You're going to train with file \" + name_file_train + \" & Saved = \"+logging.file_name)\n",
    "\n",
    "    n_fold = nFold\n",
    "\n",
    "    #nGEN , nFES\n",
    "    StoreOptimization.static_clearAll()\n",
    "    StoreOptimization.static_N_Gen(1000)\n",
    "\n",
    "    task = StoppingTask(D=train_params[\"dimension\"], nFES=train_params[\"nFES\"], optType=OptimizationType.MINIMIZATION, benchmark=BenchmarkAllTree(tree_shape,pos_start,pos_end,n_fold))\n",
    "\n",
    "    if _select_sw_train == 0 :\n",
    "    #flower pollination\n",
    "        StoreOptimization.static_appendString(\"## New training FP with D=%s,NP = %s, nFES=%s, p=%s, with fold = %s \" % (train_params[\"dimension\"],train_params[\"NP\"], train_params[\"nFES\"], train_params[\"p\"],n_fold))\n",
    "        algorithm = FlowerPollinationAlgorithm(NP = train_params[\"NP\"], p=train_params[\"p\"])\n",
    "\n",
    "    if _select_sw_train == 1 :\n",
    "    #firefly\n",
    "        StoreOptimization.static_appendString(\"## New training FF with D=%s,NP = %s, nFES=%s, alpha=%s, betamin=%s, gamma = %s with fold = %s \" % (train_params[\"dimension\"],train_params[\"NP\"], train_params[\"nFES\"], train_params[\"alpha\"],train_params[\"betamin\"],train_params[\"gamma\"] ,n_fold))\n",
    "        algorithm = FireflyAlgorithm(NP = train_params[\"NP\"], alpha=train_params[\"alpha\"], betamin=train_params[\"betamin\"], gamma=train_params[\"gamma\"])\n",
    "\n",
    "\n",
    "    #GWO\n",
    "    if _select_sw_train == 2 :\n",
    "        StoreOptimization.static_appendString(\"## New training GWO with D=%s,NP = %s, nFES=%s, A = %s, C = %s with fold = %s \" % (train_params[\"dimension\"],train_params[\"NP\"], train_params[\"nFES\"],train_params[\"A_val\"],train_params[\"C_val\"] ,n_fold))\n",
    "        algorithm = GreyWolfOptimizer(NP=train_params[\"NP\"])\n",
    "\n",
    "\n",
    "    try:\n",
    "        best = algorithm.run(task=task)\n",
    "    except Exception as e:\n",
    "        set_trace()\n",
    "        print(\"error\",e)\n",
    "    print(\"Optimal Weight and Gammar are : \")  \n",
    "#     if best[1] != StoreOptimization.store_best_error :\n",
    "#         print(\"Best not equal we find\")\n",
    "#         raise\n",
    "    best_solution = StoreOptimization.store_best_weights\n",
    "    best = StoreOptimization.store_best_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\":: Finished Fold ::\")\n",
    "    model_check = fuzzy_aggregation(tree_shape=tree_shape,current_fold_train=n_fold,ignore_writeClass=False)\n",
    "    acc_output,error_output = model_check.validate_test(best_solution,pos_start,pos_end,show_graph=False)\n",
    "    acc_from_train,_ = model_check.evaluate(best_solution,pos_start,pos_end,show_graph=True)\n",
    "\n",
    "    print(\"!! Train error = \",best,\" Train Acc = \",acc_from_train[0],\" & Validate Test = \",error_output,\" & Validate accuracy = \",acc_output)\n",
    "    StoreOptimization.static_appendString(\"!! Train error = \"+str(best)+\" Train Acc = \"+str(acc_from_train[0])+\" & Validate Test = \"+str(error_output)+\" & Validate accuracy = \"+str(acc_output))\n",
    "    \n",
    "    tn, fp, fn, tp = acc_from_train[1],acc_from_train[2],acc_from_train[3],acc_from_train[4]\n",
    "    StoreOptimization.static_appendString(\"Trained tn, fp, fn, tp : %s %s %s %s\" % (tn, fp, fn, tp))\n",
    "    \n",
    "    if acc_output > acc_cross :\n",
    "        best_sol_cross = best_solution\n",
    "        acc_cross = acc_output\n",
    "        error_cross = error_output\n",
    "        best_fold = n_fold\n",
    "        best_cross_train_err = best\n",
    "        best_acc_train = acc_from_train[0]\n",
    "    elif acc_output == acc_cross :\n",
    "        if error_output < error_cross :\n",
    "            best_sol_cross = best_solution\n",
    "            acc_cross = acc_output\n",
    "            error_cross = error_output\n",
    "            best_fold = n_fold\n",
    "\n",
    "            best_cross_train_err = best\n",
    "            best_acc_train = acc_from_train[0]\n",
    "            \n",
    "    StoreOptimization.static_appendString(\"** Best Weights now = \" + str(best_sol_cross))\n",
    "    StoreOptimization.static_appendString(\"** Best n_Fold now = \" + str(best_fold))\n",
    "    StoreOptimization.static_appendString(\"** Best Train now = \" + str(best_cross_train_err))\n",
    "    StoreOptimization.static_appendString(\"** Best Accuracy Train now = \" + str(best_acc_train))\n",
    "    StoreOptimization.static_appendString(\"** Best validation error now = \" + str(error_cross))\n",
    "    StoreOptimization.static_appendString(\"** Best Accuracy Validate now = \" + str(acc_cross))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:57:54.440721Z",
     "start_time": "2020-02-26T10:57:32.597617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're going to train with file ./Final_Data_Train_8_MultiNC.csv & Saved = log_t_f_38s_8Q_NiaPy_GWO_1_fold1.log\n",
      "Y Train count abnormal :  6\n",
      "6 114\n",
      "108 114\n",
      "## parameter weight output class zero [0.2,0.1] \n",
      "## parameter weight output class one [0.1,0.9473684210526316] \n",
      "## New training GWO with D=432,NP = 10, nFES=20, A = 2, C = 2 with fold = 1 \n",
      "fold 1 best at nFES 1 error 0.2085868388958662 (108,0,0.9473684210526315)\n",
      "Optimal Weight and Gammar are : \n",
      ":: Finished Fold ::\n",
      "Y Train count abnormal :  6\n",
      "6 114\n",
      "108 114\n",
      "## parameter weight output class zero [0.2,0.1] \n",
      "## parameter weight output class one [0.1,0.9473684210526316] \n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEYCAYAAADlIcXmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHyRJREFUeJzt3Xu8VXWZx/HPFwhFQTFR1IMNKIiKIgqoZRleIrw7440mFZU0S/OWGWk3Z3K0rNTU0aE0KSdBLcNLhUXhbRTkInhXxAgIVETRxETgmT/WOrY5wdmbfTlr7bO/b17rdfZae+21nuP29Zzfbf1+igjMzKw8HbIOwMysnjmJmplVwEnUzKwCTqJmZhVwEjUzq4CTqJlZBZxEzcwq4CRqZlYBJ1Ezswp0yjqALKlTl1DnblmHYa3Yc5ePZB2CFTFz5oylEbFVNa/ZcbN/iVj1btHz4t3XJkXEiGree0M1dhLt3I2N+h+fdRjWikemXpd1CFZElw9pfrWvGav+zkY7jyx63t9nXduj2vfeUA2dRM0spwRIWUdREidRM8sn1UeXjZOomeWQoEPHrIMoiZOomeWTq/NmZmUSrs6bmZVPLomamVXEJVEzs3K5Y8nMrHweJ2pmVqE6qc7XR5Rm1mCUJNFiW7GrSDdLelXSUwXHPizp95JeTH9ukR6XpB9JmitpjqS9SonUSdTM8qmDim/F3QK0nKBkDDA5IvoBk9N9gEOAful2BnBDSWGWcpKZWZsSScdSsa2IiHgQWNbi8FHAuPT1OODoguM/i8RjQHdJ2xa7h9tEzSyHVGqbaA9J0wv2x0bE2CKf6RkRi9PXS4Ce6esmYEHBeQvTY4tphZOomeVTab3zSyNiSLm3iIiQFOV+HlydN7O8qkLH0nq80lxNT3++mh5fBGxfcF6v9FirnETNLH+kqrSJrsfdwKj09ShgYsHxk9Ne+n2B5QXV/vVydd7M8qkKg+0l3QYMI2k7XQh8C7gCuF3SaGA+0Ly8xW+AQ4G5wArg1FLu4SRqZjlUcsdSqyLiM+t566B1nBvAWRt6DydRM8snP/ZpZlYmzydqZlYJz+JkZlYZl0TNzCrgNlEzszKpOr3zbcFJ1MxySR2cRM3MypJMbO/qvJlZeZRudcBJ1MxySC6JmplVwknUzKwCHdyxZGZWJreJmpmVT24TNTOrjJOomVkF3CZqZlYut4mamVXG1XkzszK5Y8nMrEJOomZm5RKog5OomVnZXBI1M6uAk6iZWZncsWRmVqn6yKFOomaWQ/ITS2ZmFamX6nx9pHr7wI3f+izzJ1/O9Dsu/uDYFpttwr03nM2TE7/JvTecTfduXQDYrOvG3Hn155k6YQwz7ryEk47cN6uwDbh/0u8YOKA/A3buy5XfuyLrcPJPJWw50G6TqKQpkoZkHUe1/fyexzjqrOvXOnbhqZ9iyrTn2f2o/2DKtOe58NThAHz++P15bt4S9jnhCj59+jVcccG/8qFOHbMIu+GtXr2a8845i4n3/JZZc57hjvG38ewzz2QdVq5JKrrlQS6TqCQ3M6zHIzNfYtnyFWsdO3zYQG69ZyoAt94zlSMOGAhAAF033QiATbtsxBvLV7Bq9Zo2jdcSj0+bxo479qXPDjvQuXNnjjthJPfeMzHrsHJLEh06dCi65UHNopDUW9Kzkn4s6WlJ90vqImmQpMckzZF0l6Qt0vOnSLpa0nTgXEm3SLohPXeepGGSbk6veUvBfW6QND29x6W1+n3ybOstu7Fk6VsALFn6Fltv2Q2AG8c/wM59tmHe/Zcx/Y6LufDKO4mILENtWH/96yJ69dr+g/2mpl4sWrQow4jyr1olUUnnp/nhKUm3SdpYUh9JUyXNlTRBUudy46x1Ku8HXB8RA4A3gWOAnwFfjYiBwJPAtwrO7xwRQyLiB+n+FsBHgfOBu4GrgAHA7pIGpedcEhFDgIHAJyUNbC0gSWekSXd6rHq3Or9lzjTnyU99bBfmPL+QHYZfwj4jL+eqMcfRbdONsw3OrFRVaBOV1AScAwyJiN2AjsBI4LvAVRHRF3gDGF1umLVOoi9HxBPp6xnAjkD3iHggPTYO2L/g/AktPn9PJEWnJ4FXIuLJiFgDPA30Ts85XtJMYBZJgt21tYAiYmyaqIeoU5dyf69cefX1t9mmx2YAbNNjM15b9jYAJx25LxP/OBuAeQuW8udFr9O/d8/M4mxk223XxMKFCz7YX7RoIU1NTRlGlH9VbBPtBHRJmwk3ARYDBwJ3pu+PA44uN85aJ9H3Cl6vBroXOf+d9Xx+TYtrrQE6SeoDXAgclJZs7wMarqh13wNPcuIR+wBw4hH7cO+UOQAsWPIGw/buD8DWH+7GTr178vKipZnF2ciGDB3K3Lkv8ueXX2blypXcMWE8hx1+ZNZh5ZdKTqI9mmuW6XZG4WUiYhHwfeAvJMlzOUmB7s2IWJWethAo+y9aW3fgLAfekPSJiHgIOAl4oMhnWrMZSeJdLqkncAgwpeIoc2zc5afwicH96NG9K3N/95/8542/4fs//T23fvc0Rh39Uf6yeBknXnQzAFf8+HeMvfREHr/9YiS45JqJvP5my79T1hY6derEVddcxxGHfZrVq1cz6pTT2HXAgKzDyi0hOpQ2i9PStDlv3ddJ+lyOAvqQNCneAYyoSpCpLHrBRwE3StoEmAecWu6FImK2pFnAc8AC4JHqhJhfo752yzqPH3rmtf90bPFryznii9ev42zLwohDDmXEIYdmHUbdqNIIpoNJmhVfS66pXwH7Ad0ldUpLo72Asnv5apZEI+LPwG4F+98vePufRn1HxLAW+6e0cq1T1vW6teuZWX2p0jjQvwD7poW2d4GDgOnAn4BjgfEkBbuyx5vlY6CVmVkhJSXRYlsxETGVpANpJkkHdQdgLPBV4AJJc4EtgZvKDdWD2s0sdwR07Fid+nxEfIu1h1JC0pS4dzWu7yRqZrmUl8c6i3ESNbP8KbG6ngdOomaWO8IlUTOzCuRnlqZinETNLJdKHGyfOSdRM8sft4mamZXPbaJmZhWqkxzqJGpm+eQ2UTOzcsnVeTOzsiVtollHURonUTPLIY8TNTOrSJ3kUCdRM8shuWPJzKxsHidqZlYhJ1EzswrUSQ51EjWzfHJJ1MysTFLJSyZnzknUzHKpTgqiTqJmlk8d6iSLOomaWS7VSQ5dfxKVtFlrH4yIt6ofjplZkkA7toM20aeBIBn32qx5P4CP1DAuM2twdd87HxHbt2UgZmaF6iSH0qGUkySNlHRx+rqXpMG1DcvMGpkAlfAvD4omUUnXAQcAJ6WHVgA31jIoM7MOKr7lQSm98x+LiL0kzQKIiGWSOtc4LjNrZO1ssP37kjqQdCYhaUtgTU2jMrOGJupnnGgpbaLXA78EtpJ0KfAw8N2aRmVmDU8qvuVB0ZJoRPxM0gzg4PTQcRHxVG3DMrNGV60hTpK6Az8BdiOpUZ8GPA9MAHoDfwaOj4g3yrl+Sb3zQEfgfWDlBnzGzKwszYPti20lugb4XUTsDOwBPAuMASZHRD9gcrpfllJ65y8BbgO2A3oBv5D0tXJvaGZWCpWwFb2GtDmwP3ATQESsjIg3gaOAcelp44Cjy42zlI6lk4E9I2JFGtRlwCzg8nJvamZWTInV+R6Sphfsj42IsQX7fYDXgJ9K2gOYAZwL9IyIxek5S4Ce5cZZShJd3OK8TukxM7OaSHrnSzp1aUQMaeX9TsBewJciYqqka2hRdY+IkBTlxtraBCRXkTTCLgOeljQp3R8OPF7uDc3MilLV1p1fCCyMiKnp/p0kSfQVSdtGxGJJ2wKvlnuD1kqizT3wTwP3FRx/rNybmZmVqhqD7SNiiaQFkvpHxPPAQcAz6TYKuCL9ObHce7Q2AclN5V7UzKwSG1CdL8WXgP9Nn7ScB5xK0ql+u6TRwHzg+HIvXrRNVNKOwGXArsDGzccjYqdyb2pmVky1xolGxBPAutpND6rG9UsZ83kL8FOSPw6HALeTDFI1M6uZagxxagulJNFNImISQES8FBFfJ0mmZmY1UeXB9jVVyhCn99IJSF6SdCawCOhW27DMrNHV/cz2Bc4HNgXOIWkb3Zzk2VMzs5qpkxxa0gQkzeOr3uYfEzObmdWMUN1MhdfaYPu7SOcQXZeI+LeaRGRmlqOp7opprSR6XZtFkZGBO2/P5AevzjoMM1uHjnWSRVsbbD+5LQMxM2sm2lfHkplZm8vJCKainETNLJfaXRKVtFFEvFfLYMzM4B+D7etBKTPb7y3pSeDFdH8PSdfWPDIza2j1slBdKY99/gg4HHgdICJmAwfUMigza2zNSyYX2/KglOp8h4iY36KnbHWN4jEzA+pnRcxSkugCSXsDIakjydx8L9Q2LDNrdDkpaBZVShL9AkmV/iPAK8Af0mNmZjUh5WeWpmJKeXb+VWBkG8RiZvaBOsmhJc1s/2PW8Qx9RJxRk4jMrOE1dyzVg1Kq838oeL0x8K/AgtqEY2aWqJMcWlJ1fq2lQCT9HHi4ZhGZmakdVefXoQ/Qs9qBmJk1E+1gFqdmkt7gH22iHYBlwJhaBmVm1i5KokpG2O9Bsq4SwJqIWO9EzWZm1VIvU+G1+lBAmjB/ExGr080J1MxqLumdL77lQSlPVj0hac+aR2Jm1qw9LJksqVNErAL2BB6X9BLwDskfiYiIvdooRjNrMM0l0XrQWpvoNGAv4Mg2isXM7AN10iTaahIVQES81EaxmJmlRAfqI4u2lkS3knTB+t6MiB/WIB4zs3ShuqyjKE1rSbQj0BXq5M+BmbUfgk510ijaWhJdHBH/0WaRmJmlqlkSTedBng4siojDJfUBxgNbAjOAkyJiZbnXb22IU338GTCzdqmKy4OcCzxbsP9d4KqI6Au8AYyuKM5W3juokgubmVWiGgvVSeoFHAb8JN0XcCBwZ3rKOODoSuJcb3U+IpZVcmEzs3JJJU9A0kPS9IL9sRExtmD/auAioFu6vyXwZjoGHmAh0FRJrOXM4mRmVnMlVtaXRsSQdX5eOhx4NSJmSBpWvcjW5iRqZrlTpZnt9wOOlHQoyYTymwHXAN0LnsjsxT8mWCpLvaxKamYNRiVsrYmIr0VEr4joTbJO3B8j4rPAn4Bj09NGARMridNJ1MxyqRodS+vxVeACSXNJ2khvqiROV+fNLHeEqjqzfURMAaakr+cBe1fr2k6iZpZL9TIps5OomeVSfaRQJ1EzyyO5JGpmVrZ2tdqnmVkW6iOFOomaWU7VSUHUSdTM8kfQLma2NzPLjEuiZmZl26D5QjPlJGpmuePqvJlZJSp7Nr5NOYmaWS45iZqZVUCuzpuZlcdPLJmZVahOcqgnZW4vlr/5JqeeeAL77rUbHx28O49PfTTrkKyF+yf9joED+jNg575c+b0rsg4n91TCvzyoeRKVdImkpyXNkfSEpH0k/UTSrjW+77clXVjLe+TJxRedz4EHD+exmU/xwKMz2Kn/LlmHZAVWr17NeeecxcR7fsusOc9wx/jbePaZZ7IOK7eSNZaKb3lQ0+q8pI8ChwN7RcR7knoAnSPic7W8b6N5a/lyHv2/h7nuf24GoHPnznTu3DnjqKzQ49OmseOOfemzww4AHHfCSO69ZyK77FrTskT9Uv0Mtq91SXRbkiVN3wOIiKUR8VdJUyQNAZA0WtILkqZJ+rGk69Ljt0j6kaT/kzRPUvPCUkj6iqTH09LtpQXHL0mv9TDQv8a/W27Mn/8yW/bowZfOHM0B+w3h3LPO4J133sk6LCvw178uolev7T/Yb2rqxaJFFS0y2e5VulBdW6l1Er0f2D5NbP8t6ZOFb0raDvgGsC/J8qY7t/j8tsDHSUqzV6SfGQ70I1kjZRAwWNL+kgaTrOg3CDgUGLqugCSdIWm6pOmvL11apV8zW6tWrWLOE7M49XOf50+PTGfTTTflRz/8XtZhmZWtecnkYlse1DSJRsTfgMHAGcBrwARJpxScsjfwQEQsi4j3gTtaXOLXEbEmIp4BeqbHhqfbLGAmSeLtB3wCuCsiVkTEW8Dd64lpbEQMiYghW/boUZXfM2vbNfViu6ZeDB66DwBHHHUMs5+YlXFUVmi77ZpYuHDBB/uLFi2kqakpw4jyzyXRVESsjogpEfEt4GzgmA34+HsFr1Xw8/KIGJRufSOioiVP613PntvQ1NSLF194HoAHH/gj/Xd2x1KeDBk6lLlzX+TPL7/MypUruWPCeA47/Misw8q3OsmiNU2ikvpL6ldwaBAwv2D/ceCTkraQ1InSEuwk4DRJXdN7NEnaGngQOFpSF0ndgCOq81vUh8u/fzVnfu5k9t93T56aM5vzLxyTdUhWoFOnTlx1zXUccdinGbT7Lhxz3PHsOmBA1mHlWr1U52s92L4rcK2k7sAqYC5J1f5OgIhYJOm/gGnAMuA5YHlrF4yI+yXtAjyaLmT1N+DEiJgpaQIwG3iVJEE3jN0HDmLyg1OzDsNaMeKQQxlxyKFZh1E38pEii6tpEo2IGcDH1vHWsILXv4iIsWlJ9C7g1+lnT2lxra4Fr68BrlnH/S4DLqs4cDPLXp1k0Tw8sfRtSU8ATwEvkyZRM2tcSZNnfTyxlPmz8xHRME8VmVmJcvREUjGZJ1Ezs3VyEjUzK1d+quvFOImaWS7lZARTUXnoWDIzW0sp4+xLybGStpf0J0nPpLPJnZse/7Ck30t6Mf25RbmxOomaWS5JKrqVYBXw5YjYlWSOjrPSaTjHAJMjoh8wOd0vi5OomeWSVHwrJiIWR8TM9PXbwLNAE3AUMC49bRxwdLlxuk3UzHKpxCbRHpKmF+yPjYix67ye1BvYE5gK9IyIxelbS/jHBEcbzEnUzPKn9AlGlkbEkKKXS+ba+CVwXkS8VdgUEBEhKcqM1EnUzPKneT7RqlxL+hBJAv3fiPhVevgVSdtGxGJJ25LMt1EWt4maWS5VqXdewE3AsxHxw4K37gZGpa9HARPLjdMlUTPLp+oURPcDTgKeTOfoALiYZKWM2yWNJpme8/hyb+Akama5VI0nliLiYdafjg+q+AY4iZpZTtXLE0tOomaWS06iZmZlap5PtB44iZpZ/pT4RFIeOImaWS7VSQ51EjWznKqTLOokamY5lJ8lkYtxEjWz3Cn90fnsOYmaWT7VSRZ1EjWzXPIQJzOzCnjJZDOzcnmcqJlZpeojizqJmlnuCJdEzcwqUic51EnUzPLJg+3NzCpRHznUSdTM8qlOcqiTqJnljzzEycysMqqTLOokama5VB8p1EnUzHKqTgqiTqJmlkfyBCRmZuXyE0tmZhVyEjUzq4Cr82Zm5fI4UTOz8nmNJTOzCnmwvZlZBeokh9Ih6wDMzNZFJWwlXUcaIel5SXMljal2nE6iZpZPVciikjoC1wOHALsCn5G0azXDdBI1s1xSCf9KsDcwNyLmRcRKYDxwVDXjbOg20dmzZi7t0e1D87OOo8p6AEuzDsJa1d6+o3+p9gVnzZwxaZPO6lHCqRtLml6wPzYixhbsNwELCvYXAvtUI8ZmDZ1EI2KrrGOoNknTI2JI1nHY+vk7Ki4iRmQdQ6lcnTez9mwRsH3Bfq/0WNU4iZpZe/Y40E9SH0mdgZHA3dW8QUNX59upscVPsYz5O2ojEbFK0tnAJKAjcHNEPF3Neygiqnk9M7OG4uq8mVkFnETNzCrgJNqOSPL3mWP+ftonf6nthKShwKmSNsk6FvtnkrYCTpe0bdaxWHU5ibYfXYEzgeMldck6GPsnQ9PtWElbZx2MVY+TaDsREX8CvgKMAv7diTRfIuI3wAxgEDBS0oczDsmqxONE65gkRcEYtYiYIimAS9P3fxER72YWoH1A0iHAScBikgHfkjQ+Il7JNjKrlMeJ1qnCBCrpOOAjwAMRMV3SR4HLgXHAhIhYkWGoDU/SFsBtwMURMVPS8cD+wHPAbRHxeqYBWkVcna9TBQn0bOA8YA3wc0lfBKYCY4BzgGMyC9KavUMy++VAgIi4naRE+iXg5PRxRKtTTqJ1TNJewAHAQcDKdPsEcHZEPAacDjyYXYSNSeniQJJ2kNQXCOAWYEdJ+6en/R54HvhtOs+l1SlX5+tIyzbQ9Ng2wB7AVyLi4LQkejHwzYi4OYs4DSQdDVwIzAdeAx4G+pNU4xemP8+OiEmZBWlV4ZJoHSmowh8i6ShJG0fEEuDDwJvpaa8AjwL3ZRRmw0tLn+cBw4E5JJMA3wXcCHyZpHZwkhNo++CSaB1o0Yn0OZK2tLeBmcDNwKskHRfvksydeExEPJdRuA0vXcNnFPACMBo4OSLmShocETOyjc6qzSXRnGuRQLsA25K0e34CeB84MT32GZJ2tyOcQNtWQRto85DBl0mWzDgfOC1NoJ8GbpC0/XouY3XKJdEca5FALwIOBHYCLoqIOyVtCVwCbAJcW+15Eq10kg4HRgDvR8T5ko4BDgOWkHQgfZXke7s3wzCtBlwSzbGCBDqMJIF+lWT5129KOjAdX/hfwDLa18JndUXSIOA7wENAX0lTIuKXwA0kIyb6AudGxL3NpVZrP1wSzaEWJdBhJG2gr0TEF9NjpwFnA1+LiEmSOkTEmswCbmCSdicZjzs3Ir6bHpsIbBYRB6T7HSNidYZhWg25JJozLRLoycBuwDPA1pI+LqlTOnTpJ8A3PGtTdiRtDLwHbAXsLqk/QEQcBayUNCs91SWVdswl0ZxKH938NjAiIkLSZcDmwATg0XTtmM0jYnmWcTaa5j9yacL8Msk8BRuRfFezgHsj4sX0XPfGNwCXRHNGiYEki5ktA5pnY7o03R8N7A3gBNq2ChLooSRtoB8HvgGsBi4jeazzWEk7ATiBNgYn0Rwo7GyIxBzgeyRjPgdL6pw+GngZMBeYl02kjal5Rvo0ge4CXEuSRC8ieQb+ayTjdn9A0vyyKqNQLQOuzueIpM8C/UgGz99KMkTmNJJS6DQ/Y9320nGdh5Istfu+pP1IZmM6LH1/T5IZsxYAXwfe8PfUWFwSzQlJZ5H0wr9B8oz1pHQbB3wfGJxddI0pnYG+CXgc6C5pc+AJYGNJpwNExCxgOsncvCcAa7yWUmPxl52RgqdcmqvyuwPnRMQ1EXEucA/wvYi4Ffg5sCibSBuTpJ1JnnHfhmQSkXEkNYIuJGN195L0g3QI2hHA0yTPyK/2cLPG4iSagRazMfWT9CGgFzCs4LR7Sb+fiLg+Iv7StlE2Lkm9gTuBKyPi1+lDDScDvUmeiX+apONvW5LmllNJJn3ZEujW9hFblrw8SBtrMQ60eULlu4DZwDmSlqbjQHcHekvqDixvOQWe1dQBwOSIuCmtmu9J8iz8Q8CxJGNDfxYR/y6pI8l8rleSTDTyVlZBWzacRNtYQQI9kmRIzKdJpkzbDPgD8J20s+IA4ISIeHN917KamQd8Lp005ASSKvweJE0sK0gmfekl6evpeN2tgWObx4daY3HvfAYkNZFU//4QEadJ2ohkGY/tgS1IqorLvfZONtKnwM4ATiEZUnYN8BRJdX4kydNi3SJiZkYhWo64TTQDEbGIpBo/QtLIiHgPGE8yA/oaYJkTaHYiYkVEXA0cGBHHRsRDEfEGyRNjnyT5fpxADXB1PjMR8StJ7wGXSyIixku6Bdg0It7OODwDImIZQNrx9ymS8aAX+w+cFXISzVBE3CdpDTBW0qqIuJPkyRfLiTSB7g1cAHw9Irzsiq3FbaI5IOlTwEsR4cc5cyhNpFtGxJJ1LRZojc1J1MysAu5YMjOrgJOomVkFnETNzCrgJGpmVgEnUTOzCjiJ2lokrZb0hKSnJN1RyUJ4koZJujd9faSkMa2c213SF8u4x7clXVjq8Rbn3CLp2A24V29JT21ojNa+OYlaS+9GxKCI2I1kzfQzC99M14Da4P9vIuLuiLiilVO6AxucRM2y5iRqrXkI6JuWwJ6X9DOSiTi2lzRc0qOSZqYl1q4AkkZIek7STODfmi8k6RRJ16Wve0q6S9LsdPsYcAWwY1oKvjI97yuSHpc0R9KlBde6RNILkh4mWQWgVZJOT68zW9IvW5SuD5Y0Pb3e4en5HSVdWXDvz1f6H9LaLydRWydJnYBDgCfTQ/2A/46IAcA7JOsJHRwRe5Esj3GBknXYf0wy0/tgklnh1+VHwAMRsQewF8kkx2NIntoaFBFfkTQ8vefewCCSBfv2lzSYZCalQSRrHw0t4df5VUQMTe/3LMmKqc16p/c4DLgx/R1Gk8yiNTS9/umS+pRwH2tAfnbeWuoi6Yn09UPATcB2wPyIeCw9vi+wK/BIurpJZ5Kp/XYGXi5Yd/1WkinlWjqQZKZ4ImI1sFzSFi3OGZ5us9L9riRJtRtwV0SsSO9xdwm/026SvkPSZNCVZO2qZreny3m8KGle+jsMBwYWtJdunt77hRLuZQ3GSdRaejciBhUeSBPlO4WHgN9HxGdanLfW5yok4PKI+J8W9zivjGvdAhwdEbMlncLay7C0fO450nt/KSIKk23zsiFma3F13srxGLCfpL4AkjaVtBPwHMmSJjum531mPZ+fDHwh/WxHJatovs3a6xNNAk4raGttSmeQfxA4WlIXSd1Img6K6QYsTicS+WyL946T1CGNeQfg+fTeX0jPR9JOkjYt4T7WgFwStQ0WEa+lJbrb0ln5IZkm7gVJZwD3SVpB0hywroXbziWZ/m80sBr4QkQ8KumRdAjRb9N20V2AR9OS8N+AEyNipqQJJGtSvUqynHEx3wCmkkx6PbVFTH8BppEsz3JmRPxd0k9I2kpnKrn5a8DRpf3XsUbjWZzMzCrg6ryZWQWcRM3MKuAkamZWASdRM7MKOImamVXASdTMrAJOomZmFfh/ojmhyA56IdUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! Train error =  0.2085868388958662  Train Acc =  0.9473684210526315  & Validate Test =  0.2199675987318167  & Validate accuracy =  0.9380530973451328\n",
      "!! Train error = 0.2085868388958662 Train Acc = 0.9473684210526315 & Validate Test = 0.2199675987318167 & Validate accuracy = 0.9380530973451328\n",
      "Trained tn, fp, fn, tp : 108 0 6 0\n",
      "** Best Weights now = [0.24034203135633864, 0.6905034138304923, 0.4448749814492344, 2.240420075363306, 1.1852940344370375, 1.7667892261302216, 1.1093331951120677, 0.3224430423213014, 0.214799088557714, 1.0808976287016374, 2.3756911892177808, 1.2638039178710443, 0.0349054702631967, 0.9405360036742658, 0.44987333512204636, 0.49751751208839734, 1.3567749430616318, 0.002990122631400638, 1.5762256423045757, 0.3331157652346454, 0.3520569363050779, 1.0289677895227, 1.3272137219274567, 1.125117265157027, 1.4759224211003263, 0.7813804584481902, 0.6848917197987959, 1.5308789087350512, 0.4601880634836038, 1.3351930116716042, 0.731422064918072, 0.39265543734063135, 1.3717269944266055, 1.60092316462433, 0.5770123548001018, 0.8185475531130004, 0.485834567287509, 0.36320849820386353, 2.5028507916906673, 2.067852953054784, 0.8203999989017786, 0.818960186928563, 0.8803396589680409, 0.06055334496479445, 0.3877049986545624, 0.8660197338206091, 1.129877997732755, 1.6746720652652385, 1.4902105235730891, 0.18661491864811458, 1.752679440091571, 0.09556464886738232, 0.8043606720012404, 0.30988769226910273, 0.6181872864393628, 2.5308334234897654, 0.19705435008328548, 0.9264402649310745, 0.008274982706324662, 0.821572312303921, 0.4470484855996256, 2.45058889444664, 0.15250221693818633, 1.0444566201096837, 1.0447665727056117, 1.3415673403100605, 0.8564761681627628, 0.7483916255581851, 0.4227500547125501, 1.3561350542289932, 1.185456564212153, 0.6168434650248523, 0.9755530023874657, 1.006408828104045, 1.6115053144633358, 0.8015503344518478, 0.5383240981988411, 1.2689988382176227, 0.23726552926740838, 1.5603940549094337, 0.957684120964352, 0.5002537688223738, 1.0572052169054516, 0.6304717496453095, 0.9548095920037191, 0.4638422827342216, 1.137300118500298, 1.313026042047131, 1.943091229341496, 0.4233698577165709, 1.7429776500992438, 1.280642439346987, 1.1130768810888132, 0.01701999767112706, 0.7975138005722621, 0.9837415558361453, 1.074205441846799, 0.9908222335386222, 0.7573457557584795, 1.6286257039737666, 1.2732507426709787, 1.0059756750445874, 0.7767428257279829, 0.7942054418994487, 1.461894036314949, 0.7729845362927774, 0.28632103807550846, 0.34669340365420254, 0.9152709123439842, 0.36361996285906845, 0.33459024388610875, 0.5366580530956943, 1.7273617029527624, 1.2199828481998272, 1.7346385464247103, 1.1678777302378431, 0.31211656891431816, 0.3275039819705736, 0.9628025270714132, 2.181505631032136, 0.4702830703937703, 1.3969229769585165, 0.2167221737446135, 0.6584201831766364, 1.7858394556523407, 0.19441368516175095, 1.0621163024787825, 1.2744090728768902, 0.9989328693239973, 0.20570719402449017, 1.0226410710736922, 1.4971823861714182, 1.4461009483103417, 0.4929101557403891, 0.6256728248016257, 0.3453405579689247, 1.2328454262289348, 1.7226335812561564, 0.9518418309586587, 1.573854260913762, 1.7186789046870317, 0.11194040787288638, 0.3428650301136461, 0.3929946490721212, 0.9304484421338012, 0.8346928702413259, 0.8033249044163192, 0.8710256021009908, 1.3191781505536178, 1.1252397799591787, 1.1349656643415427, 0.9811245862532232, 0.9402304109607578, 1.0897069798656915, 1.1829797359689291, 0.1661410631623534, 0.6776415199919023, 1.2144279110174634, 0.3233960423170684, 1.6758417153859078, 1.669865032290684, 0.7185917177607499, 2.2894463456914904, 1.5307452566495001, 0.756761905860064, 0.9512792027612165, 0.7760024613598884, 0.46753993934879684, 1.0099079707763514, 0.21831691755269192, 0.5740151351437358, 0.6169505029875163, 1.647039347664052, 0.07324808804853189, 1.5183209140907077, 1.395430371620155, 1.5732241482451634, 0.06955977134621083, 1.1062268559976638, 0.08955476672500973, 1.5618345738819746, 1.66957167194865, 0.7515328962063192, 1.380772823992733, 0.3356825648260305, 0.013415677173345962, 1.2834497905275464, 1.0037400014434013, 0.0707315491522249, 0.06471886915845618, 1.3978254676559971, 0.493894205421382, 1.6774204072904175, 1.458890705500853, 2.084928832941727, 0.7901721677514402, 0.03214934427972673, 0.4031620972929919, 0.04093373970130342, 2.138687413041388, 0.891100730409735, 0.24547325167298972, 0.41593869410117656, 2.025722384394149, 1.3688898215150787, 0.87325396516418, 0.6883247233807293, 1.5130505632126094, 1.5317038522542405, 1.0417942274181857, 1.4843061166383935, 1.1026952770369816, 0.8607993713486705, 0.2175592601188918, 0.24809133197202687, 0.9549640556953761, 1.0681710636848278, 1.7182782951053546, 1.2045757997585769, 1.125267755846101, 0.4645645897576305, 0.9703391368806003, 0.7478924623374092, 0.7009108966294999, 0.9271476304755752, 1.7529967808763165, 0.32433652789541767, 1.386814617383252, 0.26188497751621354, 1.3863998911222855, 1.8008324779926295, 0.38642947213952883, 0.7003052550743561, 0.07739241998401292, 0.47718512686510073, 1.606571165756643, 1.2665662731866296, 1.1719447070522824, 0.5791715826857253, 0.3886750441785529, 1.514440280107571, 0.995445820167495, 0.9357595893717118, 1.0571791077063553, 0.2212231620917589, 0.6164959734735687, 2.12960526467351, 0.10940223510223465, 2.0776203558206636, 1.6023509378843297, 0.18612296324757857, 0.26937631622054437, 0.05329315151526557, 0.32236083670162347, 1.3748971554268192, 1.8616572951345016, 0.4825991993269956, 1.975097695238728, 0.03524924136420066, 1.894845425291866, 0.6986072600429151, 1.406549265013275, 0.9850177068979411, 0.43631548819975907, 1.9085818050435202, 0.016054405801025264, 1.8748692017786037, 0.9686366164527366, 0.40397551081313837, 0.3413834351188576, 1.602868084446295, 0.8381553594272414, 1.299792407580396, 0.39472968317061335, 1.0694154302248062, 0.3272663383369194, 1.4211341803372002, 1.0466385164765284, 0.7188030328769291, 1.150480483250414, 1.6421105882304856, 0.7053632636177978, 1.542452858793342, 0.36344312254187, 0.7180089123626828, 1.0151137384086388, 0.8630270327947699, 0.9749082432730252, 0.6276903960429044, 0.29088280800467226, 1.855377775254178, 0.24492966054728837, 0.7697605434712651, 2.2993857727623017, 0.209256062655228, 1.7027169812621619, 0.9715517058887025, 0.23049933761394484, 0.8871200325237992, 1.4435169564096488, 0.8907950176505376, 1.6885123047998087, 1.545713956959072, 1.1461358318393748, 0.1677065622038143, 0.15619663629900915, 0.4414315837823479, 0.4402929247652488, 1.1273179133230051, 1.805065295083478, 0.29630127774982346, 0.9274407587080853, 1.106074995131089, 1.856075251456922, 0.7832026068559103, 0.5033867745320334, 1.1768340003618314, 1.566143747560388, 0.7735594352927796, 1.2247538221732897, 0.42467213336319, 1.0111463797047213, 1.319503707011767, 0.6305503716622235, 1.0116551618204201, 0.20103556240546477, 1.3745918154323211, 1.7649544202464056, 0.717722761969287, 1.4584165651984922, 0.06921467433506, 1.4024090385925483, 0.7666502923560594, 0.28024594717686235, 0.26398397992994627, 0.49383388856294935, 2.03820557803085, 1.6320620144316156, 0.39884330925098394, 0.8769354862255471, 2.0158897963912454, 0.7161683986049051, 2.3426865303757394, 0.04709029853177992, 0.610223171092481, 0.29465978212926935, 0.8030148233660068, 1.1969851766339932, 0.06105836009210741, 1.1688700870831066, 0.8311299129168935, 0.12620921875575253, 0.37265371315860285, 1.448715445057018, 0.9132865713835134, 1.2653442704008653, 0.5294326131946726, 1.058464014033488, 1.7134898864625816, 0.18961045147728914, 1.0384356480266406, 0.16866364865516248, 1.341323596562628, 0.6586764034373722, 0.8564095594681392, 1.0, 0.06968997817099332, 0.43004930080111686, 0.18278197324868414, 1.4799116923101794, 1.9072570336400196, 0.7371749169719325, 1.1148915028322117, 0.8851084971677884, 0.45592254378660624, 1.0, 0.7111245663242113, 1.5204200153554928, 0.47957998464450735, 0.03623097275293252, 1.1961418005820341, 0.803858199417966, 0.5393375851088837, 0.7122394384397043, 1.4621957148100466, 1.3587327384718533, 0.1642246511917982, 1.3026074570865982, 0.771232703117523, 1.0, 0.17475993056814088, 0.9999999999999999, 0.3596416463391302, 1.4286973205267808, 0.5713026794732191, 0.7190167807961279, 1.9957275635561542, 0.004272436443845808, 0.9228229946002998, 0.5289314460329712, 1.6903680947177653, 1.0118145968975334, 1.6570356722281154, 0.11185019012361494, 0.1065940735564529, 1.0066755618334386, 0.9565950074260128, 1.0367294307405484, 0.9404304630275799, 0.37504419584484966, 1.6249558041551502, 0.5280945845398336, 1.1315914858566651, 0.8684085141433351, 0.8092482794757936, 0.5429665450721939, 1.4570334549278061, 0.8851123982458823, 1.1180014169974912, 0.9965786788627269, 0.5742144204804238, 1.526700222115737, 1.421572946117236, 0.3629323154263862, 0.9714026815730631, 1.402171819549432, 1.4577781792762838, 1.403265122809825, 0.42678982929556997, 0.2824646926239457, 1.0275303564449432, 0.8324675566319849]\n",
      "** Best n_Fold now = 1\n",
      "** Best Train now = 0.2085868388958662\n",
      "** Best Accuracy Train now = 0.9473684210526315\n",
      "** Best validation error now = 0.2199675987318167\n",
      "** Best Accuracy Validate now = 0.9380530973451328\n"
     ]
    }
   ],
   "source": [
    "trainWithFold(1,name_file_fold1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:58:23.661609Z",
     "start_time": "2020-02-26T10:57:54.454641Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're going to train with file ./Final_Data_Train_8_MultiNC.csv & Saved = log_t_f_38s_8Q_NiaPy_GWO_1_fold2.log\n",
      "Y Train count abnormal :  7\n",
      "7 113\n",
      "106 113\n",
      "## parameter weight output class zero [0.2,0.1] \n",
      "## parameter weight output class one [0.1,0.9380530973451328] \n",
      "## New training GWO with D=432,NP = 10, nFES=20, A = 2, C = 2 with fold = 2 \n",
      "fold 2 best at nFES 1 error 0.22586710961009443 (0,7,0.061946902654867256)\n",
      "fold 2 best at nFES 2 error 0.22586707325021305 (5,6,0.09734513274336283)\n",
      "fold 2 best at nFES 5 error 0.22586711256092837 (106,0,0.9380530973451328)\n",
      "Optimal Weight and Gammar are : \n",
      ":: Finished Fold ::\n",
      "Y Train count abnormal :  7\n",
      "7 113\n",
      "106 113\n",
      "## parameter weight output class zero [0.2,0.1] \n",
      "## parameter weight output class one [0.1,0.9380530973451328] \n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! Train error =  0.22586711256092837  Train Acc =  0.9380530973451328  & Validate Test =  0.217034051013808  & Validate accuracy =  0.9473684210526315\n",
      "!! Train error = 0.22586711256092837 Train Acc = 0.9380530973451328 & Validate Test = 0.217034051013808 & Validate accuracy = 0.9473684210526315\n",
      "Trained tn, fp, fn, tp : 106 0 7 0\n",
      "** Best Weights now = [1.4482850076600526, 0.9134670799227516, 0.2996850450322843, 1.2177914619407137, 1.561939931743425, 1.170787256278359, 0.19366373497143222, 1.1943804824509807, 0.6736300070234617, 1.9145459153339703, 0.4407401398133359, 1.8231390893261075, 1.7813938815238406, 0.00826007643163546, 1.1770565971763627, 0.530869567216673, 0.32399473317807437, 0.5317335592639968, 0.7413322308408774, 0.9359885678061056, 0.43183615212487864, 1.274507826330311, 1.4020956108716833, 0.4039427805059122, 1.0423164230654969, 1.7679804084547344, 0.28607416394628904, 1.3259325625069862, 0.8611393433063791, 1.672099896626305, 0.45328679484995477, 1.5125303774919385, 1.6597006450172012, 0.4136592131847799, 0.10165116701645531, 0.5195357869233138, 1.4792100194598168, 0.2833231805725664, 0.439292671311558, 0.7773691975591538, 0.037903785858677756, 1.3550122623336895, 1.8078226493852354, 1.8200662335193025, 0.2235622165608856, 0.6325027766926204, 0.8456346772020422, 0.11078456895862486, 1.8874814512916736, 2.162955625785783, 1.2858896966881874, 0.7029304355973923, 0.3718207677836759, 0.5282420731170976, 1.1474512382665025, 1.3384168275464015, 1.1988782276832353, 1.2257176450370657, 0.9583846366113118, 0.9978162129293172, 0.2749646997873067, 0.8583705121388601, 0.6652342029560889, 1.4897894406171819, 1.1640698407211147, 0.2711615533859059, 0.7216704057721979, 1.1652887559042728, 1.1131626348931034, 0.7524600581202668, 1.3223973105859559, 0.21481534864269203, 1.125378778825963, 1.2450853834987414, 1.4914414740818303, 1.0813472599987035, 0.6639379594323609, 1.2952071949282091, 0.04659758525004165, 1.0510043639841506, 0.8415122257235772, 1.5171690816157506, 1.061144042207137, 0.33522015244970427, 1.4340551769291023, 0.848237539808443, 0.9969745430897113, 0.7535420907361934, 1.0536573731639582, 0.5716158910008132, 1.0158130732241675, 0.5732944018472195, 1.3010387712418554, 0.8612735319225153, 0.4340159375992624, 1.3870952981590807, 1.2693664693375155, 1.1581025166683834, 0.6041413326155802, 1.3121378062389704, 0.818275085961491, 0.8474831483344732, 1.131672612901007, 0.4677917777315339, 1.4871758799603834, 1.1304730710598097, 0.8049906178123317, 0.279805448835971, 0.467688042685394, 0.6284418246390772, 1.9697244011808779, 0.16760425085553965, 1.5545829959293451, 0.598597078915572, 1.6740792336181332, 0.9392821721760604, 0.8883874424247836, 0.9114241473098984, 1.0869109066829237, 0.9705791701432799, 0.8245116760277963, 1.5457815777792046, 1.2683491916183498, 1.205008891406293, 0.1874344390322533, 0.9293682458828848, 0.48086602577025095, 1.9759292892915727, 2.0319146209895496, 0.5716460324000194, 1.0598396509383687, 0.1869303804241618, 1.5743096328534854, 0.118564367332591, 0.053097657435964285, 0.08151332361571308, 0.33090681443878533, 2.2168353085996704, 1.1419535539761265, 0.6657601813283361, 2.1787072493495288, 0.635560838068784, 0.7487627306230555, 0.12395714168641925, 1.5599247104031402, 0.3350601942131199, 0.510149730679018, 1.4270646868059504, 1.482608964564241, 0.39081869190781776, 0.9705454823988516, 1.3238275390278615, 0.11527450295028119, 0.6388485595026556, 1.7263527089894317, 0.6062683339001039, 1.3681235655171198, 1.4865239899297897, 2.1228195753031125, 0.04666668797859866, 0.004396578879189258, 0.23434102348908759, 1.4037282499307324, 2.3475571130765642, 0.6515807590230588, 0.9682708547161271, 0.7706972472344812, 0.9389563280789636, 0.7678343336587593, 0.15137511428131334, 0.46951935541684997, 1.579337363507058, 1.8532147353351391, 0.030393732464975398, 2.2283223386481628, 0.2810170813954929, 0.37255642488532853, 0.34017039101881125, 1.314987932745032, 0.3409208324336831, 0.22354887237669732, 0.5420004267025165, 1.154136546586142, 2.3202659360944393, 1.52356871793846, 0.7322027968606946, 0.8553280792469671, 0.6489486241940841, 0.0046854158089773, 1.036143623450342, 1.8799496926135575, 0.9286106480130434, 0.07758595546634699, 0.6110149426009222, 1.1234849447737714, 0.5114351755176817, 1.8317750175643346, 0.967123825256325, 0.6230980411709441, 0.4982314434064999, 1.0578751186872992, 1.3904879766074167, 1.1209552439908304, 1.2405838541285432, 1.3337129672909604, 0.7350553547175065, 0.3782162036619099, 1.3298002762802363, 0.9438022373775733, 0.868655446703261, 0.7539941009124919, 1.5347591755323395, 1.5688939511363806, 0.3964598826547301, 0.6036349294029876, 0.5859622802012977, 1.0657720714648242, 1.5405575305554027, 0.3103221048430489, 1.5027575102164228, 1.3972263440650452, 0.09669397199430434, 1.7078985802424826, 0.3787718866184709, 0.764516632886344, 1.4406550721479723, 0.35359943315326775, 1.644817890159293, 1.3108432103036893, 0.9915305481913257, 1.4410712191159198, 0.7572517746868226, 0.06023085224171082, 0.04122057760640219, 1.0690744304359, 0.6413023526630899, 1.0604590227621007, 1.1790661791363128, 1.0365321128889542, 0.9198489621701577, 0.8638833913833729, 1.2298335485601126, 0.4085972782020626, 0.7681103097162367, 0.6716016264206005, 1.8049207272101486, 1.2103265785254163, 1.6459931554496825, 1.6617938324902324, 0.1477074909323573, 0.0895462792553266, 0.7443737081317497, 1.1909179038014444, 1.1898652166742265, 1.3234620656941354, 1.7309167082707309, 1.605958027417726, 0.5410529828853272, 0.34468795384586165, 0.0731391414105476, 0.7428402002695899, 1.6411736315637089, 0.41000203059854357, 1.0922186661532052, 0.5155951521184128, 0.15598812824014455, 1.5616082736336245, 1.7161276240938672, 0.9072864935984936, 0.19150468441934687, 0.05799570465608974, 0.6286074492593983, 0.9074268842342652, 2.2374354392265037, 1.1555707152676513, 2.226769303951392, 0.2989357069406661, 0.48725879646403303, 0.6534185416534709, 1.638210207849492, 1.3284630379095905, 1.2189935390976918, 0.49373423990674975, 1.4470383636892954, 0.33735777166722075, 0.9349783689473342, 0.6012244709326259, 0.267408138371089, 0.8253931672245695, 2.4314459991252506, 0.5332573962870518, 0.47873308128608505, 0.8022376406405669, 0.5082992475942226, 0.5235249655533768, 1.8971085022888765, 0.26789565605167387, 0.8377134606797425, 1.281654709281307, 1.172320805789735, 1.0023639080795728, 1.012301275613765, 1.258224063902094, 0.9464353864909635, 0.48898639016281936, 0.02628499478043711, 1.4990991099066224, 0.6906629806439737, 0.6196688168160088, 0.21839660267069438, 0.507278191265719, 1.7827548573728687, 0.9603229820722178, 1.7218164592518945, 0.08354821389411271, 1.2821450942449888, 1.1739052838812372, 0.4192670821868695, 1.2925441976694125, 0.6566051605004196, 0.07035100640988898, 1.714744080861622, 1.390438094245561, 0.16327817541938872, 1.5853953016578592, 0.9648607786590417, 1.771792409233534, 0.21040738757029281, 0.7603500053533334, 1.273264540892783, 0.734499078163494, 0.6994304984696617, 0.113749401981728, 1.2635936413059836, 0.7741903940865058, 0.13511884182626255, 0.5163248068406938, 0.5411024231363423, 2.2670143164586682, 0.5838602353626101, 1.9187953409829335, 0.5117565128816864, 0.8556485243824066, 0.829780904163889, 1.3145705714537044, 0.12646553580066477, 0.7602326683918889, 1.239767331608111, 0.7543087296775837, 1.639731831693583, 0.3602681683064171, 0.09803779004032209, 1.4027187860372048, 1.6684058571469456, 0.7537187777969131, 0.17515657901893628, 0.8367908869139364, 1.1085209905901017, 1.504601400794176, 0.3414214138014056, 1.0454561948143164, 0.6249108456090046, 0.9279541997781872, 1.0720458002218127, 0.9589470851750267, 1.0, 0.7762273069353872, 2.2409658257311658, 0.13256460648019774, 0.15780567556736347, 1.4686638922212736, 0.8056049700653221, 0.6784608478105151, 1.3215391521894846, 0.17766034908645967, 1.0, 0.781713170940501, 1.9753684237552782, 0.02463157624472166, 0.5252242029677344, 0.6790855815492167, 1.3209144184507833, 0.4355572160560953, 0.3041929112224511, 1.01434796053833, 1.1801627163747905, 0.9763299443998843, 1.5249664674645438, 0.7582334864622691, 1.0, 0.7312334421661609, 1.0, 0.30721601933462317, 1.2675147186922675, 0.7324852813077325, 0.3002949412347703, 1.3378813246386967, 0.6621186753613031, 0.5985572834587692, 0.6471934780672675, 0.31876684337025585, 0.8931997949276727, 1.4525417026649339, 1.6882981809698703, 0.43336714308407653, 1.151257593869497, 1.0363677565429201, 0.8123746495875832, 0.3252166836964242, 1.5177860725620995, 0.4822139274379003, 0.3589440531323834, 1.0103856280465169, 0.9896143719534832, 0.019366139547628225, 0.5914775399538699, 1.4085224600461304, 0.2034551200469108, 0.39518208596143006, 0.9780459493124452, 1.1052362018721542, 1.5063069532179225, 0.472545620467393, 1.5426831891686543, 0.7046210824762865, 2.4493289273023975, 2.2336111153599285, 0.00930567230304123, 0.7818734962966822, 0.3057608127266894, 0.22011997601126057, 0.27485361008564224]\n",
      "** Best n_Fold now = 2\n",
      "** Best Train now = 0.22586711256092837\n",
      "** Best Accuracy Train now = 0.9380530973451328\n",
      "** Best validation error now = 0.217034051013808\n",
      "** Best Accuracy Validate now = 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "trainWithFold(2,name_file_fold2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:58:23.690817Z",
     "start_time": "2020-02-26T10:58:23.677171Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Joint two files and delete split files\n",
    "'''\n",
    "\n",
    "filenames = [name_file_fold1, name_file_fold2]\n",
    "with open(\"log_t_f_38s_8Q_NiaPy_\"+getFileNameSwarmTrain(_select_sw_train)+\"_\"+getFileNumber()+\".log\", 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "        \n",
    "os.remove(name_file_fold1)\n",
    "os.remove(name_file_fold2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Fold Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:58:25.139397Z",
     "start_time": "2020-02-26T10:58:23.695292Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Train count abnormal :  7\n",
      "7 113\n",
      "106 113\n",
      "Predict [2.680296210167357e-13,9.9826799758204e-42] , Actual [0.2 0.1] \n",
      "Predict [2.06720067602345e-12,4.173118919467937e-33] , Actual [0.2 0.1] \n",
      "Predict [1.6553431768258985e-17,3.564231295956847e-43] , Actual [0.2 0.1] \n",
      "Predict [3.5693641358004423e-48,1.2044366784218943e-73] , Actual [0.2 0.1] \n",
      "Predict [1.1613711557574808e-14,3.4008542308067306e-42] , Actual [0.2 0.1] \n",
      "Predict [3.163760388389228e-14,1.0412034405959486e-41] , Actual [0.2 0.1] \n",
      "Predict [3.2755251086535293e-21,4.5521920941076985e-64] , Actual [0.2 0.1] \n",
      "Predict [7.863790523726575e-16,1.0919888688847832e-55] , Actual [0.2 0.1] \n",
      "Predict [1.6492800849280625e-20,2.5773630993224633e-48] , Actual [0.2 0.1] \n",
      "Predict [5.786857021871762e-20,8.087322101572054e-48] , Actual [0.2 0.1] \n",
      "Predict [3.2630870741434717e-16,5.621256575159983e-51] , Actual [0.2 0.1] \n",
      "Predict [2.1131031936698165e-34,4.734929954077879e-88] , Actual [0.2 0.1] \n",
      "Predict [1.1310318161913232e-16,5.000930306257469e-47] , Actual [0.2 0.1] \n",
      "Predict [1.5913953250845306e-19,2.0715715375331332e-40] , Actual [0.2 0.1] \n",
      "Predict [2.11070222864492e-16,1.0791030592666678e-44] , Actual [0.2 0.1] \n",
      "Predict [4.328144061080862e-17,7.312252196387662e-46] , Actual [0.2 0.1] \n",
      "Predict [1.5586504752813688e-16,4.2862575481288647e-38] , Actual [0.2 0.1] \n",
      "Predict [7.248636884890347e-14,2.2613943387755614e-39] , Actual [0.2 0.1] \n",
      "Predict [8.334717214192197e-16,9.285387297211549e-51] , Actual [0.2 0.1] \n",
      "Predict [2.5537042875552296e-13,1.288129422143997e-45] , Actual [0.2 0.1] \n",
      "Predict [4.1274475973574194e-25,1.183589504855248e-52] , Actual [0.2 0.1] \n",
      "Predict [2.2268856847921445e-14,8.428392285417708e-36] , Actual [0.2 0.1] \n",
      "Predict [6.8077043327114125e-15,1.5079897464575117e-47] , Actual [0.2 0.1] \n",
      "Predict [1.1599046939257404e-15,1.7003414627913394e-42] , Actual [0.2 0.1] \n",
      "Predict [9.817434854909874e-16,1.8191988423749151e-47] , Actual [0.2 0.1] \n",
      "Predict [4.790924613201824e-21,8.966378030837796e-45] , Actual [0.2 0.1] \n",
      "Predict [1.7799095788374854e-19,4.003483259904743e-80] , Actual [0.1       0.9380531] \n",
      "Predict [7.289237879459044e-16,6.756139010263572e-43] , Actual [0.2 0.1] \n",
      "Predict [1.6250330391904015e-16,6.516088133201883e-35] , Actual [0.2 0.1] \n",
      "Predict [2.1002616700355598e-47,2.346621934529364e-98] , Actual [0.2 0.1] \n",
      "Predict [1.3600621855734144e-16,8.13483126214099e-41] , Actual [0.2 0.1] \n",
      "Predict [1.0999393687294591e-28,1.2407825125211283e-74] , Actual [0.2 0.1] \n",
      "Predict [9.58801429269859e-15,4.58556797243348e-40] , Actual [0.2 0.1] \n",
      "Predict [1.1024552170457068e-13,6.73764886169429e-37] , Actual [0.2 0.1] \n",
      "Predict [9.186643125068737e-15,3.97061453656853e-49] , Actual [0.2 0.1] \n",
      "Predict [2.6492264777297748e-15,3.171314417206708e-51] , Actual [0.1       0.9380531] \n",
      "Predict [1.2067664219413274e-49,4.309242753554906e-108] , Actual [0.1       0.9380531] \n",
      "Predict [2.04373791521891e-20,2.168788127957297e-40] , Actual [0.2 0.1] \n",
      "Predict [8.785995492718041e-31,1.479541176175292e-52] , Actual [0.2 0.1] \n",
      "Predict [1.3958440861592817e-15,9.998301731062845e-43] , Actual [0.2 0.1] \n",
      "Predict [3.7745771211334007e-16,1.87370691687761e-39] , Actual [0.2 0.1] \n",
      "Predict [6.6995666831875115e-15,3.0120526915220794e-47] , Actual [0.2 0.1] \n",
      "Predict [4.166057404669432e-18,5.847357768038765e-42] , Actual [0.2 0.1] \n",
      "Predict [7.42039626196185e-15,1.523921472296914e-36] , Actual [0.2 0.1] \n",
      "Predict [1.8637320741252744e-17,2.1758043622743884e-61] , Actual [0.2 0.1] \n",
      "Predict [2.2770419781012106e-16,1.5691185249329115e-53] , Actual [0.2 0.1] \n",
      "Predict [4.61490579697509e-17,2.2462228082359943e-44] , Actual [0.2 0.1] \n",
      "Predict [3.545423093501499e-18,9.548406898140127e-48] , Actual [0.2 0.1] \n",
      "Predict [7.37311461840137e-17,1.6060995007156364e-50] , Actual [0.2 0.1] \n",
      "Predict [1.5703736173176597e-18,6.094250137537974e-64] , Actual [0.2 0.1] \n",
      "Predict [4.006648653488322e-18,4.812920272797716e-56] , Actual [0.2 0.1] \n",
      "Predict [3.2256080378527866e-12,4.4254977503370434e-35] , Actual [0.2 0.1] \n",
      "Predict [4.1918403796692476e-17,8.251235355467684e-61] , Actual [0.2 0.1] \n",
      "Predict [6.983443491420882e-35,1.3434642883259296e-74] , Actual [0.2 0.1] \n",
      "Predict [4.347970082291691e-15,4.744817350960653e-42] , Actual [0.2 0.1] \n",
      "Predict [1.7589175575255375e-13,6.0408910707053245e-31] , Actual [0.2 0.1] \n",
      "Predict [1.7236319993088627e-17,5.73253931808156e-60] , Actual [0.2 0.1] \n",
      "Predict [3.2263680118665884e-16,3.662819022109994e-65] , Actual [0.2 0.1] \n",
      "Predict [2.7062856098060453e-14,3.0271938994834697e-46] , Actual [0.2 0.1] \n",
      "Predict [8.877851141807384e-20,3.569481118805818e-57] , Actual [0.2 0.1] \n",
      "Predict [7.829038989611e-17,1.5849963912180331e-56] , Actual [0.2 0.1] \n",
      "Predict [2.9397801657955795e-27,4.8243138697713597e-63] , Actual [0.2 0.1] \n",
      "Predict [6.419865569264039e-15,1.7275914678898913e-49] , Actual [0.2 0.1] \n",
      "Predict [8.548858694561516e-15,1.3077763471582629e-45] , Actual [0.2 0.1] \n",
      "Predict [2.269912268764994e-26,5.951002671259456e-68] , Actual [0.2 0.1] \n",
      "Predict [1.2862933361804722e-19,1.515736615752186e-46] , Actual [0.2 0.1] \n",
      "Predict [2.3824958708660247e-14,5.663682019135686e-41] , Actual [0.2 0.1] \n",
      "Predict [1.371459335716824e-13,7.858502752632894e-34] , Actual [0.2 0.1] \n",
      "Predict [1.1025832405843963e-14,7.802270910232827e-42] , Actual [0.2 0.1] \n",
      "Predict [4.994991121431952e-12,1.7627624090533468e-37] , Actual [0.2 0.1] \n",
      "Predict [3.6059765614069243e-31,2.0054395708064496e-119] , Actual [0.2 0.1] \n",
      "Predict [1.965703937804588e-21,1.046018457640876e-40] , Actual [0.2 0.1] \n",
      "Predict [7.851724292332915e-15,4.62446931571485e-38] , Actual [0.2 0.1] \n",
      "Predict [5.877978638057744e-20,1.4252214523850782e-43] , Actual [0.2 0.1] \n",
      "Predict [1.886799182910037e-23,9.913040668593665e-68] , Actual [0.2 0.1] \n",
      "Predict [2.237671062440924e-16,2.3759764025142496e-58] , Actual [0.2 0.1] \n",
      "Predict [3.476898290016544e-17,1.4955509939422247e-50] , Actual [0.2 0.1] \n",
      "Predict [6.18708028900008e-13,1.1678129823762044e-41] , Actual [0.2 0.1] \n",
      "Predict [7.675335697524762e-22,1.715043467963762e-60] , Actual [0.2 0.1] \n",
      "Predict [2.4225532978132954e-22,1.4893802404373193e-54] , Actual [0.2 0.1] \n",
      "Predict [1.5899991758891417e-24,1.3922012406099061e-53] , Actual [0.1       0.9380531] \n",
      "Predict [1.107080877160493e-12,1.1706507860978514e-39] , Actual [0.2 0.1] \n",
      "Predict [2.7055221489710904e-17,3.67719382553674e-57] , Actual [0.2 0.1] \n",
      "Predict [3.9831629247792263e-14,4.909329607367054e-40] , Actual [0.2 0.1] \n",
      "Predict [4.3885092592344224e-57,1.220893817017239e-98] , Actual [0.2 0.1] \n",
      "Predict [4.839364941720865e-13,3.339245012333021e-36] , Actual [0.2 0.1] \n",
      "Predict [7.43330063464053e-17,2.834041037701109e-48] , Actual [0.2 0.1] \n",
      "Predict [5.713008303763543e-11,1.5516266334470742e-30] , Actual [0.2 0.1] \n",
      "Predict [2.715179543857939e-18,2.263146257699414e-68] , Actual [0.2 0.1] \n",
      "Predict [1e-08,1e-08] , Actual [0.1       0.9380531] \n",
      "Predict [2.2237278235052063e-15,3.589882173543713e-43] , Actual [0.2 0.1] \n",
      "Predict [5.512482060313058e-14,1.5540118592989549e-41] , Actual [0.2 0.1] \n",
      "Predict [1.9485687091488123e-20,5.735861267057933e-51] , Actual [0.1       0.9380531] \n",
      "Predict [4.85090852904117e-20,2.7297809732859643e-53] , Actual [0.2 0.1] \n",
      "Predict [2.6906698092558994e-14,4.3259332368813454e-51] , Actual [0.2 0.1] \n",
      "Predict [1.0267800236894404e-12,6.938023737748463e-43] , Actual [0.2 0.1] \n",
      "Predict [5.841976899961251e-19,1.5907071760443105e-44] , Actual [0.2 0.1] \n",
      "Predict [2.31070865208586e-16,5.788546352690144e-52] , Actual [0.2 0.1] \n",
      "Predict [1.1127274918818475e-16,2.816463438600168e-43] , Actual [0.2 0.1] \n",
      "Predict [5.444330691782832e-16,1.990648444207377e-36] , Actual [0.2 0.1] \n",
      "Predict [1.3229049880929826e-20,4.5969619779721554e-51] , Actual [0.1       0.9380531] \n",
      "Predict [7.235186396319989e-12,4.2684409380595404e-35] , Actual [0.2 0.1] \n",
      "Predict [1.84292876775159e-25,3.772037397209877e-74] , Actual [0.2 0.1] \n",
      "Predict [2.0738739976471663e-14,1.2664485527030152e-39] , Actual [0.2 0.1] \n",
      "Predict [7.964294207983788e-13,1.6025873962001903e-41] , Actual [0.2 0.1] \n",
      "Predict [9.329446885824854e-18,8.274676345698673e-45] , Actual [0.2 0.1] \n",
      "Predict [8.117821380967865e-15,3.7937272428394036e-42] , Actual [0.2 0.1] \n",
      "Predict [1.2834070275614957e-24,4.215243104394479e-50] , Actual [0.2 0.1] \n",
      "Predict [1.7643019907826286e-13,2.5648685975038513e-41] , Actual [0.2 0.1] \n",
      "Predict [1.0794188871403917e-11,2.658153637637556e-32] , Actual [0.2 0.1] \n",
      "Predict [1.1948422171688758e-22,5.875870043808786e-54] , Actual [0.2 0.1] \n",
      "Predict [1.0001785827317302e-20,7.604785707756546e-53] , Actual [0.2 0.1] \n",
      "Predict [1.079689204034389e-19,5.572006444056333e-65] , Actual [0.2 0.1] \n",
      "MAE =  0.1728600515987827\n",
      "\t-----------\t \t-----------\t\n",
      "|\t tn - 106 \t|,|\t fp - 0 \t|\n",
      "|\t fn - 7 \t|,|\t tp - 0 \t|\n",
      "\t-----------\t \t-----------\t\n",
      "Acc = 0.9380530973451328 , f1 = 0.9080696650099002\n",
      "RMSE =  0.22586711256092837\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_f = fuzzy_aggregation(tree_shape,current_fold_train=best_fold,ignore_writeClass=True).print_results_train(best_sol_cross,pos_start,pos_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:58:27.104506Z",
     "start_time": "2020-02-26T10:58:25.233775Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Train count abnormal :  7\n",
      "7 113\n",
      "106 113\n",
      "Predict [2.0099725986042574e-18,1.7202659260249346e-53] , Actual [0.2 0.1] \n",
      "Predict [2.0556519366134874e-15,1.3390861117817399e-45] , Actual [0.2 0.1] \n",
      "Predict [1.4603653147023071e-15,3.949457668031335e-40] , Actual [0.2 0.1] \n",
      "Predict [1.1309603864550993e-12,9.095960563856103e-38] , Actual [0.2 0.1] \n",
      "Predict [2.3733110744712156e-15,4.79486492879975e-46] , Actual [0.2 0.1] \n",
      "Predict [6.793218342234914e-44,3.894433794518823e-96] , Actual [0.1       0.9380531] \n",
      "Predict [5.329895696056188e-15,7.465357491566568e-40] , Actual [0.2 0.1] \n",
      "Predict [5.651056712402841e-14,4.83424580070458e-40] , Actual [0.2 0.1] \n",
      "Predict [1.1967373745708051e-15,2.5809032468117543e-38] , Actual [0.2 0.1] \n",
      "Predict [1.2367166075886355e-14,6.5692802081346085e-53] , Actual [0.2 0.1] \n",
      "Predict [3.066943254324952e-11,7.75999494604872e-34] , Actual [0.2 0.1] \n",
      "Predict [4.610881107770411e-37,3.053632213177236e-84] , Actual [0.2 0.1] \n",
      "Predict [1.8213087440543524e-20,4.500259305020663e-65] , Actual [0.2 0.1] \n",
      "Predict [2.0943567735635833e-37,9.745073366914357e-75] , Actual [0.1       0.9380531] \n",
      "Predict [1.0402805313554468e-15,1.8010328066948635e-42] , Actual [0.2 0.1] \n",
      "Predict [2.556142628730963e-14,8.405866585493032e-33] , Actual [0.2 0.1] \n",
      "Predict [9.795135329154565e-18,7.144928514615272e-40] , Actual [0.2 0.1] \n",
      "Predict [1.1156529932949662e-19,3.240524009093047e-68] , Actual [0.2 0.1] \n",
      "Predict [1.5336407808043202e-18,2.629578963325625e-54] , Actual [0.2 0.1] \n",
      "Predict [6.1181923523829745e-21,1.772566704503821e-66] , Actual [0.2 0.1] \n",
      "Predict [3.2667780280136336e-17,3.5769270742013076e-50] , Actual [0.2 0.1] \n",
      "Predict [1.5628719796762138e-17,2.4776965815200134e-57] , Actual [0.2 0.1] \n",
      "Predict [1.0659017924918171e-13,1.599722138394254e-33] , Actual [0.2 0.1] \n",
      "Predict [1.3721543261002962e-11,1.2805058176457911e-30] , Actual [0.2 0.1] \n",
      "Predict [1.6695580734757671e-15,3.776823450635215e-61] , Actual [0.2 0.1] \n",
      "Predict [5.607078427892857e-31,3.6991002733054396e-74] , Actual [0.2 0.1] \n",
      "Predict [7.628335946048714e-15,8.637752491077314e-49] , Actual [0.2 0.1] \n",
      "Predict [3.75365737540112e-16,8.074058676748366e-41] , Actual [0.2 0.1] \n",
      "Predict [4.031055125817442e-43,6.321189029641415e-81] , Actual [0.1       0.9380531] \n",
      "Predict [7.013980216142683e-13,6.83244281442239e-42] , Actual [0.2 0.1] \n",
      "Predict [7.262929284006927e-24,6.791999128579778e-44] , Actual [0.2 0.1] \n",
      "Predict [1.1594374586739054e-21,9.500630505079761e-52] , Actual [0.2 0.1] \n",
      "Predict [1.4110089265859386e-13,9.292022799682675e-39] , Actual [0.2 0.1] \n",
      "Predict [1.425550110467814e-16,2.2081993785459863e-54] , Actual [0.2 0.1] \n",
      "Predict [4.737764696133541e-20,6.175431408440248e-48] , Actual [0.2 0.1] \n",
      "Predict [1.2034793492783502e-19,2.053462736287783e-62] , Actual [0.2 0.1] \n",
      "Predict [6.06917126036305e-14,4.279286282321817e-40] , Actual [0.2 0.1] \n",
      "Predict [3.83816465965673e-18,1.0305960719736224e-60] , Actual [0.2 0.1] \n",
      "Predict [5.434171306358303e-22,2.6064860094504738e-51] , Actual [0.1       0.9380531] \n",
      "Predict [5.194456917310617e-19,1.0914921824814392e-55] , Actual [0.2 0.1] \n",
      "Predict [1.5818883225033341e-12,2.3675229621912273e-32] , Actual [0.2 0.1] \n",
      "Predict [4.340755822209538e-14,8.20921109963367e-46] , Actual [0.2 0.1] \n",
      "Predict [2.2190963870893417e-28,1.0388096296287549e-34] , Actual [0.2 0.1] \n",
      "Predict [1.5117568861751265e-36,2.856675951947779e-79] , Actual [0.2 0.1] \n",
      "Predict [3.1477002303492107e-19,2.936472147049343e-36] , Actual [0.2 0.1] \n",
      "Predict [2.7979117813208334e-24,1.2204524743220935e-115] , Actual [0.1       0.9380531] \n",
      "Predict [1.9997641946705346e-17,4.103545169436497e-46] , Actual [0.2 0.1] \n",
      "Predict [2.2027813371524795e-23,4.2996482972857027e-75] , Actual [0.2 0.1] \n",
      "Predict [2.337708616210692e-11,6.786721176259461e-38] , Actual [0.2 0.1] \n",
      "Predict [1.2192748055468835e-14,4.824862329162355e-39] , Actual [0.2 0.1] \n",
      "Predict [3.5223556388829377e-16,3.3692195833155354e-51] , Actual [0.2 0.1] \n",
      "Predict [1.0595999570898362e-14,5.753219784133399e-44] , Actual [0.2 0.1] \n",
      "Predict [1.070474968570435e-25,4.8495427317768355e-81] , Actual [0.2 0.1] \n",
      "Predict [5.033125427728039e-11,5.590756131050443e-29] , Actual [0.2 0.1] \n",
      "Predict [4.4741167343750505e-17,2.6948460284732776e-62] , Actual [0.2 0.1] \n",
      "Predict [1.1000566420094094e-12,3.3272721141313052e-34] , Actual [0.2 0.1] \n",
      "Predict [9.12373706341347e-17,2.5316307872275412e-52] , Actual [0.2 0.1] \n",
      "Predict [8.883332137534032e-35,4.298048094298722e-77] , Actual [0.1       0.9380531] \n",
      "Predict [5.55916473907382e-15,4.927520164344636e-46] , Actual [0.2 0.1] \n",
      "Predict [2.447579737895856e-16,1.4906145223761668e-49] , Actual [0.2 0.1] \n",
      "Predict [1.1310847532165772e-23,1.2329362005768725e-64] , Actual [0.2 0.1] \n",
      "Predict [5.741805905091994e-10,1.1542659930356505e-32] , Actual [0.2 0.1] \n",
      "Predict [7.680647699948854e-17,6.081689617807067e-47] , Actual [0.2 0.1] \n",
      "Predict [1.2089769452465437e-19,1.5497954797093173e-58] , Actual [0.2 0.1] \n",
      "Predict [1.3304297377974566e-13,3.2435591646745217e-45] , Actual [0.2 0.1] \n",
      "Predict [1.6945929993281704e-14,2.479089607983805e-46] , Actual [0.2 0.1] \n",
      "Predict [1.470403932636392e-26,3.412030760593033e-58] , Actual [0.2 0.1] \n",
      "Predict [1.2318765719245802e-41,3.4037855826096336e-95] , Actual [0.2 0.1] \n",
      "Predict [6.293868061089705e-43,3.7939941788936227e-109] , Actual [0.2 0.1] \n",
      "Predict [6.771132575094162e-16,1.3585256055828916e-36] , Actual [0.2 0.1] \n",
      "Predict [1.6148259523005295e-14,7.482580470084795e-40] , Actual [0.2 0.1] \n",
      "Predict [8.119046240550502e-18,9.881479410501646e-58] , Actual [0.2 0.1] \n",
      "Predict [1.1186400123723782e-21,4.238485753157532e-44] , Actual [0.2 0.1] \n",
      "Predict [3.247983313369643e-21,2.7457004395692347e-59] , Actual [0.2 0.1] \n",
      "Predict [6.320493123719029e-12,2.9789571347183255e-35] , Actual [0.2 0.1] \n",
      "Predict [8.272908833123767e-13,5.733335150353229e-39] , Actual [0.2 0.1] \n",
      "Predict [1.4446742978903826e-13,6.764714360963058e-39] , Actual [0.2 0.1] \n",
      "Predict [2.869600172914968e-12,1.7214555294519153e-38] , Actual [0.2 0.1] \n",
      "Predict [3.491272656402743e-13,1.8419157351064302e-38] , Actual [0.2 0.1] \n",
      "Predict [2.5948152141274767e-15,5.133911146650851e-47] , Actual [0.2 0.1] \n",
      "Predict [9.207131826775853e-30,3.799523120776645e-67] , Actual [0.2 0.1] \n",
      "Predict [1.2033705503844038e-22,7.88537619510962e-82] , Actual [0.2 0.1] \n",
      "Predict [1.4618205804602078e-18,9.795239510312509e-50] , Actual [0.2 0.1] \n",
      "Predict [6.029939834651076e-15,3.2423320813954725e-47] , Actual [0.2 0.1] \n",
      "Predict [3.3099303106981856e-14,7.009317592456409e-41] , Actual [0.2 0.1] \n",
      "Predict [1.8106987231369257e-14,3.933630449355332e-42] , Actual [0.2 0.1] \n",
      "Predict [5.754740979565493e-14,2.681601725508374e-41] , Actual [0.2 0.1] \n",
      "Predict [7.01593240431975e-20,1.417574006766924e-47] , Actual [0.2 0.1] \n",
      "Predict [6.24815515784492e-21,2.0871160874038544e-71] , Actual [0.2 0.1] \n",
      "Predict [1.5902021695011013e-18,4.366110111853963e-60] , Actual [0.2 0.1] \n",
      "Predict [2.7106134054275144e-24,1.9429556849594434e-83] , Actual [0.2 0.1] \n",
      "Predict [7.875879118400025e-50,6.295459397093541e-101] , Actual [0.2 0.1] \n",
      "Predict [1.9055447411326415e-15,1.4207201911029192e-40] , Actual [0.2 0.1] \n",
      "Predict [3.5562695749355035e-17,1.734169725628119e-64] , Actual [0.2 0.1] \n",
      "Predict [8.039016796688046e-13,6.563644120259849e-39] , Actual [0.2 0.1] \n",
      "Predict [1.550125367386371e-14,9.656926427340386e-36] , Actual [0.2 0.1] \n",
      "Predict [6.598540860302011e-18,3.725344361316017e-53] , Actual [0.2 0.1] \n",
      "Predict [1.3913912849684255e-21,6.026399525299559e-51] , Actual [0.2 0.1] \n",
      "Predict [1.091375689765911e-21,5.489474708848275e-63] , Actual [0.2 0.1] \n",
      "Predict [2.225945112031574e-15,7.628976893147471e-51] , Actual [0.2 0.1] \n",
      "Predict [1.1524935480860133e-17,1.0970007674371443e-61] , Actual [0.2 0.1] \n",
      "Predict [3.454724909885851e-21,1.0575227929429414e-60] , Actual [0.2 0.1] \n",
      "Predict [1.7830885621124374e-10,8.946086968831778e-35] , Actual [0.2 0.1] \n",
      "Predict [7.900653904903818e-17,1.4414809145477055e-50] , Actual [0.2 0.1] \n",
      "Predict [1.1755673790987696e-14,4.879481826239887e-46] , Actual [0.2 0.1] \n",
      "Predict [5.729915110580132e-15,3.4667025571961965e-61] , Actual [0.2 0.1] \n",
      "Predict [2.2679077343136482e-14,1.3539781147653929e-42] , Actual [0.2 0.1] \n",
      "Predict [7.537437102152642e-14,4.29442513530974e-40] , Actual [0.2 0.1] \n",
      "Predict [3.113052849909488e-18,1.1744355328148788e-56] , Actual [0.2 0.1] \n",
      "Predict [4.324803896342912e-21,4.844638324132639e-84] , Actual [0.2 0.1] \n",
      "Predict [1.8787426231043975e-14,1.8348174174557366e-47] , Actual [0.2 0.1] \n",
      "Predict [8.782514293164797e-16,3.622018608710167e-46] , Actual [0.2 0.1] \n",
      "Predict [1.1120916346727977e-14,8.387554634889088e-46] , Actual [0.2 0.1] \n",
      "Predict [8.79277840654303e-27,2.4316431383076518e-76] , Actual [0.2 0.1] \n",
      "MAE =  0.16942244992624308\n",
      "\t-----------\t \t-----------\t\n",
      "|\t tn - 108 \t|,|\t fp - 0 \t|\n",
      "|\t fn - 6 \t|,|\t tp - 0 \t|\n",
      "\t-----------\t \t-----------\t\n",
      "Acc = 0.9473684210526315 , f1 = 0.9217638691322901\n",
      "RMSE =  0.217034051013808\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "system_f = fuzzy_aggregation(tree_shape,current_fold_train=best_fold,ignore_writeClass=True).print_results(best_sol_cross,pos_start,pos_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T10:58:27.360062Z",
     "start_time": "2020-02-26T10:58:27.175146Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Train count abnormal :  6\n",
      "6 114\n",
      "108 114\n",
      "Weight layer  0 : 1   [1.4482850076600526, 0.9134670799227516, 0.2996850450322843, 1.2177914619407137, 1.561939931743425, 1.170787256278359, 0.19366373497143222, 1.1943804824509807]\n",
      "Gammar operator :  0.6736300070234617\n",
      "Sum Weights :  7.999999999999999\n",
      "\n",
      "Weight layer  0 : 2   [1.9145459153339703, 0.4407401398133359, 1.8231390893261075, 1.7813938815238406, 0.00826007643163546, 1.1770565971763627, 0.530869567216673, 0.32399473317807437]\n",
      "Gammar operator :  0.5317335592639968\n",
      "Sum Weights :  7.999999999999999\n",
      "\n",
      "Weight layer  0 : 3   [0.7413322308408774, 0.9359885678061056, 0.43183615212487864, 1.274507826330311, 1.4020956108716833, 0.4039427805059122, 1.0423164230654969, 1.7679804084547344]\n",
      "Gammar operator :  0.28607416394628904\n",
      "Sum Weights :  7.999999999999999\n",
      "\n",
      "Weight layer  0 : 4   [1.3259325625069862, 0.8611393433063791, 1.672099896626305, 0.45328679484995477, 1.5125303774919385, 1.6597006450172012, 0.4136592131847799, 0.10165116701645531]\n",
      "Gammar operator :  0.5195357869233138\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 5   [1.4792100194598168, 0.2833231805725664, 0.439292671311558, 0.7773691975591538, 0.037903785858677756, 1.3550122623336895, 1.8078226493852354, 1.8200662335193025]\n",
      "Gammar operator :  0.2235622165608856\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 6   [0.6325027766926204, 0.8456346772020422, 0.11078456895862486, 1.8874814512916736, 2.162955625785783, 1.2858896966881874, 0.7029304355973923, 0.3718207677836759]\n",
      "Gammar operator :  0.5282420731170976\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 7   [1.1474512382665025, 1.3384168275464015, 1.1988782276832353, 1.2257176450370657, 0.9583846366113118, 0.9978162129293172, 0.2749646997873067, 0.8583705121388601]\n",
      "Gammar operator :  0.6652342029560889\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 8   [1.4897894406171819, 1.1640698407211147, 0.2711615533859059, 0.7216704057721979, 1.1652887559042728, 1.1131626348931034, 0.7524600581202668, 1.3223973105859559]\n",
      "Gammar operator :  0.21481534864269203\n",
      "Sum Weights :  7.999999999999999\n",
      "\n",
      "Weight layer  0 : 9   [1.125378778825963, 1.2450853834987414, 1.4914414740818303, 1.0813472599987035, 0.6639379594323609, 1.2952071949282091, 0.04659758525004165, 1.0510043639841506]\n",
      "Gammar operator :  0.8415122257235772\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 10   [1.5171690816157506, 1.061144042207137, 0.33522015244970427, 1.4340551769291023, 0.848237539808443, 0.9969745430897113, 0.7535420907361934, 1.0536573731639582]\n",
      "Gammar operator :  0.5716158910008132\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 11   [1.0158130732241675, 0.5732944018472195, 1.3010387712418554, 0.8612735319225153, 0.4340159375992624, 1.3870952981590807, 1.2693664693375155, 1.1581025166683834]\n",
      "Gammar operator :  0.6041413326155802\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 12   [1.3121378062389704, 0.818275085961491, 0.8474831483344732, 1.131672612901007, 0.4677917777315339, 1.4871758799603834, 1.1304730710598097, 0.8049906178123317]\n",
      "Gammar operator :  0.279805448835971\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 13   [0.467688042685394, 0.6284418246390772, 1.9697244011808779, 0.16760425085553965, 1.5545829959293451, 0.598597078915572, 1.6740792336181332, 0.9392821721760604]\n",
      "Gammar operator :  0.8883874424247836\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 14   [0.9114241473098984, 1.0869109066829237, 0.9705791701432799, 0.8245116760277963, 1.5457815777792046, 1.2683491916183498, 1.205008891406293, 0.1874344390322533]\n",
      "Gammar operator :  0.9293682458828848\n",
      "Sum Weights :  7.999999999999998\n",
      "\n",
      "Weight layer  0 : 15   [0.48086602577025095, 1.9759292892915727, 2.0319146209895496, 0.5716460324000194, 1.0598396509383687, 0.1869303804241618, 1.5743096328534854, 0.118564367332591]\n",
      "Gammar operator :  0.053097657435964285\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 16   [0.08151332361571308, 0.33090681443878533, 2.2168353085996704, 1.1419535539761265, 0.6657601813283361, 2.1787072493495288, 0.635560838068784, 0.7487627306230555]\n",
      "Gammar operator :  0.12395714168641925\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 17   [1.5599247104031402, 0.3350601942131199, 0.510149730679018, 1.4270646868059504, 1.482608964564241, 0.39081869190781776, 0.9705454823988516, 1.3238275390278615]\n",
      "Gammar operator :  0.11527450295028119\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 18   [0.6388485595026556, 1.7263527089894317, 0.6062683339001039, 1.3681235655171198, 1.4865239899297897, 2.1228195753031125, 0.04666668797859866, 0.004396578879189258]\n",
      "Gammar operator :  0.23434102348908759\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 19   [1.4037282499307324, 2.3475571130765642, 0.6515807590230588, 0.9682708547161271, 0.7706972472344812, 0.9389563280789636, 0.7678343336587593, 0.15137511428131334]\n",
      "Gammar operator :  0.46951935541684997\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 20   [1.579337363507058, 1.8532147353351391, 0.030393732464975398, 2.2283223386481628, 0.2810170813954929, 0.37255642488532853, 0.34017039101881125, 1.314987932745032]\n",
      "Gammar operator :  0.3409208324336831\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 21   [0.22354887237669732, 0.5420004267025165, 1.154136546586142, 2.3202659360944393, 1.52356871793846, 0.7322027968606946, 0.8553280792469671, 0.6489486241940841]\n",
      "Gammar operator :  0.0046854158089773\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 22   [1.036143623450342, 1.8799496926135575, 0.9286106480130434, 0.07758595546634699, 0.6110149426009222, 1.1234849447737714, 0.5114351755176817, 1.8317750175643346]\n",
      "Gammar operator :  0.967123825256325\n",
      "Sum Weights :  7.999999999999999\n",
      "\n",
      "Weight layer  0 : 23   [0.6230980411709441, 0.4982314434064999, 1.0578751186872992, 1.3904879766074167, 1.1209552439908304, 1.2405838541285432, 1.3337129672909604, 0.7350553547175065]\n",
      "Gammar operator :  0.3782162036619099\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 24   [1.3298002762802363, 0.9438022373775733, 0.868655446703261, 0.7539941009124919, 1.5347591755323395, 1.5688939511363806, 0.3964598826547301, 0.6036349294029876]\n",
      "Gammar operator :  0.5859622802012977\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 25   [1.0657720714648242, 1.5405575305554027, 0.3103221048430489, 1.5027575102164228, 1.3972263440650452, 0.09669397199430434, 1.7078985802424826, 0.3787718866184709]\n",
      "Gammar operator :  0.764516632886344\n",
      "Sum Weights :  8.000000000000002\n",
      "\n",
      "Weight layer  0 : 26   [1.4406550721479723, 0.35359943315326775, 1.644817890159293, 1.3108432103036893, 0.9915305481913257, 1.4410712191159198, 0.7572517746868226, 0.06023085224171082]\n",
      "Gammar operator :  0.04122057760640219\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 27   [1.0690744304359, 0.6413023526630899, 1.0604590227621007, 1.1790661791363128, 1.0365321128889542, 0.9198489621701577, 0.8638833913833729, 1.2298335485601126]\n",
      "Gammar operator :  0.4085972782020626\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 28   [0.7681103097162367, 0.6716016264206005, 1.8049207272101486, 1.2103265785254163, 1.6459931554496825, 1.6617938324902324, 0.1477074909323573, 0.0895462792553266]\n",
      "Gammar operator :  0.7443737081317497\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 29   [1.1909179038014444, 1.1898652166742265, 1.3234620656941354, 1.7309167082707309, 1.605958027417726, 0.5410529828853272, 0.34468795384586165, 0.0731391414105476]\n",
      "Gammar operator :  0.7428402002695899\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 30   [1.6411736315637089, 0.41000203059854357, 1.0922186661532052, 0.5155951521184128, 0.15598812824014455, 1.5616082736336245, 1.7161276240938672, 0.9072864935984936]\n",
      "Gammar operator :  0.19150468441934687\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 31   [0.05799570465608974, 0.6286074492593983, 0.9074268842342652, 2.2374354392265037, 1.1555707152676513, 2.226769303951392, 0.2989357069406661, 0.48725879646403303]\n",
      "Gammar operator :  0.6534185416534709\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 32   [1.638210207849492, 1.3284630379095905, 1.2189935390976918, 0.49373423990674975, 1.4470383636892954, 0.33735777166722075, 0.9349783689473342, 0.6012244709326259]\n",
      "Gammar operator :  0.267408138371089\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 33   [0.8253931672245695, 2.4314459991252506, 0.5332573962870518, 0.47873308128608505, 0.8022376406405669, 0.5082992475942226, 0.5235249655533768, 1.8971085022888765]\n",
      "Gammar operator :  0.26789565605167387\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 34   [0.8377134606797425, 1.281654709281307, 1.172320805789735, 1.0023639080795728, 1.012301275613765, 1.258224063902094, 0.9464353864909635, 0.48898639016281936]\n",
      "Gammar operator :  0.02628499478043711\n",
      "Sum Weights :  7.999999999999999\n",
      "\n",
      "Weight layer  0 : 35   [1.4990991099066224, 0.6906629806439737, 0.6196688168160088, 0.21839660267069438, 0.507278191265719, 1.7827548573728687, 0.9603229820722178, 1.7218164592518945]\n",
      "Gammar operator :  0.08354821389411271\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 36   [1.2821450942449888, 1.1739052838812372, 0.4192670821868695, 1.2925441976694125, 0.6566051605004196, 0.07035100640988898, 1.714744080861622, 1.390438094245561]\n",
      "Gammar operator :  0.16327817541938872\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 37   [1.5853953016578592, 0.9648607786590417, 1.771792409233534, 0.21040738757029281, 0.7603500053533334, 1.273264540892783, 0.734499078163494, 0.6994304984696617]\n",
      "Gammar operator :  0.113749401981728\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 38   [1.2635936413059836, 0.7741903940865058, 0.13511884182626255, 0.5163248068406938, 0.5411024231363423, 2.2670143164586682, 0.5838602353626101, 1.9187953409829335]\n",
      "Gammar operator :  0.5117565128816864\n",
      "Sum Weights :  7.999999999999999\n",
      "\n",
      "For check sum at layer:  304.0\n",
      "\n",
      "Weight layer  1 : 1   [0.8556485243824066, 0.829780904163889, 1.3145705714537044]\n",
      "Gammar operator :  0.12646553580066477\n",
      "Sum Weights :  3.0\n",
      "\n",
      "Weight layer  1 : 2   [0.7602326683918889, 1.239767331608111]\n",
      "Gammar operator :  0.7543087296775837\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 3   [1.639731831693583, 0.3602681683064171]\n",
      "Gammar operator :  0.09803779004032209\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 4   [1.4027187860372048, 1.6684058571469456, 0.7537187777969131, 0.17515657901893628]\n",
      "Gammar operator :  0.8367908869139364\n",
      "Sum Weights :  4.0\n",
      "\n",
      "Weight layer  1 : 5   [1.1085209905901017, 1.504601400794176, 0.3414214138014056, 1.0454561948143164]\n",
      "Gammar operator :  0.6249108456090046\n",
      "Sum Weights :  3.9999999999999996\n",
      "\n",
      "Weight layer  1 : 6   [0.9279541997781872, 1.0720458002218127]\n",
      "Gammar operator :  0.9589470851750267\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 7   [1.0]\n",
      "Gammar operator :  0.7762273069353872\n",
      "Sum Weights :  1.0\n",
      "\n",
      "Weight layer  1 : 8   [2.2409658257311658, 0.13256460648019774, 0.15780567556736347, 1.4686638922212736]\n",
      "Gammar operator :  0.8056049700653221\n",
      "Sum Weights :  4.0\n",
      "\n",
      "Weight layer  1 : 9   [0.6784608478105151, 1.3215391521894846]\n",
      "Gammar operator :  0.17766034908645967\n",
      "Sum Weights :  1.9999999999999996\n",
      "\n",
      "Weight layer  1 : 10   [1.0]\n",
      "Gammar operator :  0.781713170940501\n",
      "Sum Weights :  1.0\n",
      "\n",
      "Weight layer  1 : 11   [1.9753684237552782, 0.02463157624472166]\n",
      "Gammar operator :  0.5252242029677344\n",
      "Sum Weights :  1.9999999999999998\n",
      "\n",
      "Weight layer  1 : 12   [0.6790855815492167, 1.3209144184507833]\n",
      "Gammar operator :  0.4355572160560953\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 13   [0.3041929112224511, 1.01434796053833, 1.1801627163747905, 0.9763299443998843, 1.5249664674645438]\n",
      "Gammar operator :  0.7582334864622691\n",
      "Sum Weights :  5.0\n",
      "\n",
      "Weight layer  1 : 14   [1.0]\n",
      "Gammar operator :  0.7312334421661609\n",
      "Sum Weights :  1.0\n",
      "\n",
      "Weight layer  1 : 15   [1.0]\n",
      "Gammar operator :  0.30721601933462317\n",
      "Sum Weights :  1.0\n",
      "\n",
      "Weight layer  1 : 16   [1.2675147186922675, 0.7324852813077325]\n",
      "Gammar operator :  0.3002949412347703\n",
      "Sum Weights :  2.0\n",
      "\n",
      "For check sum at layer:  38.0\n",
      "\n",
      "Weight layer  2 : 1   [1.3378813246386967, 0.6621186753613031]\n",
      "Gammar operator :  0.5985572834587692\n",
      "Sum Weights :  1.9999999999999998\n",
      "\n",
      "Weight layer  2 : 2   [0.6471934780672675, 0.31876684337025585, 0.8931997949276727, 1.4525417026649339, 1.6882981809698703]\n",
      "Gammar operator :  0.43336714308407653\n",
      "Sum Weights :  5.0\n",
      "\n",
      "Weight layer  2 : 3   [1.151257593869497, 1.0363677565429201, 0.8123746495875832]\n",
      "Gammar operator :  0.3252166836964242\n",
      "Sum Weights :  3.0\n",
      "\n",
      "Weight layer  2 : 4   [1.5177860725620995, 0.4822139274379003]\n",
      "Gammar operator :  0.3589440531323834\n",
      "Sum Weights :  1.9999999999999998\n",
      "\n",
      "Weight layer  2 : 5   [1.0103856280465169, 0.9896143719534832]\n",
      "Gammar operator :  0.019366139547628225\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  2 : 6   [0.5914775399538699, 1.4085224600461304]\n",
      "Gammar operator :  0.2034551200469108\n",
      "Sum Weights :  2.0\n",
      "\n",
      "For check sum at layer:  16.0\n",
      "\n",
      "Weight layer  3 : 1   [0.39518208596143006, 0.9780459493124452, 1.1052362018721542, 1.5063069532179225, 0.472545620467393, 1.5426831891686543]\n",
      "Gammar operator :  0.7046210824762865\n",
      "Sum Weights :  6.0\n",
      "\n",
      "Weight layer  3 : 2   [2.4493289273023975, 2.2336111153599285, 0.00930567230304123, 0.7818734962966822, 0.3057608127266894, 0.22011997601126057]\n",
      "Gammar operator :  0.27485361008564224\n",
      "Sum Weights :  5.999999999999999\n",
      "\n",
      "For check sum at layer:  12.0\n",
      "\n",
      "\n",
      "#### Left output node ####\n",
      "liquidity ratio :  0.39518208596143006\n",
      "Asset utilization or turnover ratio :  0.9780459493124452\n",
      "Profitability :  1.1052362018721542\n",
      "Gammar operator :  1.5063069532179225\n",
      "Leverage :  0.472545620467393\n",
      "Growth ratios :  1.5426831891686543\n",
      "Gammar operator :  0.7046210824762865\n",
      "For check sum :  6.0\n",
      "\n",
      "#### Right output node ####\n",
      "liquidity ratio :  2.4493289273023975\n",
      "Asset utilization or turnover ratio :  2.2336111153599285\n",
      "Profitability :  0.00930567230304123\n",
      "Asset structure ratios :  0.7818734962966822\n",
      "Leverage :  0.3057608127266894\n",
      "Growth ratios :  0.22011997601126057\n",
      "Gammar operator :  0.27485361008564224\n",
      "For check sum :  5.999999999999998\n"
     ]
    }
   ],
   "source": [
    "system_f = fuzzy_aggregation(tree_shape,ignore_writeClass=True)\n",
    "system_f.set_allWeights(best_sol_cross)\n",
    "system_f.showAllWeights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Mail Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T11:02:13.855134Z",
     "start_time": "2020-02-26T11:02:11.794245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"status\":200,\"message\":\"ok\"}'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "if _select_sw_train == 0 :\n",
    "#flower pollination\n",
    "    str_out = \"training FP with D=%s,NP = %s, nFES=%s, p=%s \" % (train_params[\"dimension\"],train_params[\"NP\"], train_params[\"nFES\"], train_params[\"p\"])\n",
    "\n",
    "if _select_sw_train == 1 :\n",
    "#firefly\n",
    "    str_out = \"training FF with D=%s,NP = %s, nFES=%s, alpha=%s, betamin=%s, gamma = %s \" % (train_params[\"dimension\"],train_params[\"NP\"], train_params[\"nFES\"], train_params[\"alpha\"],train_params[\"betamin\"],train_params[\"gamma\"])\n",
    "\n",
    "\n",
    "#GWO\n",
    "if _select_sw_train == 2 :\n",
    "    str_out = \"training GWO with D=%s,NP = %s, nFES=%s, A = %s, C = %s  \" % (train_params[\"dimension\"],train_params[\"NP\"], train_params[\"nFES\"],train_params[\"A_val\"],train_params[\"C_val\"] )\n",
    "\n",
    "dt = datetime.now()\n",
    "str_out = \"\\n\"+ dt.strftime(\"%d/%m/%Y %H:%M\") + \" à¸™.\\n\" + str_out\n",
    "    \n",
    "\n",
    "url = \"https://notify-api.line.me/api/notify\"\n",
    "\n",
    "payload = {'message': str_out}\n",
    "files = [\n",
    "\n",
    "]\n",
    "headers = {\n",
    "  'Authorization': 'Bearer bR3qarCoaVfI14aKFkSEOARPU4LMc8ZDfKpUiki1NNl',\n",
    "  'Content-Type': 'multipart/form-data; boundary=--------------------------432408813026345718460603'\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, params = payload, files = files)\n",
    "\n",
    "print(response.text.encode('utf8'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "453px",
    "width": "168px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 377.85,
   "position": {
    "height": "503.85px",
    "left": "298px",
    "right": "51px",
    "top": "300px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
