{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T11:03:05.238078Z",
     "start_time": "2020-02-26T11:03:05.226295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"notebook_name = \" + \"'\"+thename+\"'\";\n",
       "kernel.execute(command);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import Javascript\n",
    "Javascript(\"\"\"\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"notebook_name = \" + \"'\"+thename+\"'\";\n",
    "kernel.execute(command);\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T11:03:05.675483Z",
     "start_time": "2020-02-26T11:03:05.664810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Fuzzy_Aggregation3-Version91-FullTree_SSA\n"
     ]
    }
   ],
   "source": [
    "print(notebook_name)\n",
    "def getFileNumber():\n",
    "    if \"Copy\" not in notebook_name :\n",
    "        return \"1\"\n",
    "    \n",
    "    temp = notebook_name.split(\".\")[0]\n",
    "    temp = temp.split(\"-\")[-1]\n",
    "    temp = temp.replace(\"Copy\",\"\")\n",
    "    return str(int(temp) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:55.552662Z",
     "start_time": "2020-02-25T07:53:55.547603Z"
    }
   },
   "outputs": [],
   "source": [
    "#13-12-62\n",
    "#Using own file library\n",
    "from ssa_module.ssaC import ssa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:55.601049Z",
     "start_time": "2020-02-25T07:53:55.564156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "#25-2-63\n",
    "\n",
    "#Using Pocket Algor \n",
    "\n",
    "\n",
    "#https://grega.xyz/post/niapy_optimize_knn/  \n",
    "%autosave 60\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import bisect\n",
    "import math\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Display graph sequently\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "#set_trace()\n",
    "\n",
    "import random\n",
    "import gc\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:55.614664Z",
     "start_time": "2020-02-25T07:53:55.608703Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert to categorical\n",
    "def to_categorical(val,left_v,right_v) :\n",
    "    if left_v < 0.1 :\n",
    "        left_v = 0.2\n",
    "    if right_v < 0.1 :\n",
    "        right_v = 0.2\n",
    "    output = []\n",
    "    for v in val :\n",
    "        if v < 0.5 :\n",
    "            output.append([left_v,0.1])\n",
    "        else :\n",
    "            output.append([0.1,right_v])\n",
    "    return np.asarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:55.640693Z",
     "start_time": "2020-02-25T07:53:55.619583Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred,classes,normalize=False,title=None,cmap=plt.cm.Blues):\n",
    "    y_true = np.asarray(y_true).astype('int')\n",
    "    y_pred = np.asarray(y_pred).astype('int')\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    display.display(fig)\n",
    "    plt.close(fig)\n",
    "#     return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller to show result of Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:55.665966Z",
     "start_time": "2020-02-25T07:53:55.648383Z"
    }
   },
   "outputs": [],
   "source": [
    "class StoreOptimization(object) :\n",
    "    results = []\n",
    "    generation = 0\n",
    "    \n",
    "    count_generation = 0\n",
    "    \n",
    "    store_best_weights = []\n",
    "    store_best_error = 1\n",
    "    \n",
    "    #default\n",
    "    n_generation_wanted = 1\n",
    "    \n",
    "    #Pocket Algorithm\n",
    "    '''\n",
    "    Hs - history of total correct class\n",
    "    Hs1 - history class 1 correct class\n",
    "    Hs2 - history class 2 correct class\n",
    "    '''\n",
    "    \n",
    "    Hs = 0 \n",
    "    Hs1 = 0\n",
    "    Hs2 = 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def static_N_Gen(val):\n",
    "         StoreOptimization.n_generation_wanted = val\n",
    "    \n",
    "    @staticmethod\n",
    "    def static_clearAll() :\n",
    "        StoreOptimization.generation = 0\n",
    "        StoreOptimization.count_generation = 0\n",
    "        StoreOptimization.store_best_weights = []\n",
    "        StoreOptimization.store_best_error = 1\n",
    "        StoreOptimization.Hs = 0\n",
    "        StoreOptimization.Hs1 = 0\n",
    "        StoreOptimization.Hs2 = 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def static_appendString(head_str) :\n",
    "        logging.info('%s' % (head_str))\n",
    "        print('%s' % (head_str))\n",
    "    \n",
    "    @staticmethod\n",
    "    def static_addResult(Pc1,Pc2,Pc,error_rmse,weights,n_fold) :\n",
    "        \n",
    "            \n",
    "        StoreOptimization.count_generation = StoreOptimization.count_generation + 1\n",
    "        StoreOptimization.generation = StoreOptimization.generation + 1\n",
    "        \n",
    "        if StoreOptimization.n_generation_wanted == StoreOptimization.count_generation :\n",
    "            StoreOptimization.count_generation = 0\n",
    "            StoreOptimization.static_appendString('fold %s nFES %s error %s' % (n_fold,StoreOptimization.generation, error_rmse))\n",
    "        \n",
    "        if (Pc1 > StoreOptimization.Hs1 or Pc2 > StoreOptimization.Hs2) and (Pc > StoreOptimization.Hs) :\n",
    "            StoreOptimization.store_best_error = error_rmse \n",
    "            StoreOptimization.store_best_weights = weights\n",
    "            StoreOptimization.Hs1 = Pc1\n",
    "            StoreOptimization.Hs2 = Pc2\n",
    "            StoreOptimization.Hs = Pc\n",
    "            StoreOptimization.static_appendString('fold %s best at nFES %s error %s (%s,%s,%s)' % (n_fold,StoreOptimization.generation, error_rmse,Pc1,Pc2,Pc))\n",
    "            logging.info(weights)\n",
    "\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:55.697044Z",
     "start_time": "2020-02-25T07:53:55.672963Z"
    }
   },
   "outputs": [],
   "source": [
    "class leaf_node :\n",
    "    def __init__(self,count_leaves) :\n",
    "        self.list_weight_input = np.ones(count_leaves).tolist()\n",
    "        self.gammar_operator = 1.0\n",
    "        \n",
    "    def check_input(self,val) :\n",
    "        '''\n",
    "        Axiom 1\n",
    "        Check input h(0,0,0) = 0\n",
    "        Check input h(1,1,1) = 1\n",
    "        '''\n",
    "        repeat_zeros = np.repeat(0.00000001,len(val))\n",
    "        repeat_ones = np.repeat(0.99999999,len(val))\n",
    "        \n",
    "        if np.sum(val == repeat_zeros) == len(val) :\n",
    "            return np.repeat(0.0,len(val))\n",
    "        \n",
    "        if np.sum(val == repeat_ones) == len(val) :\n",
    "            return np.repeat(1.0,len(val))\n",
    "            \n",
    "        return val\n",
    "        \n",
    "        \n",
    "    def cal_y(self,input_x) :\n",
    "        \n",
    "        input_x = self.check_input(input_x)\n",
    "         \n",
    "        if len(input_x) != len(self.list_weight_input) :\n",
    "            print(\"Error cal , input not equal weight\")\n",
    "            return\n",
    "    \n",
    "        \n",
    "        temp_mul_y1 = [ np.power(input_x[idx] , self.list_weight_input[idx]) for idx in range(0,len(input_x)) ]\n",
    "        \n",
    "        prod_y1 = 1\n",
    "        for idx in  temp_mul_y1  :\n",
    "            prod_y1 = prod_y1 * idx\n",
    "            \n",
    "        y1_out = np.power(prod_y1,1-self.gammar_operator)\n",
    "        \n",
    "        temp_mul_y2 = [ np.power(1-input_x[idx],self.list_weight_input[idx]) for idx in range(0,len(input_x)) ]\n",
    "        \n",
    "        prod_y2 = 1\n",
    "        for idx in  temp_mul_y2  :\n",
    "            prod_y2 = prod_y2 * idx\n",
    "            \n",
    "        y2_out = np.power(1 - prod_y2,self.gammar_operator)\n",
    "        \n",
    "        real_out = y1_out * y2_out\n",
    "         \n",
    "        if real_out == 0 :\n",
    "            real_out = 0.00000001\n",
    "        elif real_out == 1 :\n",
    "            real_out = 0.99999999\n",
    "            \n",
    "        return real_out\n",
    "\n",
    "    def change_weights(self,new_weight,new_gammar) :\n",
    "\n",
    "        if self.count_weight() == len(new_weight) :\n",
    "            self.list_weight_input = new_weight\n",
    "            self.gammar_operator = new_gammar\n",
    "\n",
    "        else :\n",
    "            print(\"Error Update weight!!!!!\")\n",
    "            raise\n",
    "            \n",
    "    def count_weight(self) :\n",
    "        return len(self.list_weight_input)\n",
    "    \n",
    "    def get_weights_gammar(self) :\n",
    "        return self.list_weight_input + [self.gammar_operator]\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:55.831855Z",
     "start_time": "2020-02-25T07:53:55.700663Z"
    }
   },
   "outputs": [],
   "source": [
    "class root_tree :\n",
    "    \n",
    "    def __init__(self,list_group) :\n",
    "        self.number_layers = len(list_group)\n",
    "        self.all_nodes = []\n",
    "        \n",
    "\n",
    "        #Leaf (Top) -> Root (Bottom)\n",
    "        \n",
    "        for count_layer in range(0,len(list_group)) :\n",
    "            temp_leaves = []\n",
    "            for count_node in  range(0,len(list_group[count_layer])) :\n",
    "                temp_leaves.append(leaf_node(list_group[count_layer][count_node]))\n",
    "            self.all_nodes.append(temp_leaves)\n",
    "            \n",
    "        #Split vector input to input each group\n",
    "        self.group_input = list_group\n",
    "\n",
    "    \n",
    "    def get_all_weights_wGammar(self) :\n",
    "        #Return weight and gammar all tree to vector\n",
    "        output = []\n",
    "    \n",
    "        for each_layer in self.all_nodes :\n",
    "            for each_node in each_layer :\n",
    "                \n",
    "                #New position vector X\n",
    "                output = output+each_node.get_weights_gammar()\n",
    "            \n",
    "        return output\n",
    "                \n",
    "    def set_all_weights_wGammar(self,new_weights) :\n",
    "        count_weight_idx = 0\n",
    "        \n",
    "        for each_layer in self.all_nodes :\n",
    "            for each_node in each_layer :\n",
    "                \n",
    "                len_weight = each_node.count_weight()\n",
    "                temp_weights = new_weights[count_weight_idx:count_weight_idx + len_weight]\n",
    "                temp_gammar = new_weights[count_weight_idx+len_weight]\n",
    "                \n",
    "                \n",
    "                #Change weights\n",
    "                try : \n",
    "                    each_node.change_weights(temp_weights,temp_gammar)\n",
    "                except :\n",
    "                    print(\"Error change weight !\")\n",
    "                    sys.exit()\n",
    "                \n",
    "                #New position\n",
    "                count_weight_idx = count_weight_idx+len_weight+1\n",
    "        \n",
    "        \n",
    "                \n",
    "          \n",
    "    #For multi objective to find error\n",
    "    def predict_eachLeaf(self,input_X) :\n",
    "        \n",
    "        summation_error = 0\n",
    "        \n",
    "        output_previous_layer = input_X\n",
    "        \n",
    "        summation_resultY = []\n",
    "        \n",
    "        #feed only leaves tree \n",
    "        for idx,each_layer in enumerate(self.all_nodes) :\n",
    "\n",
    "            count_idx = 0\n",
    "            temp_previous_result = []\n",
    "            \n",
    "            if idx != len(self.all_nodes) - 1 :\n",
    "            \n",
    "                for each_node in each_layer :\n",
    "\n",
    "                    input_x_vector = output_previous_layer[count_idx:count_idx + each_node.count_weight()]\n",
    "\n",
    "                    resultFrom_calY = each_node.cal_y(input_x_vector)\n",
    "\n",
    "                    summation_resultY =  summation_resultY + [resultFrom_calY]\n",
    "\n",
    "                    temp_previous_result.append(resultFrom_calY)\n",
    "\n",
    "                    #New position vector X\n",
    "                    count_idx = count_idx+each_node.count_weight()\n",
    "            else :\n",
    "                \n",
    "                #Get old output from previous cal and calculate two output to classify\n",
    "        \n",
    "                temp_output2class = []\n",
    "                for each_node in each_layer :\n",
    "                    input_x_vector = output_previous_layer[0:count_idx + each_node.count_weight()]\n",
    " \n",
    "                    resultFrom_calY = each_node.cal_y(input_x_vector)\n",
    "                    temp_output2class.append(resultFrom_calY)\n",
    "\n",
    "                summation_resultY =  summation_resultY + temp_output2class\n",
    " \n",
    "            output_previous_layer =  temp_previous_result\n",
    "\n",
    "        return summation_resultY\n",
    "    \n",
    "    \n",
    "    #Feed input to get output only\n",
    "    def predict(self,input_X) :\n",
    "        temp = self.predict_eachLeaf(input_X)[-2::1]\n",
    "        if (temp[0] == 0 and temp[1] == 0) or np.isinf(temp[0]) or np.isinf(temp[1]) or np.isnan(temp[0]) or np.isnan(temp[1]) :\n",
    "            temp = self.predict_eachLeaf(input_X)[-2::1]\n",
    "        return temp\n",
    "        \n",
    "    \n",
    "    def readable(self) :\n",
    "        #Return weight and gammar all tree to vector\n",
    "        temp = []\n",
    "        for (idx,each_layer) in enumerate(self.all_nodes) :\n",
    "            temp = []\n",
    "            for (i,each_node) in enumerate(each_layer) :\n",
    "                temp_get_weight = each_node.get_weights_gammar()\n",
    "                print(\"Weight layer \",idx,\":\",i+1,\" \",temp_get_weight[0:-1])\n",
    "                print(\"Gammar operator : \",temp_get_weight[-1])\n",
    "                print(\"Sum Weights : \",np.sum(temp_get_weight[:-1]))\n",
    "                print(\"\")\n",
    "                #Show important weight each group ratios at final step\n",
    "                \n",
    "                temp.append(temp_get_weight)\n",
    "        \n",
    "            print(\"For check sum at layer: \",np.sum([np.sum(x[0:-1]) for x in temp]))\n",
    "            print(\"\")\n",
    "                \n",
    "        print(\"\")\n",
    "        print(\"#### Left output node ####\")\n",
    "        print(\"liquidity ratio : \",temp[0][0])\n",
    "        print(\"Asset utilization or turnover ratio : \",temp[0][1])\n",
    "        print(\"Profitability : \",temp[0][2])\n",
    "        print(\"Gammar operator : \",temp[0][3])\n",
    "        print(\"Leverage : \",temp[0][4])\n",
    "        print(\"Growth ratios : \",temp[0][5])\n",
    "        print(\"Gammar operator : \",temp[0][6])\n",
    "        print(\"For check sum : \",np.sum(temp[0]) - temp[0][-1])\n",
    "        print(\"\")\n",
    "        print(\"#### Right output node ####\")\n",
    "        print(\"liquidity ratio : \",temp[1][0])\n",
    "        print(\"Asset utilization or turnover ratio : \",temp[1][1])\n",
    "        print(\"Profitability : \",temp[1][2])\n",
    "        print(\"Asset structure ratios : \",temp[1][3])\n",
    "        print(\"Leverage : \",temp[1][4])\n",
    "        print(\"Growth ratios : \",temp[1][5])\n",
    "        print(\"Gammar operator : \",temp[1][6])\n",
    "        print(\"For check sum : \",np.sum(temp[1])-temp[1][-1])\n",
    "    \n",
    "    def countAllNodesOutput(self) :\n",
    "        output = 0\n",
    "        for count_layer in self.group_input :\n",
    "            output = output + len(count_layer)\n",
    "        return output\n",
    "                \n",
    "\n",
    "    \n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:55.873469Z",
     "start_time": "2020-02-25T07:53:55.837943Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from NiaPy.benchmarks.benchmark import Benchmark\n",
    "\n",
    "def try_div(x,y):\n",
    "    try: \n",
    "        return x/y\n",
    "    except ZeroDivisionError: \n",
    "        return 0\n",
    "\n",
    "    \n",
    "def swap_gammar(val) :\n",
    "    if math.isnan(val) :\n",
    "        return 0\n",
    "    return val\n",
    "\n",
    "    \n",
    "def swap_weight(val) :\n",
    "    len_weight = len(val)\n",
    "    temp_multiply =  try_div(len_weight,np.sum(val))\n",
    "    if math.isnan(temp_multiply) or temp_multiply == 0 :\n",
    "        temp_multiply = 1\n",
    "        \n",
    "    output = (temp_multiply * np.asarray(val)).tolist()\n",
    "    output = [0 if math.isnan(x) else x for x in output]\n",
    "\n",
    "    return output\n",
    "\n",
    "# our custom benchmark classs\n",
    "class BenchmarkAllTree(object):\n",
    "    def __init__(self,tree_shape,start_pos_f,end_pos_f,n_fold):\n",
    "        # define lower bound of benchmark function\n",
    "        self.Lower = 0\n",
    "        # define upper bound of benchmark function\n",
    "        self.Upper = 1\n",
    "        \n",
    "        self.tree_shape = tree_shape\n",
    "        \n",
    "        self.start_pos_f = start_pos_f\n",
    "        self.end_pos_f = end_pos_f\n",
    "        \n",
    "        self.n_fold = n_fold\n",
    "        \n",
    "        self.fuzzy_system = fuzzy_aggregation(tree_shape = self.tree_shape,current_fold_train=n_fold)\n",
    "        \n",
    "        \n",
    "\n",
    "    # function which returns evaluate function\n",
    "    def evaluate(self,sol):\n",
    "        sol = np.abs(sol)\n",
    "        \n",
    "        all_weight = []\n",
    "\n",
    "        last_idx = 0\n",
    "\n",
    "\n",
    "        for idx,each_layer in enumerate(self.tree_shape) :\n",
    "\n",
    "            if idx == len(self.tree_shape) -1 :\n",
    "\n",
    "                    for each_node in each_layer :\n",
    "                        #find weights and gamma before\n",
    "\n",
    "                        len_each_layer_weights = each_node\n",
    "                        len_each_layer_gammas = len(each_layer)\n",
    "\n",
    "                        temp_pos_next_weights = last_idx + len_each_layer_weights\n",
    "\n",
    "\n",
    "                        temp_vector_weights = swap_weight(sol[last_idx:temp_pos_next_weights])\n",
    "                        temp_vector_gammars = sol[temp_pos_next_weights : temp_pos_next_weights + len_each_layer_gammas]\n",
    "\n",
    "                        temp_idx_w = 0\n",
    "                        temp_idx_g = 0\n",
    "\n",
    "\n",
    "                        all_weight = all_weight + temp_vector_weights[temp_idx_w:temp_idx_w + each_node] + [temp_vector_gammars[temp_idx_g]]\n",
    "                        temp_idx_w = temp_idx_w + each_node\n",
    "                        temp_idx_g = temp_idx_g + 1\n",
    "                        last_idx = last_idx + each_node + 1\n",
    "\n",
    "            else :\n",
    "                #find weights and gamma before\n",
    "                    for each_node in each_layer :\n",
    "                        temp_weights = swap_weight(sol[last_idx:last_idx + each_node])\n",
    "                        temp_gammar = swap_gammar(sol[last_idx + each_node])\n",
    "                        last_idx = last_idx + each_node + 1\n",
    "                        all_weight = all_weight + temp_weights + [temp_gammar]\n",
    "\n",
    "        gc.collect()\n",
    "        set_acc,fitness = self.fuzzy_system.evaluate(all_weight,self.start_pos_f ,self.end_pos_f)\n",
    "        StoreOptimization.static_addResult(set_acc[1],set_acc[4],set_acc[0],fitness,all_weight,self.n_fold)\n",
    "\n",
    "        if math.isnan(fitness) :\n",
    "            set_trace()\n",
    "\n",
    "        return fitness\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:56.027082Z",
     "start_time": "2020-02-25T07:53:55.878178Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class fuzzy_aggregation :\n",
    "    def __init__(self,tree_shape, seed=1235,current_fold_train=1,ignore_writeClass=False) :\n",
    "        self.classifier_root_tree = root_tree(tree_shape)\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.current_fold_train = current_fold_train\n",
    "        global crossvalidation\n",
    "        self.crossvalidation = crossvalidation\n",
    "        \n",
    "        global number_feature\n",
    "        self.number_feature = number_feature\n",
    "        self.all_shape = tree_shape \n",
    "        \n",
    "        global all_data_train\n",
    "        self.all_data_train = all_data_train\n",
    "\n",
    "        random.seed(self.seed)\n",
    "        idx = random.sample(range(0, len(self.all_data_train.index)), len(self.all_data_train.index))\n",
    "\n",
    "\n",
    "#         self.data_train,self.data_blindtest = self.split_blind_test(data_train=np.asarray(self.all_data_train)[idx,:])\n",
    "        self.data_train =np.asarray(self.all_data_train)[idx,:]\n",
    "        \n",
    "        temp_X_train,temp_X_test = self.select_fold(self.current_fold_train)\n",
    "        \n",
    "        #Var for used\n",
    "        self.X_train,self.y_train = self.split_features_class(temp_X_train)\n",
    "        self.X_test,self.y_test = self.split_features_class(temp_X_test)\n",
    "        \n",
    "        \n",
    "#         self.X_blindtest,self.y_blindtest = self.split_features_class(self.data_blindtest)\n",
    "\n",
    "        print(\"Y Train count abnormal : \",len(self.y_train[self.y_train == 1]))\n",
    "    \n",
    "        #set wight output\n",
    "        self.weight_classOne = 1 - (np.sum(self.y_train==1) / len(self.y_train))\n",
    "        self.weight_classZero = 1 - (np.sum(self.y_train==0) / len(self.y_train))\n",
    "        \n",
    "       \n",
    "        self.y_real_train_category = to_categorical(self.y_train,self.weight_classZero,self.weight_classOne)\n",
    "        self.y_real_test_category = to_categorical(self.y_test,self.weight_classZero,self.weight_classOne)\n",
    "        \n",
    "        if not ignore_writeClass :\n",
    "            allWeightOutput = to_categorical([0,1],self.weight_classZero,self.weight_classOne)\n",
    "            StoreOptimization.static_appendString(\"## parameter weight output class zero [%s,%s] \" % (allWeightOutput[0][0],allWeightOutput[0][1]))\n",
    "            StoreOptimization.static_appendString(\"## parameter weight output class one [%s,%s] \" % (allWeightOutput[1][0],allWeightOutput[1][1]))\n",
    "            del allWeightOutput\n",
    "        \n",
    "    def split_blind_test(self,test_size=0.8,data_train = []) :\n",
    "    \n",
    "        range_split= math.floor(len(data_train)*test_size)\n",
    "        return data_train[0:range_split,:], data_train[range_split:,:]\n",
    "    \n",
    "    def select_fold(self,number_fold):\n",
    "        #return train,test\n",
    "\n",
    "        range_v = math.floor(len(self.data_train)/self.crossvalidation)\n",
    "        if number_fold == 1 :\n",
    "            return self.data_train[range_v:,:] , self.data_train[0:range_v,:] \n",
    "        elif number_fold == self.crossvalidation :\n",
    "            return self.data_train[0:range_v*(number_fold-1),:], self.data_train[range_v*(number_fold-1):,:] \n",
    "        else :\n",
    "            temp_data_first = self.data_train[0:range_v*(number_fold-1),:]\n",
    "\n",
    "            temp_data_second = self.data_train[range_v*(number_fold):,:]\n",
    "            final_con = np.concatenate((temp_data_first, temp_data_second))\n",
    "            return final_con , self.data_train[range_v*(number_fold-1):range_v*(number_fold),:]\n",
    "    \n",
    "    def split_features_class(self,v_input) :\n",
    "        #Vector feature , Vector class\n",
    "        return v_input[:,0:self.number_feature],v_input[:,-1]\n",
    "    \n",
    "    def calculate_RMSE(self,y_real,y_predict) :\n",
    "        if len(y_predict) != len(y_real) :\n",
    "            print(\"Cannot calculate RMSE\")\n",
    "            sys.exit()\n",
    "\n",
    "        temp_minus = y_real - y_predict\n",
    "        \n",
    "\n",
    "        error_all = []\n",
    "        for y in temp_minus :\n",
    "            error_all.append((y[0]*y[0] + y[1]*y[1])/2)\n",
    "        \n",
    "        return math.sqrt(np.sum(error_all)/len(error_all))\n",
    "\n",
    "    def predict(self,start_f_in,end_f_in) :\n",
    "       \n",
    "        return [ self.classifier_root_tree.predict( self.X_train[idx,start_f_in:end_f_in+1] ) for idx in range(self.X_train.shape[0]) ]\n",
    "        \n",
    "    def evaluate(self,weight_gammar,start_f_in,end_f_in,show_graph=False) :\n",
    "        \n",
    "        self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "        y_prediction = self.predict(start_f_in,end_f_in)\n",
    "        \n",
    "        \n",
    "        y_binary_prediction = []\n",
    "        for y in y_prediction :\n",
    "            \n",
    "            #[0,1] class 1\n",
    "            if y[0] < y[1] :\n",
    "                y_binary_prediction.append(1)\n",
    "            else :\n",
    "                #[0,1] class 1\n",
    "                y_binary_prediction.append(0)\n",
    "                \n",
    "        #Root mean squared error (RMSE)\n",
    "#         return self.accuracy_cal(self.y_train,y_binary_prediction),self.calculate_RMSE(self.y_train,y_prediction)\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_train,y_binary_prediction).ravel()\n",
    "        acc_train = self.accuracy_cal(self.y_train,y_binary_prediction,show_graph=show_graph)\n",
    "        return  [acc_train,tn, fp, fn, tp],self.calculate_RMSE(self.y_real_train_category,y_prediction)\n",
    "    \n",
    "    def predict_validateTest(self,start_f_in,end_f_in) :\n",
    "        return [ self.classifier_root_tree.predict( self.X_test[idx,start_f_in:end_f_in+1]  ) for idx in range(self.X_test.shape[0]) ]\n",
    "        \n",
    "    \n",
    "    def validate_test(self,weight_gammar,start_f_in,end_f_in,show_graph=False) :\n",
    "        self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "        \n",
    "        y_prediction = self.predict_validateTest(start_f_in,end_f_in)\n",
    "\n",
    "        y_binary_prediction = []\n",
    "        for y in y_prediction :\n",
    "            #[0,1] class 1\n",
    "            if y[0] < y[1] :\n",
    "                y_binary_prediction.append(1)\n",
    "            else :\n",
    "                #[0,1] class 1\n",
    "                y_binary_prediction.append(0)\n",
    "                \n",
    "        return self.accuracy_cal(self.y_test,y_binary_prediction,show_graph=show_graph),self.calculate_RMSE(self.y_real_test_category,y_prediction)\n",
    "    \n",
    "#     def predict_blindtest(self,start_f_in,end_f_in) :\n",
    "#         return [ self.classifier_root_tree.predict( self.X_blindtest[idx,start_f_in:end_f_in+1]  ) for idx in range(self.X_blindtest.shape[0]) ]\n",
    "        \n",
    "    \n",
    "#     def blind_test(self,weight_gammar,start_f_in,end_f_in) :\n",
    "#         self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "        \n",
    "#         y_prediction = self.predict_blindtest(start_f_in,end_f_in)\n",
    "        \n",
    "#         return self.accuracy_cal(self.y_blindtest,y_prediction),self.calculate_RMSE(self.y_blindtest,y_prediction)\n",
    "    \n",
    "    def calculate_evaluate_model(self,result_wanted_y,y_prediction,y_categorical) :\n",
    "        error_all = []\n",
    "        \n",
    "        y_binary_prediction = []\n",
    "        for (y1,y2) in zip(y_prediction,y_categorical) :\n",
    "            print(\"Predict %s , Actual %s \" % (\"[\" + str( y1[0]) +\",\"+ str(y1[1]) + \"]\",y2))\n",
    "            error_all.append(np.abs(np.sum(y1-y2)/2))\n",
    "            #[1,0] class 0\n",
    "            if  y1[0] < y1[1] :\n",
    "                y_binary_prediction.append(1)\n",
    "            else :\n",
    "                #[0,1] class 1\n",
    "                y_binary_prediction.append(0)\n",
    "            \n",
    "        print(\"MAE = \",np.sum(error_all)/len(error_all))\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(result_wanted_y,y_binary_prediction).ravel()\n",
    "        print(\"\\t-----------\\t \\t-----------\\t\")\n",
    "        print(\"|\\t tn - %s \\t|,|\\t fp - %s \\t|\" % (tn, fp))\n",
    "        print(\"|\\t fn - %s \\t|,|\\t tp - %s \\t|\" % (fn, tp))\n",
    "        print(\"\\t-----------\\t \\t-----------\\t\")\n",
    "        acc = accuracy_score( result_wanted_y,y_binary_prediction)\n",
    "        f1 = self.f1_cal(result_wanted_y, y_binary_prediction)\n",
    "\n",
    "        print(\"Acc = %s , f1 = %s\" % (acc,f1))\n",
    "        print(\"RMSE = \",self.calculate_RMSE(y_categorical,y_prediction))\n",
    "        \n",
    "        \n",
    "        plot_confusion_matrix(result_wanted_y, y_binary_prediction,classes = np.asarray(['normal', 'Signed']) )\n",
    "      \n",
    "        \n",
    "    def print_results(self,weight_gammar,start_f_in,end_f_in) :\n",
    "        self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "\n",
    "        #Config\n",
    "        result_wanted_y = self.y_test\n",
    "        result_wanted_X = self.X_test\n",
    "        \n",
    "        y_prediction = [ self.classifier_root_tree.predict( result_wanted_X[idx,start_f_in:end_f_in+1]  ) for idx in range(result_wanted_X.shape[0]) ]\n",
    "        \n",
    "        self.calculate_evaluate_model(result_wanted_y,y_prediction,self.y_real_test_category)\n",
    "    \n",
    "    def print_results_train(self,weight_gammar,start_f_in,end_f_in) :\n",
    "        self.classifier_root_tree.set_all_weights_wGammar(weight_gammar)\n",
    "\n",
    "        #Config\n",
    "        result_wanted_y = self.y_train\n",
    "        result_wanted_X = self.X_train\n",
    "        \n",
    "        y_prediction = [ self.classifier_root_tree.predict( result_wanted_X[idx,start_f_in:end_f_in+1]  ) for idx in range(result_wanted_X.shape[0]) ]\n",
    "        \n",
    "        self.calculate_evaluate_model(result_wanted_y,y_prediction,self.y_real_train_category)\n",
    "\n",
    "    def set_allWeights(self,new_weight) :\n",
    "        self.classifier_root_tree.set_all_weights_wGammar(new_weight)\n",
    "        \n",
    "    def showAllWeights(self) :\n",
    "        self.classifier_root_tree.readable()\n",
    "    \n",
    "    def accuracy_cal(self,y_real,y_predict,show_graph=False) :\n",
    "        \n",
    "        acc = accuracy_score( y_real,y_predict)\n",
    "        if show_graph == True :\n",
    "            plot_confusion_matrix( y_real,y_predict,classes = np.asarray(['normal', 'Signed']) )\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    \n",
    "    def f1_cal(self,y_real,y_predict) :\n",
    "        f_score = f1_score(y_real,y_predict,average='weighted')\n",
    "        \n",
    "        return f_score\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:56.063880Z",
     "start_time": "2020-02-25T07:53:56.029791Z"
    }
   },
   "outputs": [],
   "source": [
    "# logger = logging.getLogger()\n",
    "# fhandler = logging.FileHandler(filename='log_train_opt.log')\n",
    "# formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "# fhandler.setFormatter(formatter)\n",
    "# logger.addHandler(fhandler)\n",
    "# logger.setLevel(logging.INFO)\n",
    "import datetime \n",
    "class logging(object) :\n",
    "    \n",
    "    file_name = \"log_train_Finan_1.log\"\n",
    "    \n",
    "    def initial(file_name):\n",
    "        logging.file_name = file_name\n",
    "    \n",
    "    @staticmethod\n",
    "    def info(msg) :\n",
    "        f = open(logging.file_name, \"a+\")\n",
    "        f.write(\"%s - %s \\n\" % (datetime.datetime.now(),msg) )\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:53:56.562704Z",
     "start_time": "2020-02-25T07:53:56.078632Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import SwarmPackagePy\n",
    "\n",
    "def count_weight_from_tree(shape_OfTree):\n",
    "    count_weight = 0\n",
    "    for each_layer in shape_OfTree :\n",
    "        list_all_weight_gammar = [ w + 1 for w in each_layer ]\n",
    "        count_weight = count_weight + np.sum(list_all_weight_gammar)\n",
    "\n",
    "    return count_weight\n",
    "\n",
    "# tree_shape = [[8,8,8,8,8,8,8,8],[2,3,3],[2,1],[2]]\n",
    "tree_shape = [[8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8],\n",
    "              [3,2,2,4,4,2,1,4,2,1,2,2,5,1,1,2],\n",
    "              [2,5,3,2,2,2],[6,6]]\n",
    "n_weight_gammar = count_weight_from_tree(tree_shape) \n",
    "\n",
    "try :\n",
    "    name_file_train = \"./Final_Data_Train_8_MultiNC.csv\"\n",
    "    all_data_train = pd.read_csv(name_file_train,index_col=\"index_firm\")\n",
    "except :\n",
    "    name_file_train = \"../Final_Data_Train_8_MultiNC.csv\"\n",
    "    all_data_train = pd.read_csv(name_file_train,index_col=\"index_firm\")\n",
    "    \n",
    "for col in all_data_train.columns[:-1]:\n",
    "    all_data_train[col] = [x if x > 0 else 0.00000001 for x in all_data_train[col]]\n",
    "    all_data_train[col] = [x if x < 1 else 0.99999999 for x in all_data_train[col]]\n",
    "\n",
    "\n",
    "\n",
    "## Arrange columns ##\n",
    "list_ratio = [\"CR\",\"QR\",\"CashR\",\"CA2NA\",\"NWC2TA\"]\n",
    "list_ratio = list_ratio + [\"ART\",\"ACP\",\"ITR\",\"ASP\",\"FA2NW\",\"FAT\",\"NWCTR\",\"TAT\",\"ETR\",\"CA2TR\",\"APT\",\"APP\",\"CC\"]\n",
    "list_ratio = list_ratio + [\"ROA\",\"GPM\",\"NPM\",\"ROE\",\"P2NWC\",\"ROS\",\"OE2NS\"]\n",
    "list_ratio = list_ratio + [\"CA2TA\",\"LA2TA\",\"I2CA\",\"CCE2CA\"]\n",
    "list_ratio = list_ratio + [\"D2TAR\",\"D2E\",\"TL2NW\",\"STD2TD\",\"ICR\",\"RE2S\"]\n",
    "list_ratio = list_ratio + [\"AGR\",\"SGR\",\"NPGR\"]\n",
    "new_name_columns = []\n",
    "for idx , val in enumerate(list_ratio) :\n",
    "    for i in range(0,tree_shape[0][0]) :\n",
    "        new_name_columns.append(list_ratio[idx] + \"-Last\" + str(i))\n",
    "new_name_columns = new_name_columns + [\"class\"]\n",
    "\n",
    "all_data_train = all_data_train[new_name_columns]\n",
    "\n",
    "\n",
    "##### Columns drop #######\n",
    "# columns_drop = []\n",
    "# for ratio in [\"TAT\",\"R2NWC\",\"RE2S\",\"S2NW\",\"emscore\",\"CR\",\"QR\"] :\n",
    "#     for idx in range(0,8) :\n",
    "#         columns_drop = columns_drop + [ratio+\"-Last\"+str(idx)]\n",
    "\n",
    "# all_data_train = all_data_train.drop(columns_drop, axis=1)\n",
    "\n",
    "##################### Store Best fold #########################\n",
    "\n",
    "best_sol_cross = []\n",
    "acc_cross = 0\n",
    "error_cross = 0\n",
    "best_fold = 1\n",
    "best_cross_train = 0\n",
    "best_acc_train = 0\n",
    "\n",
    "\n",
    "############## Start & End pos feature #######################\n",
    "pos_start = 0\n",
    "pos_end =len(all_data_train.columns)-2\n",
    "\n",
    "\n",
    "\n",
    "################# In Aggregation algor ##################\n",
    "number_feature = pos_end + 1\n",
    "crossvalidation = 2\n",
    "\n",
    "\n",
    "############### Config Params ###############\n",
    "\n",
    "train_params = {}\n",
    "train_params[\"NP\"] = 4\n",
    "\n",
    "#Gen\n",
    "train_params[\"nIteration\"] = 2\n",
    "\n",
    "train_params[\"lb\"] = 0\n",
    "train_params[\"ub\"] = 1\n",
    "\n",
    "train_params[\"dimension\"] = n_weight_gammar\n",
    "\n",
    "\n",
    "#SSA\n",
    "train_params[\"pf\"] = 0.6\n",
    "\n",
    "\n",
    "# 'D' = D {array} or {int} – Shape of return random numbers \n",
    "name_file_fold1 = \"log_t_f_38s_8Q_SSA_\"+getFileNumber()+\"_fold1.log\"\n",
    "name_file_fold2 = \"log_t_f_38s_8Q_SSA_\"+getFileNumber()+\"_fold2.log\"\n",
    "\n",
    "def trainWithFold(nFold,nameSaveFile) :\n",
    "    global best_sol_cross,acc_cross,error_cross,best_fold,best_cross_train,best_acc_train,name_file_train\n",
    "    \n",
    "    logging.initial(nameSaveFile)\n",
    "    StoreOptimization.static_appendString(\"You're going to train with file \" + name_file_train + \" & Saved = \"+logging.file_name)\n",
    "\n",
    "    n_fold = nFold\n",
    "    \n",
    "    #nGEN , nFES\n",
    "    StoreOptimization.static_clearAll()\n",
    "    StoreOptimization.static_N_Gen(1000)\n",
    "    \n",
    "   #Create evaluate fn\n",
    "    eval = BenchmarkAllTree(tree_shape,pos_start,pos_end,n_fold) \n",
    "\n",
    "    #SSA\n",
    "    StoreOptimization.static_appendString(\"## New training SSA with n = %s , lb = %s , ub = %s, dimension = %s, iteration = %s, pf= %s with fold = %s \" % (train_params[\"NP\"] , train_params[\"lb\"], train_params[\"ub\"], train_params[\"dimension\"], train_params[\"nIteration\"], train_params[\"pf\"],n_fold))\n",
    "    algor = ssa(train_params[\"NP\"] , eval.evaluate, train_params[\"lb\"], train_params[\"ub\"], train_params[\"dimension\"], train_params[\"nIteration\"], pf=train_params[\"pf\"])\n",
    "\n",
    "    \n",
    "    print(\"Optimal Weight and Gammar are : \")  \n",
    "    best_solution = StoreOptimization.store_best_weights\n",
    "    best = StoreOptimization.store_best_error\n",
    "    \n",
    "    print(\":: Finished Fold ::\")\n",
    "    model_check = fuzzy_aggregation(tree_shape=tree_shape,current_fold_train=n_fold,ignore_writeClass=False)\n",
    "    acc_output,error_output = model_check.validate_test(best_solution,pos_start,pos_end,show_graph=False)\n",
    "    acc_from_train,_ = model_check.evaluate(best_solution,pos_start,pos_end,show_graph=True)\n",
    "\n",
    "    print(\"!! Train error = \",best,\" Train Acc = \",acc_from_train[0],\" & Validate Test = \",error_output,\" & Validate accuracy = \",acc_output)\n",
    "    StoreOptimization.static_appendString(\"!! Train error = \"+str(best)+\" Train Acc = \"+str(acc_from_train[0])+\" & Validate Test = \"+str(error_output)+\" & Validate accuracy = \"+str(acc_output))\n",
    "    \n",
    "    tn, fp, fn, tp = acc_from_train[1],acc_from_train[2],acc_from_train[3],acc_from_train[4]\n",
    "    StoreOptimization.static_appendString(\"Trained tn, fp, fn, tp : %s %s %s %s\" % (tn, fp, fn, tp))\n",
    "    \n",
    "    if acc_output > acc_cross :\n",
    "        best_sol_cross = best_solution\n",
    "        acc_cross = acc_output\n",
    "        error_cross = error_output\n",
    "        best_fold = n_fold\n",
    "        best_cross_train_err = best\n",
    "        best_acc_train = acc_from_train[0]\n",
    "    elif acc_output == acc_cross :\n",
    "        if error_output < error_cross :\n",
    "            best_sol_cross = best_solution\n",
    "            acc_cross = acc_output\n",
    "            error_cross = error_output\n",
    "            best_fold = n_fold\n",
    "\n",
    "            best_cross_train_err = best\n",
    "            best_acc_train = acc_from_train[0]\n",
    "            \n",
    "    StoreOptimization.static_appendString(\"** Best Weights now = \" + str(best_sol_cross))\n",
    "    StoreOptimization.static_appendString(\"** Best n_Fold now = \" + str(best_fold))\n",
    "    StoreOptimization.static_appendString(\"** Best Train now = \" + str(best_cross_train_err))\n",
    "    StoreOptimization.static_appendString(\"** Best Accuracy Train now = \" + str(best_acc_train))\n",
    "    StoreOptimization.static_appendString(\"** Best validation error now = \" + str(error_cross))\n",
    "    StoreOptimization.static_appendString(\"** Best Accuracy Validate now = \" + str(acc_cross))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:54:19.918770Z",
     "start_time": "2020-02-25T07:53:56.727307Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're going to train with file ./Final_Data_Train_8_MultiNC.csv & Saved = log_t_f_38s_8Q_SSA_1_fold1.log\n",
      "Y Train count abnormal :  6\n",
      "## parameter weight output class zero [0.2,0.1] \n",
      "## parameter weight output class one [0.1,0.9473684210526316] \n",
      "## New training SSA with n = 4 , lb = 0 , ub = 1, dimension = 432, iteration = 2, pf= 0.6 with fold = 1 \n",
      "fold 1 best at nFES 1 error 0.1912985482754235 (108,0,0.9473684210526315)\n",
      "Optimal Weight and Gammar are : \n",
      ":: Finished Fold ::\n",
      "Y Train count abnormal :  6\n",
      "## parameter weight output class zero [0.2,0.1] \n",
      "## parameter weight output class one [0.1,0.9473684210526316] \n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEYCAYAAADlIcXmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHyRJREFUeJzt3Xu8VXWZx/HPFwhFQTFR1IMNKIiKIgqoZRleIrw7440mFZU0S/OWGWk3Z3K0rNTU0aE0KSdBLcNLhUXhbRTkInhXxAgIVETRxETgmT/WOrY5wdmbfTlr7bO/b17rdfZae+21nuP29Zzfbf1+igjMzKw8HbIOwMysnjmJmplVwEnUzKwCTqJmZhVwEjUzq4CTqJlZBZxEzcwq4CRqZlYBJ1Ezswp0yjqALKlTl1DnblmHYa3Yc5ePZB2CFTFz5oylEbFVNa/ZcbN/iVj1btHz4t3XJkXEiGree0M1dhLt3I2N+h+fdRjWikemXpd1CFZElw9pfrWvGav+zkY7jyx63t9nXduj2vfeUA2dRM0spwRIWUdREidRM8sn1UeXjZOomeWQoEPHrIMoiZOomeWTq/NmZmUSrs6bmZVPLomamVXEJVEzs3K5Y8nMrHweJ2pmVqE6qc7XR5Rm1mCUJNFiW7GrSDdLelXSUwXHPizp95JeTH9ukR6XpB9JmitpjqS9SonUSdTM8qmDim/F3QK0nKBkDDA5IvoBk9N9gEOAful2BnBDSWGWcpKZWZsSScdSsa2IiHgQWNbi8FHAuPT1OODoguM/i8RjQHdJ2xa7h9tEzSyHVGqbaA9J0wv2x0bE2CKf6RkRi9PXS4Ce6esmYEHBeQvTY4tphZOomeVTab3zSyNiSLm3iIiQFOV+HlydN7O8qkLH0nq80lxNT3++mh5fBGxfcF6v9FirnETNLH+kqrSJrsfdwKj09ShgYsHxk9Ne+n2B5QXV/vVydd7M8qkKg+0l3QYMI2k7XQh8C7gCuF3SaGA+0Ly8xW+AQ4G5wArg1FLu4SRqZjlUcsdSqyLiM+t566B1nBvAWRt6DydRM8snP/ZpZlYmzydqZlYJz+JkZlYZl0TNzCrgNlEzszKpOr3zbcFJ1MxySR2cRM3MypJMbO/qvJlZeZRudcBJ1MxySC6JmplVwknUzKwCHdyxZGZWJreJmpmVT24TNTOrjJOomVkF3CZqZlYut4mamVXG1XkzszK5Y8nMrEJOomZm5RKog5OomVnZXBI1M6uAk6iZWZncsWRmVqn6yKFOomaWQ/ITS2ZmFamX6nx9pHr7wI3f+izzJ1/O9Dsu/uDYFpttwr03nM2TE7/JvTecTfduXQDYrOvG3Hn155k6YQwz7ryEk47cN6uwDbh/0u8YOKA/A3buy5XfuyLrcPJPJWw50G6TqKQpkoZkHUe1/fyexzjqrOvXOnbhqZ9iyrTn2f2o/2DKtOe58NThAHz++P15bt4S9jnhCj59+jVcccG/8qFOHbMIu+GtXr2a8845i4n3/JZZc57hjvG38ewzz2QdVq5JKrrlQS6TqCQ3M6zHIzNfYtnyFWsdO3zYQG69ZyoAt94zlSMOGAhAAF033QiATbtsxBvLV7Bq9Zo2jdcSj0+bxo479qXPDjvQuXNnjjthJPfeMzHrsHJLEh06dCi65UHNopDUW9Kzkn4s6WlJ90vqImmQpMckzZF0l6Qt0vOnSLpa0nTgXEm3SLohPXeepGGSbk6veUvBfW6QND29x6W1+n3ybOstu7Fk6VsALFn6Fltv2Q2AG8c/wM59tmHe/Zcx/Y6LufDKO4mILENtWH/96yJ69dr+g/2mpl4sWrQow4jyr1olUUnnp/nhKUm3SdpYUh9JUyXNlTRBUudy46x1Ku8HXB8RA4A3gWOAnwFfjYiBwJPAtwrO7xwRQyLiB+n+FsBHgfOBu4GrgAHA7pIGpedcEhFDgIHAJyUNbC0gSWekSXd6rHq3Or9lzjTnyU99bBfmPL+QHYZfwj4jL+eqMcfRbdONsw3OrFRVaBOV1AScAwyJiN2AjsBI4LvAVRHRF3gDGF1umLVOoi9HxBPp6xnAjkD3iHggPTYO2L/g/AktPn9PJEWnJ4FXIuLJiFgDPA30Ts85XtJMYBZJgt21tYAiYmyaqIeoU5dyf69cefX1t9mmx2YAbNNjM15b9jYAJx25LxP/OBuAeQuW8udFr9O/d8/M4mxk223XxMKFCz7YX7RoIU1NTRlGlH9VbBPtBHRJmwk3ARYDBwJ3pu+PA44uN85aJ9H3Cl6vBroXOf+d9Xx+TYtrrQE6SeoDXAgclJZs7wMarqh13wNPcuIR+wBw4hH7cO+UOQAsWPIGw/buD8DWH+7GTr178vKipZnF2ciGDB3K3Lkv8ueXX2blypXcMWE8hx1+ZNZh5ZdKTqI9mmuW6XZG4WUiYhHwfeAvJMlzOUmB7s2IWJWethAo+y9aW3fgLAfekPSJiHgIOAl4oMhnWrMZSeJdLqkncAgwpeIoc2zc5afwicH96NG9K3N/95/8542/4fs//T23fvc0Rh39Uf6yeBknXnQzAFf8+HeMvfREHr/9YiS45JqJvP5my79T1hY6derEVddcxxGHfZrVq1cz6pTT2HXAgKzDyi0hOpQ2i9PStDlv3ddJ+lyOAvqQNCneAYyoSpCpLHrBRwE3StoEmAecWu6FImK2pFnAc8AC4JHqhJhfo752yzqPH3rmtf90bPFryznii9ev42zLwohDDmXEIYdmHUbdqNIIpoNJmhVfS66pXwH7Ad0ldUpLo72Asnv5apZEI+LPwG4F+98vePufRn1HxLAW+6e0cq1T1vW6teuZWX2p0jjQvwD7poW2d4GDgOnAn4BjgfEkBbuyx5vlY6CVmVkhJSXRYlsxETGVpANpJkkHdQdgLPBV4AJJc4EtgZvKDdWD2s0sdwR07Fid+nxEfIu1h1JC0pS4dzWu7yRqZrmUl8c6i3ESNbP8KbG6ngdOomaWO8IlUTOzCuRnlqZinETNLJdKHGyfOSdRM8sft4mamZXPbaJmZhWqkxzqJGpm+eQ2UTOzcsnVeTOzsiVtollHURonUTPLIY8TNTOrSJ3kUCdRM8shuWPJzKxsHidqZlYhJ1EzswrUSQ51EjWzfHJJ1MysTFLJSyZnzknUzHKpTgqiTqJmlk8d6iSLOomaWS7VSQ5dfxKVtFlrH4yIt6ofjplZkkA7toM20aeBIBn32qx5P4CP1DAuM2twdd87HxHbt2UgZmaF6iSH0qGUkySNlHRx+rqXpMG1DcvMGpkAlfAvD4omUUnXAQcAJ6WHVgA31jIoM7MOKr7lQSm98x+LiL0kzQKIiGWSOtc4LjNrZO1ssP37kjqQdCYhaUtgTU2jMrOGJupnnGgpbaLXA78EtpJ0KfAw8N2aRmVmDU8qvuVB0ZJoRPxM0gzg4PTQcRHxVG3DMrNGV60hTpK6Az8BdiOpUZ8GPA9MAHoDfwaOj4g3yrl+Sb3zQEfgfWDlBnzGzKwszYPti20lugb4XUTsDOwBPAuMASZHRD9gcrpfllJ65y8BbgO2A3oBv5D0tXJvaGZWCpWwFb2GtDmwP3ATQESsjIg3gaOAcelp44Cjy42zlI6lk4E9I2JFGtRlwCzg8nJvamZWTInV+R6Sphfsj42IsQX7fYDXgJ9K2gOYAZwL9IyIxek5S4Ce5cZZShJd3OK8TukxM7OaSHrnSzp1aUQMaeX9TsBewJciYqqka2hRdY+IkBTlxtraBCRXkTTCLgOeljQp3R8OPF7uDc3MilLV1p1fCCyMiKnp/p0kSfQVSdtGxGJJ2wKvlnuD1kqizT3wTwP3FRx/rNybmZmVqhqD7SNiiaQFkvpHxPPAQcAz6TYKuCL9ObHce7Q2AclN5V7UzKwSG1CdL8WXgP9Nn7ScB5xK0ql+u6TRwHzg+HIvXrRNVNKOwGXArsDGzccjYqdyb2pmVky1xolGxBPAutpND6rG9UsZ83kL8FOSPw6HALeTDFI1M6uZagxxagulJNFNImISQES8FBFfJ0mmZmY1UeXB9jVVyhCn99IJSF6SdCawCOhW27DMrNHV/cz2Bc4HNgXOIWkb3Zzk2VMzs5qpkxxa0gQkzeOr3uYfEzObmdWMUN1MhdfaYPu7SOcQXZeI+LeaRGRmlqOp7opprSR6XZtFkZGBO2/P5AevzjoMM1uHjnWSRVsbbD+5LQMxM2sm2lfHkplZm8vJCKainETNLJfaXRKVtFFEvFfLYMzM4B+D7etBKTPb7y3pSeDFdH8PSdfWPDIza2j1slBdKY99/gg4HHgdICJmAwfUMigza2zNSyYX2/KglOp8h4iY36KnbHWN4jEzA+pnRcxSkugCSXsDIakjydx8L9Q2LDNrdDkpaBZVShL9AkmV/iPAK8Af0mNmZjUh5WeWpmJKeXb+VWBkG8RiZvaBOsmhJc1s/2PW8Qx9RJxRk4jMrOE1dyzVg1Kq838oeL0x8K/AgtqEY2aWqJMcWlJ1fq2lQCT9HHi4ZhGZmakdVefXoQ/Qs9qBmJk1E+1gFqdmkt7gH22iHYBlwJhaBmVm1i5KokpG2O9Bsq4SwJqIWO9EzWZm1VIvU+G1+lBAmjB/ExGr080J1MxqLumdL77lQSlPVj0hac+aR2Jm1qw9LJksqVNErAL2BB6X9BLwDskfiYiIvdooRjNrMM0l0XrQWpvoNGAv4Mg2isXM7AN10iTaahIVQES81EaxmJmlRAfqI4u2lkS3knTB+t6MiB/WIB4zs3ShuqyjKE1rSbQj0BXq5M+BmbUfgk510ijaWhJdHBH/0WaRmJmlqlkSTedBng4siojDJfUBxgNbAjOAkyJiZbnXb22IU338GTCzdqmKy4OcCzxbsP9d4KqI6Au8AYyuKM5W3juokgubmVWiGgvVSeoFHAb8JN0XcCBwZ3rKOODoSuJcb3U+IpZVcmEzs3JJJU9A0kPS9IL9sRExtmD/auAioFu6vyXwZjoGHmAh0FRJrOXM4mRmVnMlVtaXRsSQdX5eOhx4NSJmSBpWvcjW5iRqZrlTpZnt9wOOlHQoyYTymwHXAN0LnsjsxT8mWCpLvaxKamYNRiVsrYmIr0VEr4joTbJO3B8j4rPAn4Bj09NGARMridNJ1MxyqRodS+vxVeACSXNJ2khvqiROV+fNLHeEqjqzfURMAaakr+cBe1fr2k6iZpZL9TIps5OomeVSfaRQJ1EzyyO5JGpmVrZ2tdqnmVkW6iOFOomaWU7VSUHUSdTM8kfQLma2NzPLjEuiZmZl26D5QjPlJGpmuePqvJlZJSp7Nr5NOYmaWS45iZqZVUCuzpuZlcdPLJmZVahOcqgnZW4vlr/5JqeeeAL77rUbHx28O49PfTTrkKyF+yf9joED+jNg575c+b0rsg4n91TCvzyoeRKVdImkpyXNkfSEpH0k/UTSrjW+77clXVjLe+TJxRedz4EHD+exmU/xwKMz2Kn/LlmHZAVWr17NeeecxcR7fsusOc9wx/jbePaZZ7IOK7eSNZaKb3lQ0+q8pI8ChwN7RcR7knoAnSPic7W8b6N5a/lyHv2/h7nuf24GoHPnznTu3DnjqKzQ49OmseOOfemzww4AHHfCSO69ZyK77FrTskT9Uv0Mtq91SXRbkiVN3wOIiKUR8VdJUyQNAZA0WtILkqZJ+rGk69Ljt0j6kaT/kzRPUvPCUkj6iqTH09LtpQXHL0mv9TDQv8a/W27Mn/8yW/bowZfOHM0B+w3h3LPO4J133sk6LCvw178uolev7T/Yb2rqxaJFFS0y2e5VulBdW6l1Er0f2D5NbP8t6ZOFb0raDvgGsC/J8qY7t/j8tsDHSUqzV6SfGQ70I1kjZRAwWNL+kgaTrOg3CDgUGLqugCSdIWm6pOmvL11apV8zW6tWrWLOE7M49XOf50+PTGfTTTflRz/8XtZhmZWtecnkYlse1DSJRsTfgMHAGcBrwARJpxScsjfwQEQsi4j3gTtaXOLXEbEmIp4BeqbHhqfbLGAmSeLtB3wCuCsiVkTEW8Dd64lpbEQMiYghW/boUZXfM2vbNfViu6ZeDB66DwBHHHUMs5+YlXFUVmi77ZpYuHDBB/uLFi2kqakpw4jyzyXRVESsjogpEfEt4GzgmA34+HsFr1Xw8/KIGJRufSOioiVP613PntvQ1NSLF194HoAHH/gj/Xd2x1KeDBk6lLlzX+TPL7/MypUruWPCeA47/Misw8q3OsmiNU2ikvpL6ldwaBAwv2D/ceCTkraQ1InSEuwk4DRJXdN7NEnaGngQOFpSF0ndgCOq81vUh8u/fzVnfu5k9t93T56aM5vzLxyTdUhWoFOnTlx1zXUccdinGbT7Lhxz3PHsOmBA1mHlWr1U52s92L4rcK2k7sAqYC5J1f5OgIhYJOm/gGnAMuA5YHlrF4yI+yXtAjyaLmT1N+DEiJgpaQIwG3iVJEE3jN0HDmLyg1OzDsNaMeKQQxlxyKFZh1E38pEii6tpEo2IGcDH1vHWsILXv4iIsWlJ9C7g1+lnT2lxra4Fr68BrlnH/S4DLqs4cDPLXp1k0Tw8sfRtSU8ATwEvkyZRM2tcSZNnfTyxlPmz8xHRME8VmVmJcvREUjGZJ1Ezs3VyEjUzK1d+quvFOImaWS7lZARTUXnoWDIzW0sp4+xLybGStpf0J0nPpLPJnZse/7Ck30t6Mf25RbmxOomaWS5JKrqVYBXw5YjYlWSOjrPSaTjHAJMjoh8wOd0vi5OomeWSVHwrJiIWR8TM9PXbwLNAE3AUMC49bRxwdLlxuk3UzHKpxCbRHpKmF+yPjYix67ye1BvYE5gK9IyIxelbS/jHBEcbzEnUzPKn9AlGlkbEkKKXS+ba+CVwXkS8VdgUEBEhKcqM1EnUzPKneT7RqlxL+hBJAv3fiPhVevgVSdtGxGJJ25LMt1EWt4maWS5VqXdewE3AsxHxw4K37gZGpa9HARPLjdMlUTPLp+oURPcDTgKeTOfoALiYZKWM2yWNJpme8/hyb+Akama5VI0nliLiYdafjg+q+AY4iZpZTtXLE0tOomaWS06iZmZlap5PtB44iZpZ/pT4RFIeOImaWS7VSQ51EjWznKqTLOokamY5lJ8lkYtxEjWz3Cn90fnsOYmaWT7VSRZ1EjWzXPIQJzOzCnjJZDOzcnmcqJlZpeojizqJmlnuCJdEzcwqUic51EnUzPLJg+3NzCpRHznUSdTM8qlOcqiTqJnljzzEycysMqqTLOokama5VB8p1EnUzHKqTgqiTqJmlkfyBCRmZuXyE0tmZhVyEjUzq4Cr82Zm5fI4UTOz8nmNJTOzCnmwvZlZBeokh9Ih6wDMzNZFJWwlXUcaIel5SXMljal2nE6iZpZPVciikjoC1wOHALsCn5G0azXDdBI1s1xSCf9KsDcwNyLmRcRKYDxwVDXjbOg20dmzZi7t0e1D87OOo8p6AEuzDsJa1d6+o3+p9gVnzZwxaZPO6lHCqRtLml6wPzYixhbsNwELCvYXAvtUI8ZmDZ1EI2KrrGOoNknTI2JI1nHY+vk7Ki4iRmQdQ6lcnTez9mwRsH3Bfq/0WNU4iZpZe/Y40E9SH0mdgZHA3dW8QUNX59upscVPsYz5O2ojEbFK0tnAJKAjcHNEPF3Neygiqnk9M7OG4uq8mVkFnETNzCrgJNqOSPL3mWP+ftonf6nthKShwKmSNsk6FvtnkrYCTpe0bdaxWHU5ibYfXYEzgeMldck6GPsnQ9PtWElbZx2MVY+TaDsREX8CvgKMAv7diTRfIuI3wAxgEDBS0oczDsmqxONE65gkRcEYtYiYIimAS9P3fxER72YWoH1A0iHAScBikgHfkjQ+Il7JNjKrlMeJ1qnCBCrpOOAjwAMRMV3SR4HLgXHAhIhYkWGoDU/SFsBtwMURMVPS8cD+wHPAbRHxeqYBWkVcna9TBQn0bOA8YA3wc0lfBKYCY4BzgGMyC9KavUMy++VAgIi4naRE+iXg5PRxRKtTTqJ1TNJewAHAQcDKdPsEcHZEPAacDjyYXYSNSeniQJJ2kNQXCOAWYEdJ+6en/R54HvhtOs+l1SlX5+tIyzbQ9Ng2wB7AVyLi4LQkejHwzYi4OYs4DSQdDVwIzAdeAx4G+pNU4xemP8+OiEmZBWlV4ZJoHSmowh8i6ShJG0fEEuDDwJvpaa8AjwL3ZRRmw0tLn+cBw4E5JJMA3wXcCHyZpHZwkhNo++CSaB1o0Yn0OZK2tLeBmcDNwKskHRfvksydeExEPJdRuA0vXcNnFPACMBo4OSLmShocETOyjc6qzSXRnGuRQLsA25K0e34CeB84MT32GZJ2tyOcQNtWQRto85DBl0mWzDgfOC1NoJ8GbpC0/XouY3XKJdEca5FALwIOBHYCLoqIOyVtCVwCbAJcW+15Eq10kg4HRgDvR8T5ko4BDgOWkHQgfZXke7s3wzCtBlwSzbGCBDqMJIF+lWT5129KOjAdX/hfwDLa18JndUXSIOA7wENAX0lTIuKXwA0kIyb6AudGxL3NpVZrP1wSzaEWJdBhJG2gr0TEF9NjpwFnA1+LiEmSOkTEmswCbmCSdicZjzs3Ir6bHpsIbBYRB6T7HSNidYZhWg25JJozLRLoycBuwDPA1pI+LqlTOnTpJ8A3PGtTdiRtDLwHbAXsLqk/QEQcBayUNCs91SWVdswl0ZxKH938NjAiIkLSZcDmwATg0XTtmM0jYnmWcTaa5j9yacL8Msk8BRuRfFezgHsj4sX0XPfGNwCXRHNGiYEki5ktA5pnY7o03R8N7A3gBNq2ChLooSRtoB8HvgGsBi4jeazzWEk7ATiBNgYn0Rwo7GyIxBzgeyRjPgdL6pw+GngZMBeYl02kjal5Rvo0ge4CXEuSRC8ieQb+ayTjdn9A0vyyKqNQLQOuzueIpM8C/UgGz99KMkTmNJJS6DQ/Y9320nGdh5Istfu+pP1IZmM6LH1/T5IZsxYAXwfe8PfUWFwSzQlJZ5H0wr9B8oz1pHQbB3wfGJxddI0pnYG+CXgc6C5pc+AJYGNJpwNExCxgOsncvCcAa7yWUmPxl52RgqdcmqvyuwPnRMQ1EXEucA/wvYi4Ffg5sCibSBuTpJ1JnnHfhmQSkXEkNYIuJGN195L0g3QI2hHA0yTPyK/2cLPG4iSagRazMfWT9CGgFzCs4LR7Sb+fiLg+Iv7StlE2Lkm9gTuBKyPi1+lDDScDvUmeiX+apONvW5LmllNJJn3ZEujW9hFblrw8SBtrMQ60eULlu4DZwDmSlqbjQHcHekvqDixvOQWe1dQBwOSIuCmtmu9J8iz8Q8CxJGNDfxYR/y6pI8l8rleSTDTyVlZBWzacRNtYQQI9kmRIzKdJpkzbDPgD8J20s+IA4ISIeHN917KamQd8Lp005ASSKvweJE0sK0gmfekl6evpeN2tgWObx4daY3HvfAYkNZFU//4QEadJ2ohkGY/tgS1IqorLvfZONtKnwM4ATiEZUnYN8BRJdX4kydNi3SJiZkYhWo64TTQDEbGIpBo/QtLIiHgPGE8yA/oaYJkTaHYiYkVEXA0cGBHHRsRDEfEGyRNjnyT5fpxADXB1PjMR8StJ7wGXSyIixku6Bdg0It7OODwDImIZQNrx9ymS8aAX+w+cFXISzVBE3CdpDTBW0qqIuJPkyRfLiTSB7g1cAHw9Irzsiq3FbaI5IOlTwEsR4cc5cyhNpFtGxJJ1LRZojc1J1MysAu5YMjOrgJOomVkFnETNzCrgJGpmVgEnUTOzCjiJ2lokrZb0hKSnJN1RyUJ4koZJujd9faSkMa2c213SF8u4x7clXVjq8Rbn3CLp2A24V29JT21ojNa+OYlaS+9GxKCI2I1kzfQzC99M14Da4P9vIuLuiLiilVO6AxucRM2y5iRqrXkI6JuWwJ6X9DOSiTi2lzRc0qOSZqYl1q4AkkZIek7STODfmi8k6RRJ16Wve0q6S9LsdPsYcAWwY1oKvjI97yuSHpc0R9KlBde6RNILkh4mWQWgVZJOT68zW9IvW5SuD5Y0Pb3e4en5HSVdWXDvz1f6H9LaLydRWydJnYBDgCfTQ/2A/46IAcA7JOsJHRwRe5Esj3GBknXYf0wy0/tgklnh1+VHwAMRsQewF8kkx2NIntoaFBFfkTQ8vefewCCSBfv2lzSYZCalQSRrHw0t4df5VUQMTe/3LMmKqc16p/c4DLgx/R1Gk8yiNTS9/umS+pRwH2tAfnbeWuoi6Yn09UPATcB2wPyIeCw9vi+wK/BIurpJZ5Kp/XYGXi5Yd/1WkinlWjqQZKZ4ImI1sFzSFi3OGZ5us9L9riRJtRtwV0SsSO9xdwm/026SvkPSZNCVZO2qZreny3m8KGle+jsMBwYWtJdunt77hRLuZQ3GSdRaejciBhUeSBPlO4WHgN9HxGdanLfW5yok4PKI+J8W9zivjGvdAhwdEbMlncLay7C0fO450nt/KSIKk23zsiFma3F13srxGLCfpL4AkjaVtBPwHMmSJjum531mPZ+fDHwh/WxHJatovs3a6xNNAk4raGttSmeQfxA4WlIXSd1Img6K6QYsTicS+WyL946T1CGNeQfg+fTeX0jPR9JOkjYt4T7WgFwStQ0WEa+lJbrb0ln5IZkm7gVJZwD3SVpB0hywroXbziWZ/m80sBr4QkQ8KumRdAjRb9N20V2AR9OS8N+AEyNipqQJJGtSvUqynHEx3wCmkkx6PbVFTH8BppEsz3JmRPxd0k9I2kpnKrn5a8DRpf3XsUbjWZzMzCrg6ryZWQWcRM3MKuAkamZWASdRM7MKOImamVXASdTMrAJOomZmFfh/ojmhyA56IdUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! Train error =  0.1912985482754235  Train Acc =  0.9473684210526315  & Validate Test =  0.20165176842239665  & Validate accuracy =  0.9380530973451328\n",
      "!! Train error = 0.1912985482754235 Train Acc = 0.9473684210526315 & Validate Test = 0.20165176842239665 & Validate accuracy = 0.9380530973451328\n",
      "Trained tn, fp, fn, tp : 108 0 6 0\n",
      "** Best Weights now = [0.45145348411086694, 1.3081125738696628, 0.9393013286455657, 0.35744242435101065, 1.4818690898311375, 1.3074057972423976, 0.6388733677355226, 1.5155419342138352, 0.5879913386576763, 0.7206893923611856, 0.3144105922955025, 0.9060019505239215, 2.47152658380519, 0.9950560638480235, 1.5883879495056883, 0.4667665806367334, 0.5371608870237555, 0.8060734085314054, 3.4206585625182035, 0.5785165942713149, 0.4682087428841817, 0.7291792449840953, 0.026979433947031624, 0.6017721497606748, 2.0455770339674975, 0.1291082376670005, 0.7124063193262434, 0.12360422270202744, 0.05765772755061898, 0.4115700876681153, 2.4166856695679404, 1.9335803517209256, 1.985285089466275, 0.22339461796069104, 0.8482222333634067, 0.36444136021327767, 1.6856589448591432, 1.171290096798481, 0.6763421355116709, 0.7062966335870219, 0.44976689115624985, 1.6667069615908134, 1.0013413041654786, 0.6425970323311407, 0.7361651011818495, 1.0411436903623295, 0.665797004667685, 1.3495114649131639, 1.4164856539383353, 1.239293428992056, 0.22699762390983308, 1.358256409835074, 0.702514723381524, 0.5062101084648113, 1.6521705368359365, 1.3670050309266044, 0.7406238291144435, 1.723842693089207, 0.2344269276822522, 1.8529847874933945, 0.27948920812143463, 0.1494569867367276, 0.12702236664751265, 1.3070766483678358, 0.9228264509260685, 1.0360307010128784, 1.6168109129822872, 0.7127042748375043, 1.5558367970702147, 0.8005118739447983, 0.04820234085841411, 0.12307431688386516, 0.520908295024029, 1.1805954934127652, 1.264970729486218, 0.18858137098005381, 0.8198893704682829, 1.2674959471408476, 1.5599660347030562, 1.1975927587847472, 0.0951429009331557, 0.7115384066883578, 0.019649979501959604, 2.083093502288081, 0.4153544362549281, 1.9654719027404277, 1.1876290040428703, 0.2847562562992578, 1.3325065121841166, 0.6563501198649401, 1.1691872352079666, 0.6336265152295927, 1.0749070165285868, 0.7305281790783718, 1.1778479859133062, 1.185327985098896, 0.8753713931458158, 1.1532036897974647, 0.6316048131491494, 2.365277281106008, 2.5202561845505054, 0.5794369973697292, 0.5223558053693033, 0.8764147300374572, 0.5774855242133156, 0.08027971432115999, 0.4784937630325208, 0.7252264522217478, 0.6151940224237413, 0.9377342921239856, 1.3053793809129683, 0.31441963792863703, 1.7098612498376877, 1.7657888697006465, 1.1319807297568116, 0.2196418173155228, 0.39266576375341655, 0.13112477495241287, 0.4714735028247766, 0.9866726181864335, 0.15553887853782553, 2.034971870887293, 1.9485694725393183, 1.006850245739185, 1.2647986363327561, 0.6582204377938613, 1.0468731300567107, 1.0867694209925318, 0.46957149248575714, 1.4524279305759, 1.1319337088647905, 1.3878259352977442, 1.231470955203567, 0.1931274265229981, 0.441902483403388, 1.5797509223814579, 1.9810347916562046, 1.0150417415373427, 0.5327315495217065, 0.17896771609140835, 0.6052767985544958, 0.5454802721652257, 1.561716208092159, 0.01986882271020418, 1.0915579509998499, 0.2529724732659391, 1.2316206556371974, 0.40694809216350414, 0.886501171586619, 1.718328761701493, 1.8052601736851719, 0.606810720960225, 0.6523767190151795, 0.12661891437619946, 1.252398264318565, 0.19897056565056634, 1.178877613120095, 0.006933034661801808, 1.43107313837288, 1.8132321531583244, 1.9918963163415684, 0.40737619784796086, 1.3357671809528693, 1.024722495008181, 1.399854343738522, 1.5989205571894771, 0.6117953734752567, 0.6076917171364324, 1.2924561843812223, 0.12879214811803993, 0.7526784000011051, 1.5982131898211522, 1.5693553788582055, 1.081497834093792, 0.8886466368932499, 0.9316102003349949, 0.6603202516086479, 0.2652494190099195, 1.0051070893800373, 0.8911894589373208, 1.4522674161538096, 1.3189618316195784, 1.5321866001448625, 0.6032860506695349, 1.5386139178032363, 0.6379717363543377, 0.17064433210289479, 0.7460681151517455, 0.06503407455041565, 1.1370419897404442, 1.1331726914932916, 1.2742343233569602, 0.2370683708548298, 1.5725972255983975, 0.643585936668841, 0.08029283877885268, 1.9220066235083826, 0.023575813907056542, 1.641356486097566, 0.29858834784237703, 0.4238928725325483, 1.7766234242653958, 0.7439853789648209, 1.134905509731371, 1.7313303210028959, 0.2493176595630243, 0.5107318869298657, 1.1605405918106548, 0.316765376414944, 0.958216691216133, 0.9397333158639467, 2.3009105817301587, 0.537413281659948, 1.701858291361298, 0.08456186994291673, 0.2753707476454661, 0.19934415867082195, 1.7811391827448375, 1.7039100283657531, 0.9144426954506721, 0.6803313890880882, 2.0684879450054234, 0.5317273424361635, 0.12061725823823986, 0.552671009063724, 0.03569394812359555, 2.672413155158136, 0.961193494896709, 0.7300352211678537, 0.6635157640951888, 1.5030914845088166, 1.1266222174747011, 0.3074347145749995, 0.8717752585891568, 1.648544507324075, 1.3307053841038043, 0.2718614998036387, 1.4463107243265774, 0.1689382920155668, 0.9707623760258306, 1.0393607986272648, 1.1235164177732433, 0.8519503527718605, 0.5439829956793648, 1.3433060005522828, 1.984764347009503, 0.6022657452952281, 0.8153825791735889, 0.26031877762086614, 0.9980799035442868, 1.45189965112488, 0.9698194502173976, 1.6215599036964214, 1.0209320376482276, 0.6704229565324151, 0.216162216356748, 0.5415443475617474, 1.1967144404907082, 1.5934403903630965, 1.1392237073506364, 0.18291118806893014, 1.629297616791942, 0.23844930594416522, 1.1398683725822882, 1.2139888824494596, 1.270616883014035, 0.9446227345156765, 1.5176010460208245, 0.045555158681610065, 0.2556939934672021, 1.6039879067523504, 1.9028061453909755, 0.2535300940608677, 0.1177084362674716, 0.6738931694676734, 1.1353421205944274, 0.5369156716456494, 1.7758164558205842, 0.8744285753702132, 1.1295934102351248, 2.0195408255619527, 1.0305439496366284, 0.332866577304069, 1.8182354173015973, 0.001532019745477519, 0.43443020324199666, 1.233257596973154, 0.15258565245760447, 0.4378438131632323, 1.5628779867304394, 1.2019774217377601, 1.1545731170424356, 1.5485175802485915, 0.3264683949262478, 0.38082861852717087, 1.3869130676241221, 0.18834999037750755, 0.9727114622599149, 1.358886122692371, 1.6324423417794545, 0.2502293868400196, 0.3144409964625479, 0.9777933675571086, 1.643019731570571, 0.8504765908380132, 0.9571722232707249, 0.2391860049034037, 1.1622720434532703, 0.9075615116949248, 1.3361404341070307, 1.3115379243224947, 0.5525241642816675, 1.0974134675694782, 1.3933644496677289, 0.09290250217514917, 1.204781118632423, 1.1479852469055511, 1.657291920547964, 0.23000295329685586, 1.1715372274450517, 0.733891790071726, 0.2213857736939112, 1.6331239694065172, 0.8030421614006206, 1.0773898205238936, 0.5733564684170653, 0.9814907012667465, 1.4366789490284502, 0.7429496279756614, 2.1292053760935583, 0.8483979641389539, 0.21053109255567046, 0.9226025364043371, 0.9346538490186993, 0.22322860561440094, 0.16309433689459457, 0.8414468267703245, 2.0770323114256546, 1.246969161943679, 1.0471498318518593, 1.4664250764807873, 0.44074270540904714, 1.4078255531496164, 0.2597595769653252, 1.3324148698850586, 0.7439970894296949, 0.6172004982833071, 1.3827995017166927, 0.6096829384693894, 1.119899036554027, 0.880100963445973, 0.30114598071790877, 0.07175738299337267, 1.2033734796720719, 1.7515428349027606, 0.9733263024317945, 0.6639210704501747, 1.0765394870104121, 0.13027493653784028, 1.197624454295256, 1.595561122156492, 0.567729153261448, 1.030463178890985, 0.9695368211090151, 0.7218214380312066, 1.0, 0.6314101814529596, 0.3161369449882445, 1.6342103513009782, 0.6473883786475222, 1.402264325063255, 0.48522817755329917, 0.18768701152603354, 1.8123129884739664, 0.8346390057484243, 1.0, 0.48122030391005344, 1.0867347494424047, 0.9132652505575953, 0.06845487444934706, 1.6816675984174598, 0.31833240158254, 0.4326082120965504, 1.731631343653949, 0.6528745460185792, 0.15142784252019123, 1.1331528251718297, 1.3309134426354512, 0.5880485846925596, 0.9999999999999999, 0.5144386504862654, 1.0, 0.7839025006455552, 1.2923600623815752, 0.7076399376184248, 0.9978513644651195, 0.19456839317464783, 1.8054316068253522, 0.7111175365436769, 0.05829370589635923, 0.38754840616563363, 1.79330807857473, 1.0242150973680613, 1.7366347119952157, 0.9761408608220863, 0.9506949419555868, 0.9493210636032795, 1.0999839944411336, 0.5376699432474354, 0.3019033486296879, 1.6980966513703122, 0.2509275691794921, 1.0833742423037913, 0.9166257576962087, 0.12890490620201367, 0.7690828404443103, 1.2309171595556898, 0.8489144807877349, 1.3400671236580228, 0.39282358943077317, 1.1210904264947232, 0.9546399731390632, 0.7459088680119285, 1.4454700192654895, 0.9364918950045905, 0.35147462656795847, 0.9268809770232135, 1.7207012736184235, 0.034492560407766895, 1.553273451803531, 1.4131771105791067, 0.3867929728937455]\n",
      "** Best n_Fold now = 1\n",
      "** Best Train now = 0.1912985482754235\n",
      "** Best Accuracy Train now = 0.9473684210526315\n",
      "** Best validation error now = 0.20165176842239665\n",
      "** Best Accuracy Validate now = 0.9380530973451328\n"
     ]
    }
   ],
   "source": [
    "trainWithFold(1,name_file_fold1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:54:32.624388Z",
     "start_time": "2020-02-25T07:54:19.928200Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're going to train with file ./Final_Data_Train_8_MultiNC.csv & Saved = log_t_f_38s_8Q_SSA_1_fold2.log\n",
      "Y Train count abnormal :  7\n",
      "## parameter weight output class zero [0.2,0.1] \n",
      "## parameter weight output class one [0.1,0.9380530973451328] \n",
      "## New training SSA with n = 4 , lb = 0 , ub = 1, dimension = 432, iteration = 2, pf= 0.6 with fold = 2 \n",
      "fold 2 best at nFES 1 error 0.22586711276462493 (106,0,0.9380530973451328)\n",
      "Optimal Weight and Gammar are : \n",
      ":: Finished Fold ::\n",
      "Y Train count abnormal :  7\n",
      "## parameter weight output class zero [0.2,0.1] \n",
      "## parameter weight output class one [0.1,0.9380530973451328] \n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! Train error =  0.22586711276462493  Train Acc =  0.9380530973451328  & Validate Test =  0.21703405101734616  & Validate accuracy =  0.9473684210526315\n",
      "!! Train error = 0.22586711276462493 Train Acc = 0.9380530973451328 & Validate Test = 0.21703405101734616 & Validate accuracy = 0.9473684210526315\n",
      "Trained tn, fp, fn, tp : 106 0 7 0\n",
      "** Best Weights now = [0.3843574887055416, 1.8055313639277595, 0.39680563558857945, 1.2437511353360553, 1.0203409611322658, 1.908786107472466, 0.7114571673565182, 0.5289701404808145, 0.7847851253900539, 0.17290113777668364, 0.535225896295119, 1.1747788947930122, 0.5165634796019308, 1.5248797150908844, 1.3409590402389313, 1.6244674998768571, 1.1102243363265822, 0.023540287211004873, 1.697020699565047, 0.11085411009854876, 1.1922841421740622, 1.0379989386737578, 0.34342847281244526, 0.993184885033625, 1.0419543811050502, 1.5832743705374623, 0.21320277797556775, 0.7896168051603042, 0.5804059362529173, 0.45839546244870266, 1.2898719145199908, 1.6406050719228908, 1.4080734965323605, 1.2423111090831085, 0.590720204079725, 0.610527741913788, 0.3778048724767741, 0.8914208998971485, 1.5405545784427919, 0.4275641842720829, 1.3126327049431954, 0.4288690070672632, 1.7173650776431861, 1.3037886752575583, 0.4770458238419104, 0.4059691926196021, 0.49636567156032635, 2.033335055261691, 2.0506296865174742, 0.27238514895554394, 1.1100988433177053, 0.6022932939218173, 1.0289231078458407, 0.41931386803553417, 1.0979491140762114, 1.0057955606045559, 1.2360831909838035, 1.3041693659909326, 0.5447996152531053, 0.6188725467118275, 1.2923145844002852, 0.9000160219792791, 0.843013845912762, 0.21823835988649626, 1.482672530877826, 0.981919325997609, 1.1332034172543057, 0.07222387983149715, 1.1537473264284646, 1.6044736475697976, 1.3535215121540045, 0.4950543111746317, 0.7239529175191738, 1.4892954057062016, 1.5047018560121244, 0.7553698596981292, 0.9836561431896905, 1.3861414173951592, 0.3276874377143955, 0.829194962765126, 0.4796901442071262, 1.0094868425589358, 0.8087770207278352, 0.8066652243657371, 0.016211696265979507, 1.68077390642495, 1.8975443365529192, 1.450936725150902, 0.3296042479527412, 0.5923123615581208, 2.304544421093392, 0.8904550457594106, 0.9952758617803635, 0.9158971889389117, 0.7995044821687053, 0.1760672899892107, 0.14547783476213638, 1.7727778755078691, 0.602678766795828, 1.0834950494115403, 0.06405608506822565, 0.37468788183985585, 1.7415322772773782, 1.1429277069222012, 0.5200352195953241, 2.576994733388405, 0.49627104649707005, 0.17737636612597718, 1.0511791383131637, 1.3916638528024072, 1.1830695231975277, 0.3467370835267917, 1.3684705187215056, 1.0644045369871757, 0.6014217345214112, 0.993053611930017, 0.8499477636228445, 0.8771471446464758, 0.48945719916139624, 0.9830181884441058, 2.0814482068304048, 1.406144596835045, 0.8789156440520275, 1.147149904907914, 0.1367191151226302, 0.635483343153523, 1.4370306166613505, 1.2376879460518984, 0.08605632610697782, 0.7073140547063415, 0.09399334293763857, 2.96262894715448, 0.2522665882807025, 1.2230221781006114, 0.04970334744925542, 0.1400927822244879, 0.1508449176646008, 0.924276406854484, 0.48563649829151123, 1.4006562859445417, 1.9762333194953103, 1.627538982949764, 1.2947208065753002, 0.677471340808157, 0.5323443761589174, 1.2192339054433312, 0.3818148771247661, 1.7499213803722664, 0.2727932134070568, 1.8283052437445797, 0.37996852436469447, 1.6356184793843875, 0.5711712395864326, 0.4621947798398999, 0.016540257220890423, 1.0076686351442088, 1.4702247018197947, 1.0363888093010465, 1.7648052851920752, 1.970022390017087, 0.2721551414649961, 0.7569527747733142, 1.9156344779765198, 0.8658917884570592, 0.18938238992864512, 0.2784439660938204, 1.5845378900160665, 1.134626277083148, 0.23948084993118338, 1.7920023605135582, 0.9842622156493199, 0.38131965817516866, 0.7308842298755877, 1.9078211382882864, 0.9676938460830588, 0.47898525227868727, 0.6423269239522963, 1.637245673690455, 1.2537232776564597, 0.009456422522785557, 1.2589232876654024, 1.7459832160592708, 1.3080168145661433, 0.8714502217303576, 0.22557109133150674, 0.40455395483251916, 0.5101504565565681, 1.6753509572582321, 0.327557075106366, 1.617836752982117, 1.592852162510699, 0.4577253372233647, 1.3837992580879466, 1.313068362969823, 0.7911636029750206, 0.44690198114511515, 0.3966525421059141, 0.3701916361037798, 0.19517888368510075, 1.950652419673239, 0.7203334618806487, 0.6681499729260965, 0.4989331309163518, 1.3982459015367008, 1.0163524559715469, 1.5521537734103157, 0.06221409076877871, 1.2508727247440685, 0.5364102221860888, 1.4539877434003117, 1.6671640886339725, 1.0162978045056468, 1.4461853198250247, 0.6100372293693753, 0.019044867335511716, 0.3173934861989022, 0.3258693037883629, 1.0817745531225045, 1.6870026553763169, 2.1214704750815367, 1.043261237281436, 1.175951964556124, 0.4428173658067705, 0.12185244498694969, 0.028962882520855437, 1.8916415026772504, 1.4144021198181038, 0.165146329137244, 0.5084128858940655, 0.8049642942774649, 1.2094140869782009, 1.9011978370027238, 0.10482094421494563, 0.6850050984367855, 2.068320577272301, 1.6558327591188575, 1.9469696919193975, 0.14334150668570836, 0.1961394400933084, 0.7120060030360712, 0.9828233711937011, 0.2945666506806543, 0.4295680840867987, 0.9670624720109092, 0.8563735507508098, 1.6599309706957146, 0.5301890216724561, 1.2433103979176674, 1.079820808108157, 1.3456022899164843, 0.3177104889278012, 0.1674175576184831, 1.8013075556568454, 1.3346187601136186, 0.745674056354652, 0.11248288484726807, 1.703457531102687, 0.09689412572844708, 1.9405073363292746, 0.26505774986720765, 0.26166286758219104, 0.8401626540463388, 2.0344645715536456, 0.16065787913582788, 0.04800160913056394, 1.7924820274300524, 1.998299882963433, 0.18371957043574302, 0.9422118053043946, 0.39364702617491565, 0.41072745651884024, 0.10567957483531508, 0.4312255603648636, 2.0952672903128207, 1.2736041032673955, 0.6990953494455697, 1.1480187205530765, 1.8363819447021188, 0.8119381136433398, 0.6497858172981128, 0.6614019649287041, 0.16954585192773453, 1.0937891388016596, 0.8614328271955979, 2.1729282773063425, 0.08311607465256227, 2.308000047889288, 0.5107160300764699, 0.317537025841525, 0.4384587180144262, 1.1476776037621308, 1.804362284639209, 0.18806046326876577, 0.3310347846939674, 1.8328822179014912, 1.9399869018784845, 0.255145050422417, 0.9748410188464848, 1.533290204419896, 1.7411939695494563, 0.9643395445836158, 0.9209367945386853, 0.5954563370343686, 0.17401923908892913, 1.0959228919385637, 0.06813909787412264, 0.5455723209873026, 1.9660814588413549, 0.46894571794531104, 0.25340726537955666, 1.7227193207174893, 1.2605684141903528, 1.4029371189725854, 0.37976838296604776, 0.5374711580131643, 1.2957043527784855, 2.1270195549349347, 0.8704714276709212, 1.459917176746926, 0.1713727117275889, 0.5792247652922012, 0.030332762205390527, 1.465957248643551, 0.22972598336906047, 0.49622847117586705, 1.7793424909278122, 1.2788606038377437, 0.543113702872222, 0.7071637981421194, 0.9908674533452083, 0.6040628858267879, 1.60036059387224, 0.13307879359754815, 0.6176602984373994, 0.4045458800392088, 0.9890415136094884, 1.2422340485912788, 1.190053668909053, 0.44139347709888194, 1.2665536212671136, 1.8485174920475766, 0.8641590398293584, 0.6704349926058549, 1.2969836306772775, 1.032581376716868, 0.3727908229016247, 1.0751484183040532, 0.9248515816959467, 0.8329734870264872, 0.01431702544032188, 1.985682974559678, 0.8970408703642374, 1.3019349259846644, 0.6292349765748748, 1.5023301183330282, 0.5664999791074329, 0.07795876123538525, 1.0370725122201476, 0.5883916712082033, 2.1821426402539874, 0.19239317631766134, 0.6402410724624193, 1.1625748834760914, 0.8374251165239086, 0.2753482948131011, 1.0, 0.08382163613661175, 0.12630767516645972, 0.970056155417102, 1.1365139426550974, 1.7671222267613411, 0.7124299033469106, 0.6667845160262944, 1.3332154839737056, 0.8361022104302129, 1.0, 0.7761380606528172, 0.7860321028853461, 1.213967897114654, 0.2867931001024516, 0.8290360205283709, 1.1709639794716291, 0.46689436264908746, 1.892362640049149, 1.4523212719278675, 0.03176617632838913, 1.4731201190036733, 0.15042979269092174, 0.8391454949127978, 1.0, 0.4520412118204429, 1.0, 0.10109352176931863, 1.0378598273828443, 0.9621401726171557, 0.08534581671734476, 1.3046890796810584, 0.6953109203189417, 0.2698707494907605, 1.0058606918977837, 0.7260049377952105, 0.08794676956598303, 1.7973370988695676, 1.3828505018714552, 0.7220493568172229, 0.6286078973978454, 0.5467644703602134, 1.8246276322419412, 0.33819036023801974, 0.38037526588012077, 1.6196247341198793, 0.13774754555474822, 0.6505047284779905, 1.3494952715220097, 0.19492702074004697, 0.826001626459548, 1.173998373540452, 0.7356765883919767, 0.5672016091676708, 1.0027052754634274, 1.3517643019310595, 1.435869842827481, 1.0864406499122898, 0.5560183206980706, 0.6349509445380217, 0.012801740582022755, 2.155997361223908, 1.1942108665754863, 0.05360923913257631, 0.8160502390721639, 1.7673305534138437, 0.10329575581177863]\n",
      "** Best n_Fold now = 2\n",
      "** Best Train now = 0.22586711276462493\n",
      "** Best Accuracy Train now = 0.9380530973451328\n",
      "** Best validation error now = 0.21703405101734616\n",
      "** Best Accuracy Validate now = 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "trainWithFold(2,name_file_fold2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:54:32.645957Z",
     "start_time": "2020-02-25T07:54:32.632763Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Joint two files and delete split files\n",
    "'''\n",
    "\n",
    "filenames = [name_file_fold1, name_file_fold2]\n",
    "with open(\"log_t_f_38s_8Q_SSA_\"+getFileNumber()+\".log\", 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "        \n",
    "os.remove(name_file_fold1)\n",
    "os.remove(name_file_fold2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Fold Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:54:34.029977Z",
     "start_time": "2020-02-25T07:54:32.654760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Train count abnormal :  7\n",
      "Predict [1.8192897376855335e-18,1.5822462395679882e-38] , Actual [0.2 0.1] \n",
      "Predict [1.2607413696992962e-19,1.9067788381824265e-34] , Actual [0.2 0.1] \n",
      "Predict [2.997835338698193e-16,1.9050682413697208e-38] , Actual [0.2 0.1] \n",
      "Predict [5.626973485041723e-42,1.3607713784012863e-76] , Actual [0.2 0.1] \n",
      "Predict [6.408151704376368e-16,3.300433740765099e-31] , Actual [0.2 0.1] \n",
      "Predict [8.058084335247742e-27,1.082637376395216e-36] , Actual [0.2 0.1] \n",
      "Predict [1.6752280387907685e-14,5.552364482173009e-36] , Actual [0.2 0.1] \n",
      "Predict [8.018272996945163e-16,5.809506827693297e-32] , Actual [0.2 0.1] \n",
      "Predict [2.111118543218705e-21,4.2701142686525315e-58] , Actual [0.2 0.1] \n",
      "Predict [4.831430138494534e-24,1.5644214041318357e-45] , Actual [0.2 0.1] \n",
      "Predict [4.141444688713512e-19,4.295669844319974e-31] , Actual [0.2 0.1] \n",
      "Predict [2.8311266893134774e-26,9.146670888209207e-51] , Actual [0.2 0.1] \n",
      "Predict [2.930739706302673e-17,4.4114299480736543e-35] , Actual [0.2 0.1] \n",
      "Predict [4.32254156563143e-19,1.7747170557097516e-37] , Actual [0.2 0.1] \n",
      "Predict [5.293939454268526e-16,7.882307645275417e-30] , Actual [0.2 0.1] \n",
      "Predict [4.0323651575448816e-18,8.627869923013966e-38] , Actual [0.2 0.1] \n",
      "Predict [1.4050507607381126e-20,1.129619567387562e-40] , Actual [0.2 0.1] \n",
      "Predict [1.0936621722394925e-16,2.875550001779555e-31] , Actual [0.2 0.1] \n",
      "Predict [5.794638113438769e-16,2.7143162322690147e-30] , Actual [0.2 0.1] \n",
      "Predict [3.28909857338611e-19,2.3603015678117716e-38] , Actual [0.2 0.1] \n",
      "Predict [1.4930944281589499e-21,1.718099048799271e-50] , Actual [0.2 0.1] \n",
      "Predict [2.7117843110248533e-30,1.8802259269970903e-42] , Actual [0.2 0.1] \n",
      "Predict [1.829245879703466e-17,9.195318904935546e-35] , Actual [0.2 0.1] \n",
      "Predict [4.542187289527698e-20,2.1587897781676972e-42] , Actual [0.2 0.1] \n",
      "Predict [7.591730773302874e-17,4.4472583785555084e-33] , Actual [0.2 0.1] \n",
      "Predict [3.5270587223733376e-20,9.921666949343097e-41] , Actual [0.2 0.1] \n",
      "Predict [5.941206947807031e-31,8.999394583374125e-33] , Actual [0.1       0.9380531] \n",
      "Predict [4.9838626593305025e-19,5.142498303234182e-43] , Actual [0.2 0.1] \n",
      "Predict [9.486251862637299e-23,4.050364830101784e-46] , Actual [0.2 0.1] \n",
      "Predict [1.352604454634418e-30,5.603225852578614e-53] , Actual [0.2 0.1] \n",
      "Predict [1.3345732951648709e-15,6.499672521678464e-33] , Actual [0.2 0.1] \n",
      "Predict [1.0952055959246883e-29,1.7072347800378604e-63] , Actual [0.2 0.1] \n",
      "Predict [2.6822712421061666e-15,1.937352001850603e-29] , Actual [0.2 0.1] \n",
      "Predict [1.9107371747233083e-15,2.8100025346065047e-28] , Actual [0.2 0.1] \n",
      "Predict [3.1281354893572436e-17,3.616087772502351e-34] , Actual [0.2 0.1] \n",
      "Predict [2.392503363450576e-16,1.5575589087024082e-46] , Actual [0.1       0.9380531] \n",
      "Predict [2.047663355486332e-43,4.259579521124239e-84] , Actual [0.1       0.9380531] \n",
      "Predict [6.348703377073971e-17,2.115857130387816e-45] , Actual [0.2 0.1] \n",
      "Predict [6.328983457634661e-22,9.673937838727912e-37] , Actual [0.2 0.1] \n",
      "Predict [2.554659662460357e-19,8.767009741347669e-40] , Actual [0.2 0.1] \n",
      "Predict [3.5097027515236074e-19,2.338866311738033e-36] , Actual [0.2 0.1] \n",
      "Predict [1.8173937419972352e-21,6.904224455354137e-31] , Actual [0.2 0.1] \n",
      "Predict [2.0012399729111258e-18,3.180988460008069e-37] , Actual [0.2 0.1] \n",
      "Predict [1.202918969638972e-16,5.612362822979145e-27] , Actual [0.2 0.1] \n",
      "Predict [5.5094855046746825e-17,3.4803793529771634e-32] , Actual [0.2 0.1] \n",
      "Predict [1.8035722351840213e-21,1.1461408182539403e-32] , Actual [0.2 0.1] \n",
      "Predict [2.548243506908942e-19,1.6598160687770016e-38] , Actual [0.2 0.1] \n",
      "Predict [3.3133660826722493e-22,2.0050031311450664e-36] , Actual [0.2 0.1] \n",
      "Predict [3.337898825294114e-21,3.0745595614736578e-37] , Actual [0.2 0.1] \n",
      "Predict [1.772532285069913e-22,8.330603787435788e-38] , Actual [0.2 0.1] \n",
      "Predict [9.951651612273405e-23,1.1165247094617354e-40] , Actual [0.2 0.1] \n",
      "Predict [3.1319522416270367e-18,8.009320855545884e-33] , Actual [0.2 0.1] \n",
      "Predict [1.2929690042278312e-23,2.059317311168569e-40] , Actual [0.2 0.1] \n",
      "Predict [6.239418779104891e-24,1.3945807562489422e-49] , Actual [0.2 0.1] \n",
      "Predict [3.519902492867536e-18,8.994735273143325e-31] , Actual [0.2 0.1] \n",
      "Predict [3.990094073093505e-19,2.220961851451192e-39] , Actual [0.2 0.1] \n",
      "Predict [3.823127288778898e-12,1.0243051432734119e-18] , Actual [0.2 0.1] \n",
      "Predict [1.0377781420696099e-33,1.6007826585807104e-34] , Actual [0.2 0.1] \n",
      "Predict [1.9545436429131965e-18,4.2309337578378285e-28] , Actual [0.2 0.1] \n",
      "Predict [9.106113177743288e-19,2.993431864198388e-40] , Actual [0.2 0.1] \n",
      "Predict [1.0020115961585987e-20,2.0941176698291413e-43] , Actual [0.2 0.1] \n",
      "Predict [5.786227683240722e-22,4.6489031976259043e-38] , Actual [0.2 0.1] \n",
      "Predict [4.2124215433741775e-21,6.2088682514685384e-30] , Actual [0.2 0.1] \n",
      "Predict [2.1069727860363007e-20,2.3522349879023823e-35] , Actual [0.2 0.1] \n",
      "Predict [8.420403101032704e-21,7.791573384124752e-32] , Actual [0.2 0.1] \n",
      "Predict [7.341424039544947e-33,3.2465239405615785e-59] , Actual [0.2 0.1] \n",
      "Predict [2.8119593409171255e-22,9.515649010028058e-32] , Actual [0.2 0.1] \n",
      "Predict [1.506881092016767e-20,3.5632030761792765e-41] , Actual [0.2 0.1] \n",
      "Predict [1.3912698136981352e-15,6.588858739840311e-30] , Actual [0.2 0.1] \n",
      "Predict [9.853465944495288e-22,2.6051465026919748e-25] , Actual [0.2 0.1] \n",
      "Predict [2.3819794052985354e-46,4.918462636731445e-52] , Actual [0.2 0.1] \n",
      "Predict [7.02867157473937e-20,4.4531882150861e-56] , Actual [0.2 0.1] \n",
      "Predict [2.3117518797517413e-16,7.628058746090447e-38] , Actual [0.2 0.1] \n",
      "Predict [2.8689428193737786e-20,3.80347251593585e-36] , Actual [0.2 0.1] \n",
      "Predict [3.8029965992195246e-26,6.006997043298338e-39] , Actual [0.2 0.1] \n",
      "Predict [3.0583739033333316e-22,3.264599838536388e-45] , Actual [0.2 0.1] \n",
      "Predict [2.2265505014188124e-19,4.6220496732678555e-31] , Actual [0.2 0.1] \n",
      "Predict [1.42864478371218e-17,1.8500969887667227e-26] , Actual [0.2 0.1] \n",
      "Predict [2.627936975305252e-24,2.3397014893580794e-42] , Actual [0.2 0.1] \n",
      "Predict [3.3661127445062864e-19,6.39626094011947e-32] , Actual [0.2 0.1] \n",
      "Predict [2.982308871021823e-24,5.189434659985094e-47] , Actual [0.1       0.9380531] \n",
      "Predict [4.0497317684850606e-18,1.3277582926784643e-27] , Actual [0.2 0.1] \n",
      "Predict [6.311690689041081e-19,1.058589908317463e-31] , Actual [0.2 0.1] \n",
      "Predict [2.498668339554201e-19,3.5028088166631154e-35] , Actual [0.2 0.1] \n",
      "Predict [8.310935114253808e-39,1.2389295871107998e-65] , Actual [0.2 0.1] \n",
      "Predict [4.0567058029647208e-19,1.515682821668697e-36] , Actual [0.2 0.1] \n",
      "Predict [1.564629015356636e-14,1.034751241427734e-27] , Actual [0.2 0.1] \n",
      "Predict [6.856583078694977e-19,1.0700321828875714e-29] , Actual [0.2 0.1] \n",
      "Predict [9.926043108075141e-26,5.894815513278768e-32] , Actual [0.2 0.1] \n",
      "Predict [2.776211605771815e-50,1.3594576646849158e-68] , Actual [0.1       0.9380531] \n",
      "Predict [5.497049343807522e-17,8.406550797826528e-29] , Actual [0.2 0.1] \n",
      "Predict [1.9525819144898578e-15,1.0030671578667706e-29] , Actual [0.2 0.1] \n",
      "Predict [9.32031588296095e-20,4.770095762052894e-49] , Actual [0.1       0.9380531] \n",
      "Predict [2.7500230265432927e-17,1.6377073371263033e-30] , Actual [0.2 0.1] \n",
      "Predict [1.997545821656265e-17,1.7027822743871794e-28] , Actual [0.2 0.1] \n",
      "Predict [2.5044001284348338e-21,8.391100510288712e-36] , Actual [0.2 0.1] \n",
      "Predict [2.708648229112918e-22,8.006041399330958e-38] , Actual [0.2 0.1] \n",
      "Predict [1.2275113949415634e-15,2.3276625535096897e-30] , Actual [0.2 0.1] \n",
      "Predict [2.8181839473553976e-16,9.535891038049858e-36] , Actual [0.2 0.1] \n",
      "Predict [8.786235250125005e-20,1.067891571027863e-50] , Actual [0.2 0.1] \n",
      "Predict [7.082439180979838e-31,1.6259102026164604e-53] , Actual [0.1       0.9380531] \n",
      "Predict [1.6909352318051592e-16,2.1581009846553676e-30] , Actual [0.2 0.1] \n",
      "Predict [2.6556046217328226e-17,1.3919028245658193e-33] , Actual [0.2 0.1] \n",
      "Predict [3.5646129768662813e-13,2.6042044755017125e-29] , Actual [0.2 0.1] \n",
      "Predict [2.8073421208939746e-16,3.777011150098487e-25] , Actual [0.2 0.1] \n",
      "Predict [1.2269771921313902e-18,5.96807514678286e-40] , Actual [0.2 0.1] \n",
      "Predict [1.5354345886442918e-15,2.0474863938545907e-29] , Actual [0.2 0.1] \n",
      "Predict [3.674241665993625e-21,2.2324714474616638e-35] , Actual [0.2 0.1] \n",
      "Predict [2.8119193166105653e-21,1.8016836362547783e-34] , Actual [0.2 0.1] \n",
      "Predict [1.6267680202320744e-14,2.851520984951161e-28] , Actual [0.2 0.1] \n",
      "Predict [1.2424257771252012e-21,6.643755040758588e-44] , Actual [0.2 0.1] \n",
      "Predict [9.787067190064752e-20,1.0157778538787406e-36] , Actual [0.2 0.1] \n",
      "Predict [2.3182012847712256e-20,2.2467274309298628e-30] , Actual [0.2 0.1] \n",
      "MAE =  0.1728600516876623\n",
      "\t-----------\t \t-----------\t\n",
      "|\t tn - 106 \t|,|\t fp - 0 \t|\n",
      "|\t fn - 7 \t|,|\t tp - 0 \t|\n",
      "\t-----------\t \t-----------\t\n",
      "Acc = 0.9380530973451328 , f1 = 0.9080696650099002\n",
      "RMSE =  0.22586711276462493\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_f = fuzzy_aggregation(tree_shape,current_fold_train=best_fold,ignore_writeClass=True).print_results_train(best_sol_cross,pos_start,pos_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:54:35.209793Z",
     "start_time": "2020-02-25T07:54:34.037836Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Train count abnormal :  7\n",
      "Predict [9.584554610093681e-15,1.0748017669046977e-30] , Actual [0.2 0.1] \n",
      "Predict [6.229946610193737e-18,2.2088882293966808e-40] , Actual [0.2 0.1] \n",
      "Predict [4.854583633191433e-16,4.31009564151554e-36] , Actual [0.2 0.1] \n",
      "Predict [4.067217837638062e-15,2.2801249848999826e-26] , Actual [0.2 0.1] \n",
      "Predict [2.948589073074509e-16,4.7572876327244755e-29] , Actual [0.2 0.1] \n",
      "Predict [8.800838099079111e-60,5.04583058840253e-91] , Actual [0.1       0.9380531] \n",
      "Predict [7.4625411086312e-16,9.798505955934996e-29] , Actual [0.2 0.1] \n",
      "Predict [5.112113539441883e-16,2.7658666654489753e-25] , Actual [0.2 0.1] \n",
      "Predict [5.882528956207954e-16,5.669423618597022e-32] , Actual [0.2 0.1] \n",
      "Predict [3.055323930192345e-13,2.021532575275927e-20] , Actual [0.2 0.1] \n",
      "Predict [1.064058724346485e-15,5.907366486591436e-31] , Actual [0.2 0.1] \n",
      "Predict [4.049183212184201e-22,2.488521021965513e-55] , Actual [0.2 0.1] \n",
      "Predict [1.8992336360189843e-23,2.7896612229076784e-42] , Actual [0.2 0.1] \n",
      "Predict [3.0752255419004e-32,8.212794109039266e-38] , Actual [0.1       0.9380531] \n",
      "Predict [1.1755959090357753e-16,1.7768456623791396e-36] , Actual [0.2 0.1] \n",
      "Predict [4.0056507132051925e-16,6.205741958310126e-34] , Actual [0.2 0.1] \n",
      "Predict [2.185621907491014e-15,6.988525213112862e-29] , Actual [0.2 0.1] \n",
      "Predict [9.090252335974386e-17,5.133756921280016e-22] , Actual [0.2 0.1] \n",
      "Predict [8.109328945592963e-24,1.4800884819863435e-35] , Actual [0.2 0.1] \n",
      "Predict [7.631561518238127e-22,1.2514195941720358e-34] , Actual [0.2 0.1] \n",
      "Predict [2.98450024237473e-17,3.555905222806181e-30] , Actual [0.2 0.1] \n",
      "Predict [4.693537287936042e-18,7.847614733140912e-34] , Actual [0.2 0.1] \n",
      "Predict [5.814685007896344e-16,1.6049881782012185e-39] , Actual [0.2 0.1] \n",
      "Predict [7.635184201197937e-25,1.2247634723291716e-38] , Actual [0.2 0.1] \n",
      "Predict [3.1362324525449203e-25,2.5298856652986845e-28] , Actual [0.2 0.1] \n",
      "Predict [5.071352548121999e-18,1.1671727067278298e-25] , Actual [0.2 0.1] \n",
      "Predict [8.714405263781595e-18,2.04352933224331e-26] , Actual [0.2 0.1] \n",
      "Predict [4.799353398041011e-20,6.718526669996153e-37] , Actual [0.2 0.1] \n",
      "Predict [2.766899806101348e-56,1.1659143205253647e-98] , Actual [0.1       0.9380531] \n",
      "Predict [1.7773667161410215e-21,1.1746225167018659e-39] , Actual [0.2 0.1] \n",
      "Predict [1.6287563154261355e-20,7.631835284538235e-43] , Actual [0.2 0.1] \n",
      "Predict [7.528922737301606e-25,8.907041399853652e-34] , Actual [0.2 0.1] \n",
      "Predict [1.83038167384152e-15,2.7934338299877245e-26] , Actual [0.2 0.1] \n",
      "Predict [1.1196421929207776e-15,5.272666599234836e-32] , Actual [0.2 0.1] \n",
      "Predict [2.283586177963994e-23,3.57275092433123e-45] , Actual [0.2 0.1] \n",
      "Predict [6.015279615965803e-16,3.0912347046064527e-35] , Actual [0.2 0.1] \n",
      "Predict [7.206926200081566e-16,5.198451713527325e-29] , Actual [0.2 0.1] \n",
      "Predict [2.4046986860899276e-24,7.194226739857346e-39] , Actual [0.2 0.1] \n",
      "Predict [2.5332331121013086e-22,3.0049448470403416e-42] , Actual [0.1       0.9380531] \n",
      "Predict [4.2678272245630795e-19,2.1043396038337703e-29] , Actual [0.2 0.1] \n",
      "Predict [2.313355754697465e-21,1.3340592231373272e-44] , Actual [0.2 0.1] \n",
      "Predict [4.133407694847243e-14,1.0981441280902976e-21] , Actual [0.2 0.1] \n",
      "Predict [1.338946433738429e-30,1.1792891113455284e-53] , Actual [0.2 0.1] \n",
      "Predict [4.394628853859077e-30,2.267757309691691e-54] , Actual [0.2 0.1] \n",
      "Predict [3.594516591448862e-21,3.1799239642832306e-48] , Actual [0.2 0.1] \n",
      "Predict [2.6529463947849795e-33,2.897788786249624e-49] , Actual [0.1       0.9380531] \n",
      "Predict [4.470683212684367e-15,2.3303737791580583e-25] , Actual [0.2 0.1] \n",
      "Predict [2.3266023413935494e-23,3.433971865762532e-41] , Actual [0.2 0.1] \n",
      "Predict [1.555461100532511e-18,2.3122007058161363e-31] , Actual [0.2 0.1] \n",
      "Predict [2.73162603736299e-15,2.7705231192118756e-33] , Actual [0.2 0.1] \n",
      "Predict [4.938172937597133e-19,7.042233655348395e-31] , Actual [0.2 0.1] \n",
      "Predict [1.4668773583068163e-14,8.974095754871837e-27] , Actual [0.2 0.1] \n",
      "Predict [4.1721474718884913e-29,1.1019021389703008e-36] , Actual [0.2 0.1] \n",
      "Predict [8.08520225535649e-15,1.6322480932026183e-19] , Actual [0.2 0.1] \n",
      "Predict [4.505237581757397e-22,1.1491946040227081e-34] , Actual [0.2 0.1] \n",
      "Predict [5.263249327856257e-20,1.578762143079604e-42] , Actual [0.2 0.1] \n",
      "Predict [1.6370962700926288e-22,9.374618404471576e-34] , Actual [0.2 0.1] \n",
      "Predict [4.408034332057366e-33,2.1527995911519077e-67] , Actual [0.1       0.9380531] \n",
      "Predict [5.548285959213324e-31,6.123704157603914e-39] , Actual [0.2 0.1] \n",
      "Predict [1.8046222065444872e-17,2.179207220383299e-30] , Actual [0.2 0.1] \n",
      "Predict [1.769194472564559e-25,8.598274649027033e-45] , Actual [0.2 0.1] \n",
      "Predict [1.0797294627018798e-11,2.1161496713507204e-21] , Actual [0.2 0.1] \n",
      "Predict [7.313973004058935e-21,3.480844328692576e-37] , Actual [0.2 0.1] \n",
      "Predict [2.3925703650798564e-18,2.0375001479449152e-42] , Actual [0.2 0.1] \n",
      "Predict [5.362559290516e-17,7.619478300237904e-34] , Actual [0.2 0.1] \n",
      "Predict [2.689400050108513e-19,2.9230612357766997e-29] , Actual [0.2 0.1] \n",
      "Predict [3.0004038592066164e-19,7.693852409210118e-45] , Actual [0.2 0.1] \n",
      "Predict [9.289874279375501e-44,4.9004113493361354e-55] , Actual [0.2 0.1] \n",
      "Predict [2.537098231696592e-24,4.5086870105448725e-38] , Actual [0.2 0.1] \n",
      "Predict [7.949205697086543e-26,1.556270358401438e-42] , Actual [0.2 0.1] \n",
      "Predict [3.350383114394255e-17,8.750222931750927e-32] , Actual [0.2 0.1] \n",
      "Predict [1.7827429958631737e-15,4.538345008265487e-30] , Actual [0.2 0.1] \n",
      "Predict [2.674276850044152e-20,2.394293931398622e-40] , Actual [0.2 0.1] \n",
      "Predict [6.403613821612492e-22,1.5966153818456155e-36] , Actual [0.2 0.1] \n",
      "Predict [5.532709116906653e-13,2.544491557787018e-23] , Actual [0.2 0.1] \n",
      "Predict [1.1762106954683847e-13,1.0135750036795164e-22] , Actual [0.2 0.1] \n",
      "Predict [8.216538125697256e-15,6.629890148177274e-29] , Actual [0.2 0.1] \n",
      "Predict [5.656803169128402e-18,5.822169653931839e-32] , Actual [0.2 0.1] \n",
      "Predict [1.0130968240943638e-15,6.31173429300099e-26] , Actual [0.2 0.1] \n",
      "Predict [4.0346181990953287e-20,1.0816741463489691e-38] , Actual [0.2 0.1] \n",
      "Predict [1.3540774482695885e-17,1.5004528921952827e-29] , Actual [0.2 0.1] \n",
      "Predict [4.301779374819026e-45,3.416880482067757e-51] , Actual [0.2 0.1] \n",
      "Predict [8.213052798565807e-18,1.3109092434670338e-38] , Actual [0.2 0.1] \n",
      "Predict [6.580896050076653e-15,1.527099397016664e-31] , Actual [0.2 0.1] \n",
      "Predict [3.74046749182666e-17,4.9172138650307716e-30] , Actual [0.2 0.1] \n",
      "Predict [1.4399404937651832e-15,8.839230910331696e-28] , Actual [0.2 0.1] \n",
      "Predict [5.4995504087279245e-17,9.390266677752767e-26] , Actual [0.2 0.1] \n",
      "Predict [1.1777905395404274e-15,5.074698085354813e-30] , Actual [0.2 0.1] \n",
      "Predict [1.2220156689742997e-20,5.851934565195945e-31] , Actual [0.2 0.1] \n",
      "Predict [3.760348733564708e-20,2.2535258924844964e-35] , Actual [0.2 0.1] \n",
      "Predict [1.4342983782406446e-20,1.5005534026731855e-29] , Actual [0.2 0.1] \n",
      "Predict [1.2435386349396248e-25,2.0275881889571813e-42] , Actual [0.2 0.1] \n",
      "Predict [2.5834749038874366e-21,1.9309318138941308e-37] , Actual [0.2 0.1] \n",
      "Predict [4.75553125567658e-44,1.1867712806082112e-51] , Actual [0.2 0.1] \n",
      "Predict [5.432503500478092e-14,1.2086696722637303e-31] , Actual [0.2 0.1] \n",
      "Predict [1.5095170709495706e-16,1.3737380035161887e-41] , Actual [0.2 0.1] \n",
      "Predict [1.2544843056614513e-21,1.3970801113006512e-46] , Actual [0.2 0.1] \n",
      "Predict [5.215662957802174e-22,4.8615690742387006e-39] , Actual [0.2 0.1] \n",
      "Predict [4.745482256108627e-20,1.8776656529451441e-28] , Actual [0.2 0.1] \n",
      "Predict [3.525020465045581e-19,5.695207113817571e-29] , Actual [0.2 0.1] \n",
      "Predict [1.4010708818930854e-17,7.174212255297443e-29] , Actual [0.2 0.1] \n",
      "Predict [1.1527211884938748e-27,1.2465183366424312e-46] , Actual [0.2 0.1] \n",
      "Predict [6.97972191268548e-16,1.5880441628941815e-31] , Actual [0.2 0.1] \n",
      "Predict [1.227535295849565e-17,1.425522993899175e-35] , Actual [0.2 0.1] \n",
      "Predict [1.049248675449459e-22,1.4723886280231887e-36] , Actual [0.2 0.1] \n",
      "Predict [1.0553392817607341e-26,3.976848608672387e-35] , Actual [0.2 0.1] \n",
      "Predict [8.454623864907901e-15,2.2333609633014936e-28] , Actual [0.2 0.1] \n",
      "Predict [1.6630425150551345e-15,5.846285791658271e-26] , Actual [0.2 0.1] \n",
      "Predict [2.107270435823733e-19,4.488644512372859e-32] , Actual [0.2 0.1] \n",
      "Predict [9.651264889228264e-24,2.5165010597949583e-24] , Actual [0.2 0.1] \n",
      "Predict [9.09292215488378e-24,4.4119338361847936e-35] , Actual [0.2 0.1] \n",
      "Predict [2.661829048683734e-21,1.345976664146548e-35] , Actual [0.2 0.1] \n",
      "Predict [3.806040785512071e-18,2.729466789374603e-36] , Actual [0.2 0.1] \n",
      "Predict [6.190065397413445e-22,3.868447286366946e-37] , Actual [0.2 0.1] \n",
      "MAE =  0.1694224499300826\n",
      "\t-----------\t \t-----------\t\n",
      "|\t tn - 108 \t|,|\t fp - 0 \t|\n",
      "|\t fn - 6 \t|,|\t tp - 0 \t|\n",
      "\t-----------\t \t-----------\t\n",
      "Acc = 0.9473684210526315 , f1 = 0.9217638691322901\n",
      "RMSE =  0.21703405101734616\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "system_f = fuzzy_aggregation(tree_shape,current_fold_train=best_fold,ignore_writeClass=True).print_results(best_sol_cross,pos_start,pos_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:54:35.269677Z",
     "start_time": "2020-02-25T07:54:35.212481Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Train count abnormal :  6\n",
      "Weight layer  0 : 1   [0.3843574887055416, 1.8055313639277595, 0.39680563558857945, 1.2437511353360553, 1.0203409611322658, 1.908786107472466, 0.7114571673565182, 0.5289701404808145]\n",
      "Gammar operator :  0.7847851253900539\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 2   [0.17290113777668364, 0.535225896295119, 1.1747788947930122, 0.5165634796019308, 1.5248797150908844, 1.3409590402389313, 1.6244674998768571, 1.1102243363265822]\n",
      "Gammar operator :  0.023540287211004873\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 3   [1.697020699565047, 0.11085411009854876, 1.1922841421740622, 1.0379989386737578, 0.34342847281244526, 0.993184885033625, 1.0419543811050502, 1.5832743705374623]\n",
      "Gammar operator :  0.21320277797556775\n",
      "Sum Weights :  7.999999999999998\n",
      "\n",
      "Weight layer  0 : 4   [0.7896168051603042, 0.5804059362529173, 0.45839546244870266, 1.2898719145199908, 1.6406050719228908, 1.4080734965323605, 1.2423111090831085, 0.590720204079725]\n",
      "Gammar operator :  0.610527741913788\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 5   [0.3778048724767741, 0.8914208998971485, 1.5405545784427919, 0.4275641842720829, 1.3126327049431954, 0.4288690070672632, 1.7173650776431861, 1.3037886752575583]\n",
      "Gammar operator :  0.4770458238419104\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 6   [0.4059691926196021, 0.49636567156032635, 2.033335055261691, 2.0506296865174742, 0.27238514895554394, 1.1100988433177053, 0.6022932939218173, 1.0289231078458407]\n",
      "Gammar operator :  0.41931386803553417\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 7   [1.0979491140762114, 1.0057955606045559, 1.2360831909838035, 1.3041693659909326, 0.5447996152531053, 0.6188725467118275, 1.2923145844002852, 0.9000160219792791]\n",
      "Gammar operator :  0.843013845912762\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 8   [0.21823835988649626, 1.482672530877826, 0.981919325997609, 1.1332034172543057, 0.07222387983149715, 1.1537473264284646, 1.6044736475697976, 1.3535215121540045]\n",
      "Gammar operator :  0.4950543111746317\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 9   [0.7239529175191738, 1.4892954057062016, 1.5047018560121244, 0.7553698596981292, 0.9836561431896905, 1.3861414173951592, 0.3276874377143955, 0.829194962765126]\n",
      "Gammar operator :  0.4796901442071262\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 10   [1.0094868425589358, 0.8087770207278352, 0.8066652243657371, 0.016211696265979507, 1.68077390642495, 1.8975443365529192, 1.450936725150902, 0.3296042479527412]\n",
      "Gammar operator :  0.5923123615581208\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 11   [2.304544421093392, 0.8904550457594106, 0.9952758617803635, 0.9158971889389117, 0.7995044821687053, 0.1760672899892107, 0.14547783476213638, 1.7727778755078691]\n",
      "Gammar operator :  0.602678766795828\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 12   [1.0834950494115403, 0.06405608506822565, 0.37468788183985585, 1.7415322772773782, 1.1429277069222012, 0.5200352195953241, 2.576994733388405, 0.49627104649707005]\n",
      "Gammar operator :  0.17737636612597718\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 13   [1.0511791383131637, 1.3916638528024072, 1.1830695231975277, 0.3467370835267917, 1.3684705187215056, 1.0644045369871757, 0.6014217345214112, 0.993053611930017]\n",
      "Gammar operator :  0.8499477636228445\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 14   [0.8771471446464758, 0.48945719916139624, 0.9830181884441058, 2.0814482068304048, 1.406144596835045, 0.8789156440520275, 1.147149904907914, 0.1367191151226302]\n",
      "Gammar operator :  0.635483343153523\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 15   [1.4370306166613505, 1.2376879460518984, 0.08605632610697782, 0.7073140547063415, 0.09399334293763857, 2.96262894715448, 0.2522665882807025, 1.2230221781006114]\n",
      "Gammar operator :  0.04970334744925542\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 16   [0.1400927822244879, 0.1508449176646008, 0.924276406854484, 0.48563649829151123, 1.4006562859445417, 1.9762333194953103, 1.627538982949764, 1.2947208065753002]\n",
      "Gammar operator :  0.677471340808157\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 17   [0.5323443761589174, 1.2192339054433312, 0.3818148771247661, 1.7499213803722664, 0.2727932134070568, 1.8283052437445797, 0.37996852436469447, 1.6356184793843875]\n",
      "Gammar operator :  0.5711712395864326\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 18   [0.4621947798398999, 0.016540257220890423, 1.0076686351442088, 1.4702247018197947, 1.0363888093010465, 1.7648052851920752, 1.970022390017087, 0.2721551414649961]\n",
      "Gammar operator :  0.7569527747733142\n",
      "Sum Weights :  7.999999999999998\n",
      "\n",
      "Weight layer  0 : 19   [1.9156344779765198, 0.8658917884570592, 0.18938238992864512, 0.2784439660938204, 1.5845378900160665, 1.134626277083148, 0.23948084993118338, 1.7920023605135582]\n",
      "Gammar operator :  0.9842622156493199\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 20   [0.38131965817516866, 0.7308842298755877, 1.9078211382882864, 0.9676938460830588, 0.47898525227868727, 0.6423269239522963, 1.637245673690455, 1.2537232776564597]\n",
      "Gammar operator :  0.009456422522785557\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 21   [1.2589232876654024, 1.7459832160592708, 1.3080168145661433, 0.8714502217303576, 0.22557109133150674, 0.40455395483251916, 0.5101504565565681, 1.6753509572582321]\n",
      "Gammar operator :  0.327557075106366\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 22   [1.617836752982117, 1.592852162510699, 0.4577253372233647, 1.3837992580879466, 1.313068362969823, 0.7911636029750206, 0.44690198114511515, 0.3966525421059141]\n",
      "Gammar operator :  0.3701916361037798\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 23   [0.19517888368510075, 1.950652419673239, 0.7203334618806487, 0.6681499729260965, 0.4989331309163518, 1.3982459015367008, 1.0163524559715469, 1.5521537734103157]\n",
      "Gammar operator :  0.06221409076877871\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 24   [1.2508727247440685, 0.5364102221860888, 1.4539877434003117, 1.6671640886339725, 1.0162978045056468, 1.4461853198250247, 0.6100372293693753, 0.019044867335511716]\n",
      "Gammar operator :  0.3173934861989022\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 25   [0.3258693037883629, 1.0817745531225045, 1.6870026553763169, 2.1214704750815367, 1.043261237281436, 1.175951964556124, 0.4428173658067705, 0.12185244498694969]\n",
      "Gammar operator :  0.028962882520855437\n",
      "Sum Weights :  8.000000000000002\n",
      "\n",
      "Weight layer  0 : 26   [1.8916415026772504, 1.4144021198181038, 0.165146329137244, 0.5084128858940655, 0.8049642942774649, 1.2094140869782009, 1.9011978370027238, 0.10482094421494563]\n",
      "Gammar operator :  0.6850050984367855\n",
      "Sum Weights :  7.999999999999999\n",
      "\n",
      "Weight layer  0 : 27   [2.068320577272301, 1.6558327591188575, 1.9469696919193975, 0.14334150668570836, 0.1961394400933084, 0.7120060030360712, 0.9828233711937011, 0.2945666506806543]\n",
      "Gammar operator :  0.4295680840867987\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 28   [0.9670624720109092, 0.8563735507508098, 1.6599309706957146, 0.5301890216724561, 1.2433103979176674, 1.079820808108157, 1.3456022899164843, 0.3177104889278012]\n",
      "Gammar operator :  0.1674175576184831\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 29   [1.8013075556568454, 1.3346187601136186, 0.745674056354652, 0.11248288484726807, 1.703457531102687, 0.09689412572844708, 1.9405073363292746, 0.26505774986720765]\n",
      "Gammar operator :  0.26166286758219104\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 30   [0.8401626540463388, 2.0344645715536456, 0.16065787913582788, 0.04800160913056394, 1.7924820274300524, 1.998299882963433, 0.18371957043574302, 0.9422118053043946]\n",
      "Gammar operator :  0.39364702617491565\n",
      "Sum Weights :  7.999999999999999\n",
      "\n",
      "Weight layer  0 : 31   [0.41072745651884024, 0.10567957483531508, 0.4312255603648636, 2.0952672903128207, 1.2736041032673955, 0.6990953494455697, 1.1480187205530765, 1.8363819447021188]\n",
      "Gammar operator :  0.8119381136433398\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 32   [0.6497858172981128, 0.6614019649287041, 0.16954585192773453, 1.0937891388016596, 0.8614328271955979, 2.1729282773063425, 0.08311607465256227, 2.308000047889288]\n",
      "Gammar operator :  0.5107160300764699\n",
      "Sum Weights :  8.000000000000002\n",
      "\n",
      "Weight layer  0 : 33   [0.317537025841525, 0.4384587180144262, 1.1476776037621308, 1.804362284639209, 0.18806046326876577, 0.3310347846939674, 1.8328822179014912, 1.9399869018784845]\n",
      "Gammar operator :  0.255145050422417\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 34   [0.9748410188464848, 1.533290204419896, 1.7411939695494563, 0.9643395445836158, 0.9209367945386853, 0.5954563370343686, 0.17401923908892913, 1.0959228919385637]\n",
      "Gammar operator :  0.06813909787412264\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 35   [0.5455723209873026, 1.9660814588413549, 0.46894571794531104, 0.25340726537955666, 1.7227193207174893, 1.2605684141903528, 1.4029371189725854, 0.37976838296604776]\n",
      "Gammar operator :  0.5374711580131643\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 36   [1.2957043527784855, 2.1270195549349347, 0.8704714276709212, 1.459917176746926, 0.1713727117275889, 0.5792247652922012, 0.030332762205390527, 1.465957248643551]\n",
      "Gammar operator :  0.22972598336906047\n",
      "Sum Weights :  7.999999999999998\n",
      "\n",
      "Weight layer  0 : 37   [0.49622847117586705, 1.7793424909278122, 1.2788606038377437, 0.543113702872222, 0.7071637981421194, 0.9908674533452083, 0.6040628858267879, 1.60036059387224]\n",
      "Gammar operator :  0.13307879359754815\n",
      "Sum Weights :  8.0\n",
      "\n",
      "Weight layer  0 : 38   [0.6176602984373994, 0.4045458800392088, 0.9890415136094884, 1.2422340485912788, 1.190053668909053, 0.44139347709888194, 1.2665536212671136, 1.8485174920475766]\n",
      "Gammar operator :  0.8641590398293584\n",
      "Sum Weights :  8.000000000000002\n",
      "\n",
      "For check sum at layer:  304.0\n",
      "\n",
      "Weight layer  1 : 1   [0.6704349926058549, 1.2969836306772775, 1.032581376716868]\n",
      "Gammar operator :  0.3727908229016247\n",
      "Sum Weights :  3.0000000000000004\n",
      "\n",
      "Weight layer  1 : 2   [1.0751484183040532, 0.9248515816959467]\n",
      "Gammar operator :  0.8329734870264872\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 3   [0.01431702544032188, 1.985682974559678]\n",
      "Gammar operator :  0.8970408703642374\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 4   [1.3019349259846644, 0.6292349765748748, 1.5023301183330282, 0.5664999791074329]\n",
      "Gammar operator :  0.07795876123538525\n",
      "Sum Weights :  4.0\n",
      "\n",
      "Weight layer  1 : 5   [1.0370725122201476, 0.5883916712082033, 2.1821426402539874, 0.19239317631766134]\n",
      "Gammar operator :  0.6402410724624193\n",
      "Sum Weights :  3.9999999999999996\n",
      "\n",
      "Weight layer  1 : 6   [1.1625748834760914, 0.8374251165239086]\n",
      "Gammar operator :  0.2753482948131011\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 7   [1.0]\n",
      "Gammar operator :  0.08382163613661175\n",
      "Sum Weights :  1.0\n",
      "\n",
      "Weight layer  1 : 8   [0.12630767516645972, 0.970056155417102, 1.1365139426550974, 1.7671222267613411]\n",
      "Gammar operator :  0.7124299033469106\n",
      "Sum Weights :  4.0\n",
      "\n",
      "Weight layer  1 : 9   [0.6667845160262944, 1.3332154839737056]\n",
      "Gammar operator :  0.8361022104302129\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 10   [1.0]\n",
      "Gammar operator :  0.7761380606528172\n",
      "Sum Weights :  1.0\n",
      "\n",
      "Weight layer  1 : 11   [0.7860321028853461, 1.213967897114654]\n",
      "Gammar operator :  0.2867931001024516\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 12   [0.8290360205283709, 1.1709639794716291]\n",
      "Gammar operator :  0.46689436264908746\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  1 : 13   [1.892362640049149, 1.4523212719278675, 0.03176617632838913, 1.4731201190036733, 0.15042979269092174]\n",
      "Gammar operator :  0.8391454949127978\n",
      "Sum Weights :  5.0\n",
      "\n",
      "Weight layer  1 : 14   [1.0]\n",
      "Gammar operator :  0.4520412118204429\n",
      "Sum Weights :  1.0\n",
      "\n",
      "Weight layer  1 : 15   [1.0]\n",
      "Gammar operator :  0.10109352176931863\n",
      "Sum Weights :  1.0\n",
      "\n",
      "Weight layer  1 : 16   [1.0378598273828443, 0.9621401726171557]\n",
      "Gammar operator :  0.08534581671734476\n",
      "Sum Weights :  2.0\n",
      "\n",
      "For check sum at layer:  38.0\n",
      "\n",
      "Weight layer  2 : 1   [1.3046890796810584, 0.6953109203189417]\n",
      "Gammar operator :  0.2698707494907605\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  2 : 2   [1.0058606918977837, 0.7260049377952105, 0.08794676956598303, 1.7973370988695676, 1.3828505018714552]\n",
      "Gammar operator :  0.7220493568172229\n",
      "Sum Weights :  5.0\n",
      "\n",
      "Weight layer  2 : 3   [0.6286078973978454, 0.5467644703602134, 1.8246276322419412]\n",
      "Gammar operator :  0.33819036023801974\n",
      "Sum Weights :  3.0\n",
      "\n",
      "Weight layer  2 : 4   [0.38037526588012077, 1.6196247341198793]\n",
      "Gammar operator :  0.13774754555474822\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  2 : 5   [0.6505047284779905, 1.3494952715220097]\n",
      "Gammar operator :  0.19492702074004697\n",
      "Sum Weights :  2.0\n",
      "\n",
      "Weight layer  2 : 6   [0.826001626459548, 1.173998373540452]\n",
      "Gammar operator :  0.7356765883919767\n",
      "Sum Weights :  2.0\n",
      "\n",
      "For check sum at layer:  16.0\n",
      "\n",
      "Weight layer  3 : 1   [0.5672016091676708, 1.0027052754634274, 1.3517643019310595, 1.435869842827481, 1.0864406499122898, 0.5560183206980706]\n",
      "Gammar operator :  0.6349509445380217\n",
      "Sum Weights :  5.999999999999999\n",
      "\n",
      "Weight layer  3 : 2   [0.012801740582022755, 2.155997361223908, 1.1942108665754863, 0.05360923913257631, 0.8160502390721639, 1.7673305534138437]\n",
      "Gammar operator :  0.10329575581177863\n",
      "Sum Weights :  6.000000000000001\n",
      "\n",
      "For check sum at layer:  12.0\n",
      "\n",
      "\n",
      "#### Left output node ####\n",
      "liquidity ratio :  0.5672016091676708\n",
      "Asset utilization or turnover ratio :  1.0027052754634274\n",
      "Profitability :  1.3517643019310595\n",
      "Gammar operator :  1.435869842827481\n",
      "Leverage :  1.0864406499122898\n",
      "Growth ratios :  0.5560183206980706\n",
      "Gammar operator :  0.6349509445380217\n",
      "For check sum :  5.999999999999999\n",
      "\n",
      "#### Right output node ####\n",
      "liquidity ratio :  0.012801740582022755\n",
      "Asset utilization or turnover ratio :  2.155997361223908\n",
      "Profitability :  1.1942108665754863\n",
      "Asset structure ratios :  0.05360923913257631\n",
      "Leverage :  0.8160502390721639\n",
      "Growth ratios :  1.7673305534138437\n",
      "Gammar operator :  0.10329575581177863\n",
      "For check sum :  6.000000000000001\n"
     ]
    }
   ],
   "source": [
    "system_f = fuzzy_aggregation(tree_shape,ignore_writeClass=True)\n",
    "system_f.set_allWeights(best_sol_cross)\n",
    "system_f.showAllWeights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "str_out = \"training SSA with n = %s , lb = %s , ub = %s, dimension = %s, iteration = %s, pf= %s \" % (train_params[\"NP\"] , train_params[\"lb\"], train_params[\"ub\"], train_params[\"dimension\"], train_params[\"nIteration\"], train_params[\"pf\"])\n",
    "    \n",
    "dt = datetime.now()\n",
    "str_out = \"\\n\"+ dt.strftime(\"%d/%m/%Y %H:%M\") + \" น.\\n\" + str_out\n",
    "    \n",
    "\n",
    "url = \"https://notify-api.line.me/api/notify\"\n",
    "\n",
    "payload = {'message': str_out}\n",
    "files = [\n",
    "\n",
    "]\n",
    "headers = {\n",
    "  'Authorization': 'Bearer bR3qarCoaVfI14aKFkSEOARPU4LMc8ZDfKpUiki1NNl',\n",
    "  'Content-Type': 'multipart/form-data; boundary=--------------------------432408813026345718460603'\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, params = payload, files = files)\n",
    "\n",
    "print(response.text.encode('utf8'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 377.85,
   "position": {
    "height": "399.85px",
    "left": "1032px",
    "right": "51px",
    "top": "122px",
    "width": "357px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
